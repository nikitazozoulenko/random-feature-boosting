{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import openml\n",
    "\n",
    "from optuna_kfoldCV import evaluate_dataset_with_model, run_all_openML_with_model\n",
    "from regression_param_specs import evaluate_Ridge, evaluate_XGBoostRegressor\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the collection with ID 353\n",
    "collection = openml.study.get_suite(353)\n",
    "dataset_ids = collection.data\n",
    "metadata_list = []\n",
    "\n",
    "# Fetch and process each dataset\n",
    "for i, dataset_id in enumerate(dataset_ids):\n",
    "    dataset = openml.datasets.get_dataset(dataset_id)\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        target=dataset.default_target_attribute\n",
    "    )\n",
    "\n",
    "    #count missing values in X\n",
    "    missing_values_count = X.isnull().sum().sum()\n",
    "    print(f\"Missing values in X: {missing_values_count}\")\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)[..., None]\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    # Determine if the dataset has categorical features\n",
    "    has_categorical = any(categorical_indicator)\n",
    "    \n",
    "    # Extract the required metadata\n",
    "    metadata = {\n",
    "        'dataset_id': dataset.id,\n",
    "        'name': dataset.name,\n",
    "        'n_obs': int(dataset.qualities['NumberOfInstances']),\n",
    "        'n_features': int(dataset.qualities['NumberOfFeatures']),\n",
    "        '%_unique_y': len(np.unique(y))/len(y),\n",
    "        'n_unique_y': len(np.unique(y)),\n",
    "        'has_categorical': has_categorical,\n",
    "        'n_missing_values': missing_values_count,\n",
    "    }\n",
    "    \n",
    "    metadata_list.append(metadata)\n",
    "    print(f\" {i+1}/{len(dataset_ids)} Processed dataset {dataset.id}: {dataset.name}\")\n",
    "\n",
    "# Create a DataFrame from the metadata list\n",
    "df_metadata = pd.DataFrame(metadata_list).sort_values('%_unique_y', ascending=False).set_index(\"dataset_id\").sort_index()\n",
    "df_metadata.sort_values('%_unique_y', ascending=True)\n",
    "\n",
    "# Display the metadata DataFrame\n",
    "df_metadata.loc[44962, \"has_categorical\"] = True\n",
    "df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna_kfoldCV import np_load_openml_dataset, openML_reg_ids\n",
    "import numpy as np\n",
    "\n",
    "for id in openML_reg_ids:\n",
    "    X,y = np_load_openml_dataset(id, \"regression\")\n",
    "    print(\"id\", id, \"X\", X.shape, \"y\", y.shape, np.isnan(X).sum(), np.isnan(y).sum())\n",
    "\n",
    "# for id in df_metadata.index:\n",
    "#     X,y = np_load_openml_dataset(id, \"regression\")\n",
    "#     print(\"id\", id, \"X\", X.shape, \"y\", y.shape, np.isnan(X).sum(), np.isnan(y).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiments (just for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python regression_param_specs.py \\\n",
    "#     --models End2End \\\n",
    "#     --dataset_indices 0 \\\n",
    "#     --save_dir /home/nikita/Code/random-feature-boosting/save/OpenMLRegression/ \\\n",
    "#     --n_optuna_trials 2 \\\n",
    "#     --device cuda \\\n",
    "#     --k_folds 2 \\\n",
    "#     --cv_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python regression_param_specs.py \\\n",
    "#     --models Ridge \\\n",
    "#     --dataset_indices 10 \\\n",
    "#     --save_dir /home/nikita/Code/random-feature-boosting/save/OpenMLRegression/ \\\n",
    "#     --n_optuna_trials 100 \\\n",
    "#     --device cuda \\\n",
    "#     --k_folds 2 \\\n",
    "#     --cv_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python regression_param_specs.py \\\n",
    "#     --models XGBoostRegressor \\\n",
    "#     --dataset_indices 0 \\\n",
    "#     --save_dir /home/nikita/Code/random-feature-boosting/save/OpenMLRegression/ \\\n",
    "#     --n_optuna_trials 2 \\\n",
    "#     --device cpu \\\n",
    "#     --k_folds 2 \\\n",
    "#     --cv_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python regression_param_specs.py \\\n",
    "    --models GradientRFRBoost_upscaleiid \\\n",
    "    --dataset_indices 0 \\\n",
    "    --save_dir /home/nikita/Code/random-feature-boosting/save/OpenMLRegression/ \\\n",
    "    --n_optuna_trials 2 \\\n",
    "    --device cpu \\\n",
    "    --k_folds 2 \\\n",
    "    --cv_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-12-15 13:02:00,386]\u001b[0m A new study created in memory with name: no-name-0511a962-106c-4d33-941c-97220f2d26fa\u001b[0m\n",
      "/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [16, 512] and step=32, but the range is not divisible by `step`. It will be replaced by [16, 496].\n",
      "  warnings.warn(\n",
      "/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/optuna/distributions.py:693: UserWarning: The distribution is specified by [0.5, 1.00001] and step=0.1, but the range is not divisible by `step`. It will be replaced by [0.5, 1.0].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2024-12-15 13:02:00,607]\u001b[0m Trial 0 finished with value: 0.25049588084220886 and parameters: {'in_dim': 72, 'out_dim': 1, 'feature_type': 'iid', 'upscale_type': 'iid', 'randfeat_xt_dim': 512, 'randfeat_x0_dim': 512, 'n_layers': 2, 'hidden_dim': 496, 'l2_reg': 0.15702970884055384, 'l2_ghat': 0.0015509913987594303, 'boost_lr': 0.5, 'iid_scale': 0.2051110418843398}. Best is trial 0 with value: 0.25049588084220886.\u001b[0m\n",
      "\u001b[32m[I 2024-12-15 13:02:00,704]\u001b[0m Trial 1 finished with value: 0.3651888370513916 and parameters: {'in_dim': 72, 'out_dim': 1, 'feature_type': 'iid', 'upscale_type': 'iid', 'randfeat_xt_dim': 512, 'randfeat_x0_dim': 512, 'n_layers': 1, 'hidden_dim': 432, 'l2_reg': 0.06358358856676251, 'l2_ghat': 0.00904707195756839, 'boost_lr': 0.5, 'iid_scale': 8.706020878304859}. Best is trial 0 with value: 0.25049588084220886.\u001b[0m\n",
      "\u001b[32m[I 2024-12-15 13:02:00,885]\u001b[0m A new study created in memory with name: no-name-5b604d10-4c88-4ba6-9f18-14cbd0e2d70a\u001b[0m\n",
      "\u001b[32m[I 2024-12-15 13:02:01,045]\u001b[0m Trial 0 finished with value: 0.2980804294347763 and parameters: {'in_dim': 72, 'out_dim': 1, 'feature_type': 'iid', 'upscale_type': 'iid', 'randfeat_xt_dim': 512, 'randfeat_x0_dim': 512, 'n_layers': 2, 'hidden_dim': 496, 'l2_reg': 0.15702970884055384, 'l2_ghat': 0.0015509913987594303, 'boost_lr': 0.5, 'iid_scale': 0.2051110418843398}. Best is trial 0 with value: 0.2980804294347763.\u001b[0m\n",
      "\u001b[32m[I 2024-12-15 13:02:01,125]\u001b[0m Trial 1 finished with value: 0.3927428275346756 and parameters: {'in_dim': 72, 'out_dim': 1, 'feature_type': 'iid', 'upscale_type': 'iid', 'randfeat_xt_dim': 512, 'randfeat_x0_dim': 512, 'n_layers': 1, 'hidden_dim': 432, 'l2_reg': 0.06358358856676251, 'l2_ghat': 0.00904707195756839, 'boost_lr': 0.5, 'iid_scale': 8.706020878304859}. Best is trial 0 with value: 0.2980804294347763.\u001b[0m\n",
      " 1/1 Processed dataset 41021\n"
     ]
    }
   ],
   "source": [
    "!python regression_param_specs.py \\\n",
    "    --models GradientRFRBoost_upscaleiid_iidfeat \\\n",
    "    --dataset_indices 0 \\\n",
    "    --save_dir /home/nikita/Code/random-feature-boosting/save/OpenMLRegression/ \\\n",
    "    --n_optuna_trials 2 \\\n",
    "    --device cpu \\\n",
    "    --k_folds 2 \\\n",
    "    --cv_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python regression_param_specs.py \\\n",
    "#     --models GradientRFRBoostID \\\n",
    "#     --dataset_indices 0 \\\n",
    "#     --save_dir /home/nikita/Code/random-feature-boosting/save/OpenMLRegression/ \\\n",
    "#     --n_optuna_trials 2 \\\n",
    "#     --device cpu \\\n",
    "#     --k_folds 2 \\\n",
    "#     --cv_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python regression_param_specs.py \\\n",
    "#     --models GreedyRFRBoostDense \\\n",
    "#     --dataset_indices 0 \\\n",
    "#     --save_dir /home/nikita/Code/random-feature-boosting/save/OpenMLRegression/ \\\n",
    "#     --n_optuna_trials 2 \\\n",
    "#     --device cpu \\\n",
    "#     --k_folds 2 \\\n",
    "#     --cv_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python regression_param_specs.py \\\n",
    "#     --models GreedyRFRBoostDiag \\\n",
    "#     --dataset_indices 0 \\\n",
    "#     --save_dir /home/nikita/Code/random-feature-boosting/save/OpenMLRegression/ \\\n",
    "#     --n_optuna_trials 2 \\\n",
    "#     --device cpu \\\n",
    "#     --k_folds 2 \\\n",
    "#     --cv_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python regression_param_specs.py \\\n",
    "#     --models GreedyRFRBoostScalar \\\n",
    "#     --dataset_indices 0 \\\n",
    "#     --save_dir /home/nikita/Code/random-feature-boosting/save/OpenMLRegression/ \\\n",
    "#     --n_optuna_trials 2 \\\n",
    "#     --device cpu \\\n",
    "#     --k_folds 2 \\\n",
    "#     --cv_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python regression_param_specs.py \\\n",
    "#     --models RandomFeatureNetwork \\\n",
    "#     --dataset_indices 0 \\\n",
    "#     --save_dir /home/nikita/Code/random-feature-boosting/save/OpenMLRegression/ \\\n",
    "#     --n_optuna_trials 2 \\\n",
    "#     --device cpu \\\n",
    "#     --k_folds 2 \\\n",
    "#     --cv_seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# join json results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.v1.utils import deep_update\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from optuna_kfoldCV import openML_reg_ids\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "def custom_deep_update(original, update):\n",
    "    for key, value in update.items():\n",
    "        if isinstance(value, dict) and key in original:\n",
    "            custom_deep_update(original[key], value)\n",
    "        else:\n",
    "            original[key] = value\n",
    "\n",
    "\n",
    "\n",
    "def get_joined_results_json(\n",
    "        models = [\"End2End\", \"Ridge\", \"XGBoostRegressor\", \n",
    "                  \"GradientRFRBoost\", \"GradientRFRBoostID\", \n",
    "                  \"GreedyRFRBoostDense\", \"GreedyRFRBoostDiag\", \"GreedyRFRBoostScalar\",\n",
    "                  \"RandomFeatureNetwork\"],\n",
    "        datasets = openML_reg_ids,\n",
    "        save_dir = \"/home/nikita/Code/random-feature-boosting/save/OpenMLRegression/\",\n",
    "        ):\n",
    "    results_json = {}\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            path = os.path.join(save_dir, f\"regression_{dataset}_{model}.json\")\n",
    "            res = read_json(path)\n",
    "            if results_json == {}:\n",
    "                results_json = res\n",
    "            else:\n",
    "                custom_deep_update(results_json, res)\n",
    "    return results_json\n",
    "\n",
    "\n",
    "\n",
    "def join_jsons_into_array(\n",
    "        results_json,\n",
    "        ):\n",
    "    results = []\n",
    "    for dataset, dataset_results in results_json.items():\n",
    "        res = []\n",
    "        for model_name, model_results in dataset_results.items():\n",
    "            model_res = np.stack([model_results[\"score_train\"], model_results[\"score_test\"], model_results[\"t_fit\"], model_results[\"t_inference\"]])\n",
    "            res.append(model_res)\n",
    "        results.append(res)\n",
    "    return np.stack(results) # (n_datasets, n_models, 4, n_folds)\n",
    "\n",
    "\n",
    "def results_to_df(\n",
    "        models = [\"End2End\", \"Ridge\", \"XGBoostRegressor\", \n",
    "                  \"GradientRFRBoost\", \"GradientRFRBoostID\", \n",
    "                  \"GreedyRFRBoostDense\", \"GreedyRFRBoostDiag\", \"GreedyRFRBoostScalar\",\n",
    "                  \"RandomFeatureNetwork\"],\n",
    "        datasets = openML_reg_ids[:],\n",
    "        save_dir = \"/home/nikita/Code/random-feature-boosting/save/OpenMLRegression/\",\n",
    "        ):\n",
    "    # Load and join the JSON data\n",
    "    results_json = get_joined_results_json(models, datasets, save_dir)\n",
    "    results = join_jsons_into_array(results_json) # (n_datasets, n_models, 4, n_folds)\n",
    "    \n",
    "    # Calculate means and stds across folds\n",
    "    results_mean = np.mean(results, axis=-1)  # (n_datasets, n_models, 4)\n",
    "    results_std = np.std(results, axis=-1)    # (n_datasets, n_models, 4)\n",
    "    \n",
    "    # Create a dictionary to hold both mean and std DataFrames\n",
    "    metrics = [\"score_train\", \"score_test\", \"t_fit\", \"t_inference\"]\n",
    "    metric_dfs = {}\n",
    "    \n",
    "    # Initialize DataFrames for both mean and std metrics\n",
    "    for metric in metrics:\n",
    "        metric_dfs[metric] = pd.DataFrame(index=datasets, columns=models)\n",
    "        metric_dfs[f\"{metric}_std\"] = pd.DataFrame(index=datasets, columns=models)\n",
    "    \n",
    "    # Populate the DataFrames for each metric\n",
    "    for dataset_idx, dataset in enumerate(datasets):\n",
    "        for model_idx, model in enumerate(models):\n",
    "            for metric_idx, metric in enumerate(metrics):\n",
    "                # Set mean value\n",
    "                metric_dfs[metric].loc[dataset, model] = results_mean[dataset_idx, model_idx, metric_idx]\n",
    "                # Set* std value\n",
    "                metric_dfs[f\"{metric}_std\"].loc[dataset, model] = results_std[dataset_idx, model_idx, metric_idx]\n",
    "    \n",
    "    return metric_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(35):\n",
    "    for model in [\"End2End\", \"Ridge\", #\"XGBoostRegressor\", \n",
    "                  \"GradientRFRBoost\", \"GradientRFRBoostID\", \n",
    "                  \"GradientRFRBoost_upscaleiid\",\n",
    "                  \"GreedyRFRBoostDense\", \"GreedyRFRBoostDiag\", \"GreedyRFRBoostScalar\",\n",
    "                  \"GreedyRFRBoostDense_upscaleiid\", \"GreedyRFRBoostDiag_upscaleiid\", \"GreedyRFRBoostScalar_upscaleiid\",\n",
    "                  \"RandomFeatureNetwork\",\n",
    "                  \"RandomFeatureNetwork_iid\"]:\n",
    "        try:\n",
    "            results = results_to_df(models=[model], datasets=[openML_reg_ids[i]])\n",
    "            # print(results)\n",
    "        except:\n",
    "            print(f\"Failed for {model} on {i}, ie {openML_reg_ids[i]}\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = results_to_df(        \n",
    "#     datasets = openML_reg_ids_noCat[[0,1,2,3,4,5,6,7,8,9,10,     13,14,15,16,17,18,19]],\n",
    "#     save_dir = \"/home/nikita/Code/random-feature-boosting/save/regv2_added40nlayers/OpenMLRegression/\",\n",
    "#     )\n",
    "df = results_to_df(        \n",
    "    models = [\"End2End\", \"Ridge\", #\"XGBoostRegressor\", \n",
    "                  #\"GradientRFRBoost\", \"GradientRFRBoostID\", \n",
    "                  \"GradientRFRBoost_upscaleiid\",\n",
    "                  #\"GreedyRFRBoostDense\", \"GreedyRFRBoostDiag\", \"GreedyRFRBoostScalar\",\n",
    "                  \"GreedyRFRBoostDense_upscaleiid\", \"GreedyRFRBoostDiag_upscaleiid\", \"GreedyRFRBoostScalar_upscaleiid\",\n",
    "                  #\"RandomFeatureNetwork\",\n",
    "                  \"RandomFeatureNetwork_iid\"],\n",
    "    datasets = openML_reg_ids[:],\n",
    "    save_dir = \"/home/nikita/Code/random-feature-boosting/save/OpenMLRegression/\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientRFRBoost_upscaleiid        0.409032\n",
       "End2End                              0.4101\n",
       "GreedyRFRBoostDense_upscaleiid     0.411854\n",
       "GreedyRFRBoostDiag_upscaleiid       0.43373\n",
       "GreedyRFRBoostScalar_upscaleiid    0.456377\n",
       "RandomFeatureNetwork_iid           0.494089\n",
       "Ridge                              0.529513\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"score_test\"].mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "End2End                            0.307045\n",
       "GradientRFRBoost_upscaleiid        0.357332\n",
       "GreedyRFRBoostDense_upscaleiid     0.362053\n",
       "GreedyRFRBoostDiag_upscaleiid      0.388637\n",
       "GreedyRFRBoostScalar_upscaleiid    0.414305\n",
       "RandomFeatureNetwork_iid           0.474475\n",
       "Ridge                              0.522192\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"score_train\"].mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientRFRBoost_upscaleiid        2.400000\n",
       "GreedyRFRBoostDense_upscaleiid     2.400000\n",
       "End2End                            3.542857\n",
       "GreedyRFRBoostDiag_upscaleiid      3.685714\n",
       "GreedyRFRBoostScalar_upscaleiid    4.657143\n",
       "RandomFeatureNetwork_iid           5.371429\n",
       "Ridge                              5.942857\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"score_test\"].rank(axis=1).mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GreedyRFRBoostScalar_upscaleiid     0\n",
       "RandomFeatureNetwork_iid            1\n",
       "GreedyRFRBoostDiag_upscaleiid       2\n",
       "Ridge                               5\n",
       "GreedyRFRBoostDense_upscaleiid      6\n",
       "End2End                             9\n",
       "GradientRFRBoost_upscaleiid        12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of first places\n",
    "(df[\"score_test\"].rank(axis=1) == 1).sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = results_to_df(        \n",
    "#     datasets = openML_reg_ids_noCat[[0,1,2  ,4,5,6,7,8,9,10,     13,14,15,16,17,18,19]],\n",
    "#     save_dir = \"/home/nikita/Code/random-feature-boosting/save/regv2_added40nlayers/OpenMLRegression/\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2[\"score_test\"].mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2[\"score_train\"].mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2[\"score_test\"].rank(axis=1).mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of first places\n",
    "# (df2[\"score_test\"].rank(axis=1) == 1).sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoostRegressor       0.389427\n",
    "# End2End                0.408184\n",
    "# GreedyRFBoostDense      0.43249\n",
    "# GradientRFBoost        0.434696\n",
    "# GradientRFBoostID      0.436364\n",
    "# GreedyRFBoostDiag      0.445675\n",
    "# GreedyRFBoostScalar    0.528314\n",
    "# Ridge                  0.606384\n",
    "# RidgeCV                0.606385\n",
    "# dtype: object\n",
    "\n",
    "\n",
    "# XGBoostRegressor       0.170585\n",
    "# End2End                0.345058\n",
    "# GreedyRFBoostDense     0.387421\n",
    "# GradientRFBoost        0.400575\n",
    "# GradientRFBoostID      0.405132\n",
    "# GreedyRFBoostDiag       0.41455\n",
    "# GreedyRFBoostScalar    0.510962\n",
    "# RidgeCV                0.600331\n",
    "# Ridge                   0.60034\n",
    "# dtype: object\n",
    "\n",
    "\n",
    "# XGBoostRegressor       2.666667\n",
    "# End2End                3.333333\n",
    "# GreedyRFBoostDense     3.500000\n",
    "# GradientRFBoost        3.611111\n",
    "# GradientRFBoostID      4.111111\n",
    "# GreedyRFBoostDiag      4.777778\n",
    "# GreedyRFBoostScalar    6.722222\n",
    "# Ridge                  8.055556\n",
    "# RidgeCV                8.222222\n",
    "# dtype: float64\n",
    "\n",
    "\n",
    "# Ridge                   0\n",
    "# RidgeCV                 0\n",
    "# GradientRFBoostID       0\n",
    "# GreedyRFBoostScalar     0\n",
    "# GradientRFBoost         1\n",
    "# GreedyRFBoostDense      1\n",
    "# GreedyRFBoostDiag       1\n",
    "# End2End                 4\n",
    "# XGBoostRegressor       11\n",
    "# dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less5000 = df_metadata.query(\"n_obs < 5000\").index\n",
    "less1000 = df_metadata.query(\"n_obs < 1000\").index\n",
    "less5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score_test\"].loc[less5000].mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score_test\"].loc[less5000].rank(axis=1).mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score_test\"].loc[less5000].rank(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score_test\"].loc[less1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at distribution of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_param_distribution(\n",
    "        models = [\"End2End\", \"Ridge\", #\"XGBoostRegressor\", \n",
    "                  #\"GradientRFRBoost\", \"GradientRFRBoostID\", \n",
    "                  \"GradientRFRBoost_upscaleiid\",\n",
    "                  #\"GreedyRFRBoostDense\", \"GreedyRFRBoostDiag\", \"GreedyRFRBoostScalar\",\n",
    "                  \"GreedyRFRBoostDense_upscaleiid\", \"GreedyRFRBoostDiag_upscaleiid\", \"GreedyRFRBoostScalar_upscaleiid\",\n",
    "                  #\"RandomFeatureNetwork\",\n",
    "                  \"RandomFeatureNetwork_iid\"],\n",
    "        datasets = openML_reg_ids[:],\n",
    "        save_dir = \"/home/nikita/Code/random-feature-boosting/save/OpenMLRegression/\",\n",
    "        # save_dir = \"/home/nikita/Code/random-feature-boosting/save/regv2_added40nlayers/OpenMLRegression/\",\n",
    "        ):\n",
    "    # Load and join the JSON data\n",
    "    results_json = get_joined_results_json(models, datasets, save_dir)\n",
    "\n",
    "    # model: list_of_param_names\n",
    "    modelwise_param_names = {model: list(results_json[str(datasets[0])][model]['hyperparams'][0])\n",
    "                            for model in models} \n",
    "\n",
    "    # model: param_name: list_of_param_values\n",
    "    param_distribution = { model: {param: [] for param in param_names}\n",
    "                          for model, param_names in modelwise_param_names.items()}\n",
    "\n",
    "    #populate teh param_districution nested dict\n",
    "    for dataset, dataset_results in results_json.items():\n",
    "        for model_name, model_results in dataset_results.items():\n",
    "            for fold in model_results[\"hyperparams\"]:\n",
    "                for param_name, param_val in fold.items():\n",
    "                    param_distribution[model_name][param_name].append(param_val)\n",
    "\n",
    "    # For each model, plot the distribution of each parameter\n",
    "    for model, param_dict in param_distribution.items():\n",
    "        print(f\"Model: {model}\")\n",
    "        for param_name, param_values in param_dict.items():\n",
    "            if param_name not in [\n",
    "                \"out_dim\", \"loss\", \"objective\", \"feature_type\",\n",
    "                \"upscale_type\", \"sandwich_solver\"\n",
    "                ]:\n",
    "                # Create figure with two subplots side by side\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "                \n",
    "                # Linear scale plot\n",
    "                ax1.hist(param_values, bins=20)\n",
    "                ax1.set_title(f\"{model} {param_name}\\n(linear scale)\")\n",
    "                ax1.set_xlabel(param_name)\n",
    "                ax1.set_ylabel(\"Count\")\n",
    "                \n",
    "                # Log scale plot\n",
    "                min_val = np.min(param_values)  # Avoid log(0)\n",
    "                max_val = np.max(param_values)\n",
    "                bins = np.logspace(np.log10(min_val), np.log10(max_val), 20)\n",
    "                ax2.hist(param_values, bins=bins)\n",
    "                ax2.set_xscale('log')\n",
    "                ax2.set_title(f\"{model} {param_name}\\n(log scale)\")\n",
    "                ax2.set_xlabel(param_name)\n",
    "                ax2.set_ylabel(\"Count\")\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "plot_param_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable, Type\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "\n",
    "from optuna_kfoldCV import evaluate_pytorch_model_kfoldcv, evaluate_dataset_with_model, pytorch_load_openml_dataset\n",
    "from models.models import GreedyRFBoostRegressor, GreedyRFBoostRegressor_ScalarDiagDelta, GradientRFBoostRegressor\n",
    "from regression_param_specs import evaluate_Ridge\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_all, y_all = pytorch_load_abalone()\n",
    "device = \"cuda\"\n",
    "X_all, y_all = pytorch_load_openml_dataset(44975, \"regression\", device)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost:\n",
    "from models.models import XGBoostRegressorWrapper, RidgeCVModule, End2EndMLPResNet\n",
    "\n",
    "model = XGBoostRegressorWrapper(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.3,\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model(X_train)\n",
    "y_pred_test = model(X_test)\n",
    "\n",
    "print(\"XGBoost\")\n",
    "print(f\"Train RMSE: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "print(f\"Test RMSE {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# RidgeCV:\n",
    "model = RidgeCVModule(\n",
    "    lower_alpha=1e-6, upper_alpha=10, n_alphas=10\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model(X_train)\n",
    "y_pred_test = model(X_test)\n",
    "\n",
    "print(\"RidgeCV\")\n",
    "print(f\"Train RMSE: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "print(f\"Test RMSE {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End2EndMLPResNet:\n",
    "model = End2EndMLPResNet(\n",
    "    in_dim = X_train.shape[1],\n",
    "    hidden_dim = X_train.shape[1],\n",
    "    bottleneck_dim = 32,\n",
    "    out_dim = 1,\n",
    "    n_blocks = 4,\n",
    "    lr = 0.1,\n",
    "    end_lr_factor = 0.001,\n",
    "    n_epochs = 20,\n",
    "    weight_decay = 0.001,\n",
    "    batch_size = 512\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model(X_train)\n",
    "y_pred_test = model(X_test)\n",
    "\n",
    "print(\"End2End\")\n",
    "print(f\"Train RMSE: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "print(f\"Test RMSE {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a model on a specific dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientRFBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models import FittableModule, create_layer, Identity, fit_ridge_ALOOCV, sandwiched_LS_scalar\n",
    "\n",
    "\n",
    "n_layers = 40\n",
    "model = GradientRFBoostRegressor(\n",
    "    hidden_dim = X_train.shape[1],\n",
    "    randfeat_xt_dim = 512,\n",
    "    randfeat_x0_dim = 512,\n",
    "    n_layers = n_layers,\n",
    "    feature_type = \"SWIM\",\n",
    "    boost_lr = 1.0,\n",
    "    upscale = \"identity\",\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "rmse_test = F.mse_loss(model(X_test), y_test).sqrt()\n",
    "print(f\"GradientRFBoostRegressor: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_results_for_every_layer(X_train, X_test):\n",
    "    with torch.no_grad():\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "\n",
    "        if model.upscale is not None:\n",
    "            X_train = model.upscale_fun(X0_train)\n",
    "            X_test = model.upscale_fun(X0_test)\n",
    "\n",
    "        y_pred_train = X_train @ model.Ws[0] + model.bs[0]\n",
    "        y_pred_test = X_test @ model.Ws[0] + model.bs[0]\n",
    "        print(f\"Train RMSE at layer 0: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "        print(f\"Test RMSE at layer 0: {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "        print()\n",
    "        \n",
    "        for t, (fxt_fun, fx0_fun, (Delta, Delta_b)) in enumerate(zip(model.layers_fxt, model.layers_fx0, model.deltas)):\n",
    "            features_train = torch.cat([fxt_fun(X_train), fx0_fun(X0_train)], dim=1)\n",
    "            features_test = torch.cat([fxt_fun(X_test), fx0_fun(X0_test)], dim=1)\n",
    "            X_train = X_train + model.boost_lr * (features_train @ Delta + Delta_b)\n",
    "            X_test = X_test + model.boost_lr * (features_test @ Delta + Delta_b)\n",
    "\n",
    "            y_pred_train = X_train @ model.Ws[t+1] + model.bs[t+1]\n",
    "            y_pred_test = X_test @ model.Ws[t+1] + model.bs[t+1]\n",
    "\n",
    "            print(f\"Train RMSE at layer {t+1}: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "            print(f\"Test RMSE at layer {t+1}: {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "see_results_for_every_layer(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly better test results by tuning l2_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense sandwich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 40\n",
    "model = GreedyRFBoostRegressor(\n",
    "    hidden_dim = 100,\n",
    "    randfeat_xt_dim = 256,\n",
    "    randfeat_x0_dim = 256,\n",
    "    n_layers = n_layers,\n",
    "    feature_type = \"SWIM\",\n",
    "    boost_lr = 0.9,\n",
    "    upscale = \"identity\",\n",
    "    l2_reg_sandwich = 0.00000001,\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_rmse = F.mse_loss(model(X_test), y_test).sqrt()\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_results_for_every_layer_greedy(X_train, X_test):\n",
    "    with torch.no_grad():\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "\n",
    "        if model.upscale is not None:\n",
    "            X_train = model.upscale_fun(X0_train)\n",
    "            X_test = model.upscale_fun(X0_test)\n",
    "        \n",
    "        y_pred_train = X_train @ model.Ws[0] + model.bs[0]\n",
    "        y_pred_test = X_test @ model.Ws[0] + model.bs[0]\n",
    "        print(f\"Train RMSE at layer 0: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "        print(f\"Test RMSE at layer 0: {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "        print()\n",
    "\n",
    "        for t, (fxt_fun, fx0_fun, Delta) in enumerate(zip(model.layers_fxt, model.layers_fx0, model.deltas)):\n",
    "            features_train = torch.cat([fxt_fun(X_train), fx0_fun(X0_train)], dim=1)\n",
    "            features_test = torch.cat([fxt_fun(X_test), fx0_fun(X0_test)], dim=1)\n",
    "            X_train = X_train + model.boost_lr * (features_train @ Delta)\n",
    "            X_test = X_test + model.boost_lr * (features_test @ Delta)\n",
    "\n",
    "            y_pred_train = X_train @ model.Ws[t+1] + model.bs[t+1]\n",
    "            y_pred_test = X_test @ model.Ws[t+1] + model.bs[t+1]\n",
    "\n",
    "            print(f\"Train RMSE at layer {t+1}: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "            print(f\"Test RMSE at layer {t+1}: {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "see_results_for_every_layer_greedy(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diag sandwich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 40\n",
    "model = GreedyRFBoostRegressor_ScalarDiagDelta(\n",
    "    hidden_dim = 512,\n",
    "    n_layers = n_layers,\n",
    "    feature_type = \"SWIM\",\n",
    "    boost_lr = 0.9,\n",
    "    upscale = \"dense\",\n",
    "    l2_reg_sandwich = 0.0000001,\n",
    "    sandwich_solver=\"diag\",\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_rmse = F.mse_loss(model(X_test), y_test).sqrt()\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_results_for_every_layer_diag_scalar(X_train, X_test):\n",
    "    with torch.no_grad():\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "\n",
    "        if model.upscale is not None:\n",
    "            X_train = model.upscale_fun(X0_train)\n",
    "            X_test = model.upscale_fun(X0_test)\n",
    "\n",
    "        y_pred_train = X_train @ model.Ws[0] + model.bs[0]\n",
    "        y_pred_test = X_test @ model.Ws[0] + model.bs[0]\n",
    "        print(f\"Train RMSE at layer 0: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "        print(f\"Test RMSE at layer 0: {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "        print()\n",
    "\n",
    "        for t, (layer, Delta) in enumerate(zip(model.layers, model.deltas)):\n",
    "            features_train = layer(torch.cat([X_train, X0_train], dim=1))\n",
    "            features_test = layer(torch.cat([X_test, X0_test], dim=1))\n",
    "            X_train = X_train + model.boost_lr * model.XDelta_op(features_train, Delta)\n",
    "            X_test = X_test + model.boost_lr * model.XDelta_op(features_test, Delta)\n",
    "\n",
    "            y_pred_train = X_train @ model.Ws[t+1] + model.bs[t+1]\n",
    "            y_pred_test = X_test @ model.Ws[t+1] + model.bs[t+1]\n",
    "\n",
    "            print(f\"Train RMSE at layer {t+1}: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "            print(f\"Test RMSE at layer {t+1}: {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "see_results_for_every_layer_diag_scalar(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar sandwich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 40\n",
    "model = GreedyRFBoostRegressor_ScalarDiagDelta(\n",
    "    hidden_dim = 512,\n",
    "    n_layers = n_layers,\n",
    "    feature_type = \"SWIM\",\n",
    "    boost_lr = 0.9,\n",
    "    upscale = \"dense\",\n",
    "    l2_reg_sandwich = 0.000001,\n",
    "    sandwich_solver=\"scalar\",\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_rmse = F.mse_loss(model(X_test), y_test).sqrt()\n",
    "print(test_rmse)\n",
    "\n",
    "\n",
    "see_results_for_every_layer_diag_scalar(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoosted randFeats --- better! (i hope also after tuning params)\n",
    "\n",
    "# XGBoost:\n",
    "from models.models import XGBoostRegressorWrapper, RidgeCVModule, End2EndMLPResNet\n",
    "from models.models import FittableModule, create_layer, Identity, fit_ridge_ALOOCV, sandwiched_LS_scalar\n",
    "\n",
    "\n",
    "n_layers = 20\n",
    "model = GradientRFBoostRegressor(\n",
    "    hidden_dim = X_train.shape[1],\n",
    "    randfeat_xt_dim = 512,\n",
    "    randfeat_x0_dim = 512,\n",
    "    n_layers = n_layers,\n",
    "    feature_type = \"SWIM\",\n",
    "    boost_lr = 0.7,\n",
    "    upscale = \"identity\",\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "rmse_test = F.mse_loss(model(X_test), y_test).sqrt()\n",
    "print(f\"GradientRFBoostRegressor: {rmse_test}\")\n",
    "\n",
    "def get_boosted_features(model, X_train, X_test):\n",
    "    with torch.no_grad():\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "\n",
    "        if model.upscale is not None:\n",
    "            X_train = model.upscale_fun(X0_train)\n",
    "            X_test = model.upscale_fun(X0_test)\n",
    "\n",
    "        y_pred_train = X_train @ model.Ws[0] + model.bs[0]\n",
    "        y_pred_test = X_test @ model.Ws[0] + model.bs[0]\n",
    "        print(f\"Train RMSE at layer 0: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "        print(f\"Test RMSE at layer 0: {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "        print()\n",
    "        \n",
    "        for t, (fxt_fun, fx0_fun, (Delta, Delta_b)) in enumerate(zip(model.layers_fxt, model.layers_fx0, model.deltas)):\n",
    "            features_train = torch.cat([fxt_fun(X_train), fx0_fun(X0_train)], dim=1)\n",
    "            features_test = torch.cat([fxt_fun(X_test), fx0_fun(X0_test)], dim=1)\n",
    "            X_train = X_train + model.boost_lr * (features_train @ Delta + Delta_b)\n",
    "            X_test = X_test + model.boost_lr * (features_test @ Delta + Delta_b)\n",
    "\n",
    "            y_pred_train = X_train @ model.Ws[t+1] + model.bs[t+1]\n",
    "            y_pred_test = X_test @ model.Ws[t+1] + model.bs[t+1]\n",
    "\n",
    "            print(f\"Train RMSE at layer {t+1}: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "            print(f\"Test RMSE at layer {t+1}: {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "            print()\n",
    "    return X_train, X_test\n",
    "\n",
    "xgb = XGBoostRegressorWrapper(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.3,\n",
    ")\n",
    "boosted_features_train, boosted_features_test = get_boosted_features(model, X_train, X_test)\n",
    "xgb.fit(boosted_features_train, y_train)\n",
    "y_pred_train = xgb(boosted_features_train)\n",
    "y_pred_test = xgb(boosted_features_test)\n",
    "\n",
    "print(\"XGBoost\")\n",
    "print(f\"Train RMSE: {F.mse_loss(y_pred_train, y_train).sqrt()}\")\n",
    "print(f\"Test RMSE {F.mse_loss(y_pred_test, y_test).sqrt()}\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

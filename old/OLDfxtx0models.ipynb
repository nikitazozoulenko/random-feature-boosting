{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable, Type\n",
    "import abc\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from models.ridge_ALOOCV import fit_ridge_ALOOCV\n",
    "from models.sandwiched_least_squares import sandwiched_LS_dense, sandwiched_LS_diag, sandwiched_LS_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make regression data X, y\n",
    "N = 1000\n",
    "N_test = 1000\n",
    "D = 10\n",
    "d = 3\n",
    "X = torch.randn(N, D)\n",
    "X_test = torch.randn(N_test, D)\n",
    "w_true = torch.randn(D, d)\n",
    "y = (X @ w_true)**2 + torch.randn(N, d) * 0.1  # Adding some noise\n",
    "y_test = (X_test @ w_true)**2 + torch.randn(N_test, d) * 0.1  # Adding some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(17.5370) std tensor(0.)\n",
      "test rmse tensor(16.4296) std tensor(0.)\n",
      "train tensor([17.5370, 17.5370, 17.5370, 17.5370, 17.5370, 17.5370, 17.5370, 17.5370,\n",
      "        17.5370, 17.5370])\n",
      "test tensor([16.4296, 16.4296, 16.4296, 16.4296, 16.4296, 16.4296, 16.4296, 16.4296,\n",
      "        16.4296, 16.4296])\n"
     ]
    }
   ],
   "source": [
    "from models.models import RidgeCVModule\n",
    "\n",
    "#dense      \n",
    "model = RidgeCVModule(\n",
    "        lower_alpha=1e-6,\n",
    "        upper_alpha=1e6,\n",
    "        n_alphas=10,\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(4.8036) std tensor(0.9972)\n",
      "test rmse tensor(6.3538) std tensor(1.4504)\n",
      "train tensor([4.2472, 5.7213, 3.8636, 5.3439, 6.8142, 4.6236, 5.3510, 4.5071, 3.5925,\n",
      "        3.9714])\n",
      "test tensor([5.6311, 7.1638, 4.7403, 6.8273, 9.6347, 6.3930, 6.7078, 6.2518, 4.4417,\n",
      "        5.7469])\n"
     ]
    }
   ],
   "source": [
    "from models.models import GreedyRandFeatBoostRegression\n",
    "\n",
    "#dense      \n",
    "model = GreedyRandFeatBoostRegression(\n",
    "     hidden_dim=128, \n",
    "     bottleneck_dim=128, \n",
    "     out_dim=d, \n",
    "     n_layers=5, \n",
    "     l2_reg=0.1, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\", \n",
    "     sandwich_solver=\"dense\"\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(2.7414) std tensor(0.7641)\n",
      "test rmse tensor(3.6085) std tensor(0.7143)\n",
      "train tensor([2.5947, 2.1840, 3.1344, 2.2468, 2.1898, 3.9471, 2.1478, 1.7762, 3.4990,\n",
      "        3.6942])\n",
      "test tensor([3.5613, 2.9309, 4.1656, 3.0828, 3.1777, 4.7995, 2.9935, 2.8063, 4.3402,\n",
      "        4.2271])\n"
     ]
    }
   ],
   "source": [
    "#diag\n",
    "model = GreedyRandFeatBoostRegression(\n",
    "     hidden_dim=512, \n",
    "     bottleneck_dim=512, \n",
    "     out_dim=d, \n",
    "     n_layers=3, \n",
    "     l2_reg=0.1, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\", \n",
    "     sandwich_solver=\"diag\"\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(3.1384) std tensor(3.2530)\n",
      "test rmse tensor(4.0387) std tensor(2.9482)\n",
      "train tensor([ 1.2765,  1.7262,  1.1376, 11.4514,  5.5821,  1.4210,  1.2141,  1.1346,\n",
      "         3.2988,  3.1421])\n",
      "test tensor([ 2.1791,  2.8977,  2.2073, 11.5325,  5.9572,  2.5953,  2.1592,  1.9598,\n",
      "         4.4281,  4.4707])\n"
     ]
    }
   ],
   "source": [
    "#scalar\n",
    "model = GreedyRandFeatBoostRegression(\n",
    "     hidden_dim=512, \n",
    "     bottleneck_dim=512, \n",
    "     out_dim=d, \n",
    "     n_layers=3, \n",
    "     l2_reg=0.0001, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\", \n",
    "     sandwich_solver=\"scalar\"\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(3.5296) std tensor(0.4859)\n",
      "test rmse tensor(5.3978) std tensor(0.6923)\n",
      "train tensor([4.6294, 3.4208, 3.5516, 2.9593, 3.5966, 3.9041, 3.6188, 3.0283, 3.4702,\n",
      "        3.1173])\n",
      "test tensor([6.8492, 5.0806, 5.6701, 4.6747, 5.6699, 5.6498, 5.9215, 4.5661, 4.9716,\n",
      "        4.9247])\n"
     ]
    }
   ],
   "source": [
    "from models.models import GradientRandFeatBoostRegression\n",
    "        \n",
    "model = GradientRandFeatBoostRegression(\n",
    "     hidden_dim=32, \n",
    "     bottleneck_dim=512, \n",
    "     out_dim=d, \n",
    "     n_layers=5, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\", \n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End2End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.93it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 12.71it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 12.49it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 12.47it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(2.6174) std tensor(1.8957)\n",
      "test rmse tensor(2.8031) std tensor(1.5548)\n",
      "train tensor([1.1134, 4.5657, 1.3029, 4.8146, 1.2905])\n",
      "test tensor([1.6028, 3.8519, 1.7320, 5.0321, 1.7966])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from models.models import End2EndMLPResNet\n",
    "\n",
    "model = End2EndMLPResNet(\n",
    "        in_dim=D,\n",
    "        hidden_dim=128,\n",
    "        bottleneck_dim=32,\n",
    "        out_dim=d,\n",
    "        n_blocks=3,\n",
    "        loss = \"mse\",\n",
    "        lr = 0.1,\n",
    "        n_epochs = 30,\n",
    "        end_lr_factor= 0.1,\n",
    "        weight_decay = 0.001,\n",
    "        batch_size = 64,\n",
    "        )\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next model f(x_t, x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(1.3204) std tensor(0.4021)\n",
      "test rmse tensor(2.7518) std tensor(0.5856)\n",
      "train tensor([1.7848, 1.7316, 1.0005, 1.0945, 0.9907])\n",
      "test tensor([3.3208, 3.4563, 2.3362, 2.2485, 2.3973])\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable, Type\n",
    "import abc\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "\n",
    "from models.ridge_ALOOCV import fit_ridge_ALOOCV\n",
    "from models.sandwiched_least_squares import sandwiched_LS_dense, sandwiched_LS_diag, sandwiched_LS_scalar\n",
    "from models.models import FittableModule, create_layer\n",
    "\n",
    "\n",
    "class GradientRandFeatBoostRegression_fxtx0(FittableModule):\n",
    "    def __init__(self, \n",
    "                 hidden_dim: int = 128,\n",
    "                 bottleneck_dim: int = 128,\n",
    "                 out_dim: int = 1,\n",
    "                 n_layers: int = 5,\n",
    "                 activation: nn.Module = nn.Tanh(),\n",
    "                #  l2_reg: float = 0.01,   #TODO ALOOCV or fixed l2_reg\n",
    "                 feature_type = \"SWIM\", # \"dense\", identity\n",
    "                 boost_lr: float = 1.0,\n",
    "                 upscale: Optional[str] = \"dense\",\n",
    "                 concat_phi_t_x0: bool = False,\n",
    "                 ):\n",
    "        super(GradientRandFeatBoostRegression_fxtx0, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bottleneck_dim = bottleneck_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        # self.l2_reg = l2_reg\n",
    "        self.feature_type = feature_type\n",
    "        self.boost_lr = boost_lr\n",
    "        self.upscale = upscale\n",
    "        self.concat_phi_t_x0 = concat_phi_t_x0\n",
    "\n",
    "\n",
    "    def fit(self, X: Tensor, y: Tensor):\n",
    "        in_dim = X.shape[1]\n",
    "        with torch.no_grad():\n",
    "            X0 = X\n",
    "            #optional upscale\n",
    "            if self.upscale == \"dense\":\n",
    "                self.upscale_fun = create_layer(self.upscale, X.shape[1], self.hidden_dim, None)\n",
    "                X = self.upscale_fun.fit_transform(X, y)\n",
    "            elif self.upscale == \"SWIM\":\n",
    "                self.upscale_fun = create_layer(self.upscale, X.shape[1], self.hidden_dim, self.activation)\n",
    "                X = self.upscale_fun.fit_transform(X, y)\n",
    "\n",
    "            # Create regressor W_0\n",
    "            self.W, self.b, _ = fit_ridge_ALOOCV(X, y)\n",
    "            self.layers = []\n",
    "            self.deltas = []\n",
    "\n",
    "            # Layerwise boosting\n",
    "            N = X.size(0)\n",
    "            for t in range(self.n_layers):\n",
    "                Xt0 = X\n",
    "                #Step 0: do we want f(x_t) or f(x_t, x_0)?\n",
    "                if self.concat_phi_t_x0:\n",
    "                    X = torch.cat([X, X0], dim=1) \n",
    "                    X = (X-X.mean(dim=0, keepdim=True)) / torch.std(X, dim=0, keepdim=True) #TODO keep normalization?\n",
    "                else:\n",
    "                    in_dim = 0 # hack\n",
    "\n",
    "                # Step 1: Create random feature layer\n",
    "                layer = create_layer(self.feature_type, self.hidden_dim+in_dim, self.bottleneck_dim, self.activation)\n",
    "                F = layer.fit_transform(X, y)\n",
    "\n",
    "                # Step 2: Obtain activation gradient and learn Delta\n",
    "                # X shape (N, D) --- ResNet neurons\n",
    "                # F shape (N, p) --- random features\n",
    "                # y shape (N, d) --- target\n",
    "                # W shape (D, d) --- top level classifier\n",
    "                # G shape (N, D) --- gradient of neurons\n",
    "                # r shape (N, d) --- residual at currect boosting iteration\n",
    "\n",
    "                r = y - Xt0 @ self.W - self.b\n",
    "                G = r @ self.W.T\n",
    "                \n",
    "                # fit to negative gradient (finding functional direction)\n",
    "                Delta, Delta_b, _ = fit_ridge_ALOOCV(F, G)\n",
    "                Ghat = F @ Delta + Delta_b\n",
    "\n",
    "                # Line search closed form risk minimization of R(W_t, Phi_{t+1})\n",
    "                linesearch = sandwiched_LS_scalar(r, self.W, Ghat, 0.00001)\n",
    "\n",
    "\n",
    "                # Step 3: Learn top level classifier\n",
    "                X = Xt0 + self.boost_lr * linesearch * Ghat\n",
    "                self.W, self.b, _ = fit_ridge_ALOOCV(X, y)\n",
    "\n",
    "                #update Delta scale\n",
    "                Delta = Delta * linesearch\n",
    "                Delta_b = Delta_b * linesearch\n",
    "\n",
    "                # store\n",
    "                self.layers.append(layer)\n",
    "                self.deltas.append((Delta, Delta_b))\n",
    "\n",
    "            return X @ self.W + self.b\n",
    "        \n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            X0 = X\n",
    "            if self.upscale is not None:\n",
    "                X = self.upscale_fun(X)\n",
    "            for layer, (Delta, Delta_b) in zip(self.layers, self.deltas):\n",
    "                Xt0 = X\n",
    "                if self.concat_phi_t_x0:\n",
    "                    X = torch.cat([X, X0], dim=1) \n",
    "                    X = (X-X.mean(dim=0, keepdim=True)) / torch.std(X, dim=0, keepdim=True) #TODO keep normalization?\n",
    "                X = Xt0 + self.boost_lr * (layer(X) @ Delta + Delta_b)\n",
    "            return X @ self.W + self.b\n",
    "        \n",
    "\n",
    "model = GradientRandFeatBoostRegression_fxtx0(\n",
    "     hidden_dim=32, \n",
    "     bottleneck_dim=1000, \n",
    "     out_dim=d, \n",
    "     n_layers=10, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\",\n",
    "     concat_phi_t_x0=True,\n",
    "     boost_lr=1.0\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# another option: concat[f(x_t), h(x_0)]. Do this next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO batch normalization??? Wolfe-Franke???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.009999999776482582\n",
      "linesearch tensor(0.2441)\n",
      "alpha 0.009999999776482582\n",
      "linesearch tensor(3.3613e-10)\n",
      "alpha 0.0010000000474974513\n",
      "linesearch tensor(5.0127e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(6.8426e-08)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.6636e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.4780e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(5.4926e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.7899e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(4.7869e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(6.2879e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(7.5734e-08)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.7390e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(2.2127e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.1747e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.3316e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(7.7201e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.2717e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.9584e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.3475e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.0122e-05)\n",
      "alpha 0.009999999776482582\n",
      "linesearch tensor(0.2619)\n",
      "alpha 0.10000000149011612\n",
      "linesearch tensor(0.0004)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.4803e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.8711e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(4.9972e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(6.4664e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(2.5182e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(7.2413e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(8.1470e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.4368e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.7790e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(5.5605e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.7673e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.1526e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(0.0004)\n",
      "alpha 10.0\n",
      "linesearch tensor(9.3279e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(0.0003)\n",
      "alpha 10.0\n",
      "linesearch tensor(8.3744e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(0.0002)\n",
      "alpha 10.0\n",
      "linesearch tensor(0.0003)\n",
      "alpha 0.0010000000474974513\n",
      "linesearch tensor(0.1869)\n",
      "alpha 0.009999999776482582\n",
      "linesearch tensor(6.7496e-07)\n",
      "alpha 0.10000000149011612\n",
      "linesearch tensor(7.3657e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(4.3727e-08)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.5170e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.1342e-07)\n",
      "alpha 1.0\n",
      "linesearch tensor(1.2329e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(6.1201e-08)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.1136e-08)\n",
      "alpha 10.0\n",
      "linesearch tensor(2.5040e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(5.0479e-07)\n",
      "alpha 1.0\n",
      "linesearch tensor(1.6895e-06)\n",
      "alpha 1.0\n",
      "linesearch tensor(2.1249e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.6968e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(6.8139e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(6.7429e-08)\n",
      "alpha 10.0\n",
      "linesearch tensor(5.4570e-08)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.4105e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.2692e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(5.1191e-07)\n",
      "alpha 0.009999999776482582\n",
      "linesearch tensor(0.3025)\n",
      "alpha 0.009999999776482582\n",
      "linesearch tensor(0.0001)\n",
      "alpha 10.0\n",
      "linesearch tensor(7.7508e-08)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.0395e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.4887e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(8.2654e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.0881e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.6708e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(8.9891e-08)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.6611e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.1647e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.8022e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(8.3321e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(5.2994e-08)\n",
      "alpha 10.0\n",
      "linesearch tensor(9.5044e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.2461e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(2.5607e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.0661e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.2313e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.4255e-06)\n",
      "alpha 0.0010000000474974513\n",
      "linesearch tensor(0.2270)\n",
      "alpha 0.0010000000474974513\n",
      "linesearch tensor(0.0004)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.1050e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(4.3133e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(9.7149e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.3662e-07)\n",
      "alpha 10.0\n",
      "linesearch tensor(2.9129e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(0.0001)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.1914e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(7.8143e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.9978e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.1432e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(0.0002)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.6578e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(2.5170e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(3.2162e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(5.1064e-05)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.7210e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(1.6529e-06)\n",
      "alpha 10.0\n",
      "linesearch tensor(2.7446e-05)\n",
      "train rmse tensor(0.6378) std tensor(0.1157)\n",
      "test rmse tensor(1.4909) std tensor(0.1492)\n",
      "train tensor([0.7082, 0.7169, 0.6248, 0.6979, 0.4414])\n",
      "test tensor([1.5842, 1.5577, 1.5719, 1.5123, 1.2284])\n"
     ]
    }
   ],
   "source": [
    "class CONCATFEATGradientRandFeatBoostRegression_fxtx0(FittableModule):\n",
    "    def __init__(self, \n",
    "                 hidden_dim: int = 128,\n",
    "                 randfeat_xt_dim: int = 128,\n",
    "                 randfeat_x0_dim: int = 128,\n",
    "                 out_dim: int = 1,\n",
    "                 n_layers: int = 5,\n",
    "                 activation: nn.Module = nn.Tanh(),\n",
    "                 #  l2_reg: float = 0.01,   #TODO ALOOCV or fixed l2_reg\n",
    "                 feature_type = \"SWIM\", # \"dense\", identity\n",
    "                 boost_lr: float = 1.0,\n",
    "                 upscale: Optional[str] = \"dense\",\n",
    "                 ):\n",
    "        super(CONCATFEATGradientRandFeatBoostRegression_fxtx0, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.randfeat_xt_dim = randfeat_xt_dim\n",
    "        self.randfeat_x0_dim = randfeat_x0_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        # self.l2_reg = l2_reg\n",
    "        self.feature_type = feature_type\n",
    "        self.boost_lr = boost_lr\n",
    "        self.upscale = upscale\n",
    "\n",
    "\n",
    "    def fit(self, X: Tensor, y: Tensor):\n",
    "        with torch.no_grad():\n",
    "            X0 = X\n",
    "            #optional upscale\n",
    "            if self.upscale == \"dense\":\n",
    "                self.upscale_fun = create_layer(self.upscale, X.shape[1], self.hidden_dim, None)\n",
    "                X = self.upscale_fun.fit_transform(X, y)\n",
    "            elif self.upscale == \"SWIM\":\n",
    "                self.upscale_fun = create_layer(self.upscale, X.shape[1], self.hidden_dim, self.activation)\n",
    "                X = self.upscale_fun.fit_transform(X, y)\n",
    "\n",
    "            # Create regressor W_0\n",
    "            W, b, _ = fit_ridge_ALOOCV(X, y)\n",
    "            self.layers_fxt = []\n",
    "            self.layers_fx0 = []\n",
    "            self.deltas = []\n",
    "            self.Ws = [W]\n",
    "            self.bs = [b]\n",
    "\n",
    "            # Layerwise boosting\n",
    "            N = X.size(0)\n",
    "            for t in range(self.n_layers):\n",
    "                # X0 shape (N, raw_in_dim) --- raw input data\n",
    "                # X  shape (N, D) --- ResNet neurons\n",
    "                # F  shape (N, p) --- random features\n",
    "                # y  shape (N, d) --- target data\n",
    "                # W  shape (D, d) --- top level classifier\n",
    "                # G  shape (N, D) --- gradient of neurons\n",
    "                # r  shape (N, d) --- residual at currect boosting iteration\n",
    "                # Delta shape (p, D) --- random feature weights\n",
    "\n",
    "                # Step 1: Create random feature layer\n",
    "                fxt_fun = create_layer(self.feature_type, self.hidden_dim, self.randfeat_xt_dim, self.activation)\n",
    "                fx0_fun = create_layer(self.feature_type, X0.size(1), self.randfeat_x0_dim, self.activation)\n",
    "                Fxt = fxt_fun.fit_transform(X, y)\n",
    "                Fx0 = fx0_fun.fit_transform(X0, y)\n",
    "                F = torch.cat([Fxt, Fx0], dim=1)\n",
    "\n",
    "                # Step 2: Obtain activation gradient and learn Delta\n",
    "                r = y - X @ W - b\n",
    "                G = r @ W.T\n",
    "                Gnorm = torch.norm(G)\n",
    "                # fit to negative gradient (finding functional direction)\n",
    "                Delta, Delta_b, _ = fit_ridge_ALOOCV(F, G)\n",
    "                print(\"alpha\", _)\n",
    "                Ghat = (F @ Delta + Delta_b)\n",
    "                # line search closed form risk minimization of R(W_t, Phi_{t+1})\n",
    "                linesearch = sandwiched_LS_scalar(r, W, Ghat, 0.0001)\n",
    "                print(\"linesearch\", linesearch)\n",
    "\n",
    "\n",
    "                # Step 3: Learn top level classifier\n",
    "                X = X + self.boost_lr * linesearch * Ghat\n",
    "                W, b, _ = fit_ridge_ALOOCV(X, y)\n",
    "                # update Delta magnitude\n",
    "                Delta = Delta * linesearch\n",
    "                Delta_b = Delta_b * linesearch\n",
    "                # store\n",
    "                self.layers_fxt.append(fxt_fun)\n",
    "                self.layers_fx0.append(fx0_fun)\n",
    "                self.deltas.append((Delta, Delta_b))\n",
    "                self.Ws.append(W)\n",
    "                self.bs.append(b)\n",
    "\n",
    "            return X @ W + b\n",
    "        \n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            X0 = X\n",
    "            if self.upscale is not None:\n",
    "                X = self.upscale_fun(X)\n",
    "            for fxt_fun, fx0_fun, (Delta, Delta_b) in zip(self.layers_fxt, self.layers_fx0, self.deltas):\n",
    "                features = torch.cat([fxt_fun(X), fx0_fun(X0)], dim=1)\n",
    "                X = X + self.boost_lr * (features @ Delta + Delta_b)\n",
    "            return X @ self.Ws[-1] + self.bs[-1]\n",
    "\n",
    "\n",
    "model = CONCATFEATGradientRandFeatBoostRegression_fxtx0(\n",
    "     hidden_dim=32, \n",
    "     randfeat_xt_dim = 32,\n",
    "     randfeat_x0_dim = 1028,\n",
    "     out_dim=d, \n",
    "     n_layers=20, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\",\n",
    "     boost_lr=1.0\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train rmse tensor(0.9585) std tensor(0.1429)\n",
    "# test rmse tensor(1.9235) std tensor(0.1405)\n",
    "# train tensor([0.9936, 1.1145, 0.7242, 0.9691, 0.9910])\n",
    "# test tensor([2.0114, 2.0840, 1.7119, 1.9109, 1.8996])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable, Type\n",
    "import abc\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from models.ridge_ALOOCV import fit_ridge_ALOOCV\n",
    "from models.sandwiched_least_squares import sandwiched_LS_dense, sandwiched_LS_diag, sandwiched_LS_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make regression data X, y\n",
    "N = 1000\n",
    "N_test = 1000\n",
    "D = 10\n",
    "d = 3\n",
    "X = torch.randn(N, D)\n",
    "X_test = torch.randn(N_test, D)\n",
    "w_true = torch.randn(D, d)\n",
    "y = (X @ w_true)**2 + torch.randn(N, d) * 0.1  # Adding some noise\n",
    "y_test = (X_test @ w_true)**2 + torch.randn(N_test, d) * 0.1  # Adding some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(8.6430) std tensor(0.)\n",
      "test rmse tensor(9.0828) std tensor(0.)\n",
      "train tensor([8.6430, 8.6430, 8.6430, 8.6430, 8.6430, 8.6430, 8.6430, 8.6430, 8.6430,\n",
      "        8.6430])\n",
      "test tensor([9.0828, 9.0828, 9.0828, 9.0828, 9.0828, 9.0828, 9.0828, 9.0828, 9.0828,\n",
      "        9.0828])\n"
     ]
    }
   ],
   "source": [
    "from models.models import RidgeCVModule\n",
    "\n",
    "#dense      \n",
    "model = RidgeCVModule(\n",
    "        lower_alpha=1e-6,\n",
    "        upper_alpha=1e6,\n",
    "        n_alphas=10,\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(2.3312) std tensor(0.5462)\n",
      "test rmse tensor(3.3370) std tensor(0.6041)\n",
      "train tensor([2.5459, 1.9770, 2.1338, 3.6567, 1.9788, 2.5264, 2.1827, 1.9025, 2.6022,\n",
      "        1.8058])\n",
      "test tensor([3.7197, 2.8087, 2.9656, 4.6065, 2.9207, 3.3118, 3.1447, 3.0470, 4.0548,\n",
      "        2.7906])\n"
     ]
    }
   ],
   "source": [
    "from models.models import GreedyRandFeatBoostRegression\n",
    "\n",
    "#dense      \n",
    "model = GreedyRandFeatBoostRegression(\n",
    "     hidden_dim=128, \n",
    "     bottleneck_dim=128, \n",
    "     out_dim=d, \n",
    "     n_layers=5, \n",
    "     l2_reg=0.1, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\", \n",
    "     sandwich_solver=\"dense\"\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(1.8218) std tensor(0.5824)\n",
      "test rmse tensor(2.4492) std tensor(0.5188)\n",
      "train tensor([1.3506, 1.9186, 1.5029, 1.6868, 1.7180, 1.5808, 1.7199, 3.3880, 1.9279,\n",
      "        1.4242])\n",
      "test tensor([1.9153, 2.5489, 2.1412, 2.3750, 2.3784, 2.3791, 2.3258, 3.8210, 2.5010,\n",
      "        2.1065])\n"
     ]
    }
   ],
   "source": [
    "#diag\n",
    "model = GreedyRandFeatBoostRegression(\n",
    "     hidden_dim=512, \n",
    "     bottleneck_dim=512, \n",
    "     out_dim=d, \n",
    "     n_layers=3, \n",
    "     l2_reg=0.1, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\", \n",
    "     sandwich_solver=\"diag\"\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(2.3644) std tensor(2.2127)\n",
      "test rmse tensor(3.0741) std tensor(2.2342)\n",
      "train tensor([2.1745, 0.8754, 5.7361, 0.6286, 6.2912, 0.9110, 0.6166, 1.4423, 4.2110,\n",
      "        0.7568])\n",
      "test tensor([3.1005, 1.5892, 6.5635, 1.3099, 6.9387, 1.5568, 1.3191, 2.1315, 4.8798,\n",
      "        1.3522])\n"
     ]
    }
   ],
   "source": [
    "#scalar\n",
    "model = GreedyRandFeatBoostRegression(\n",
    "     hidden_dim=512, \n",
    "     bottleneck_dim=512, \n",
    "     out_dim=d, \n",
    "     n_layers=3, \n",
    "     l2_reg=0.0001, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\", \n",
    "     sandwich_solver=\"scalar\"\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(1.6803) std tensor(0.4142)\n",
      "test rmse tensor(2.7276) std tensor(0.4814)\n",
      "train tensor([1.8557, 1.3188, 1.9709, 2.5846, 1.5602, 1.6036, 1.9472, 1.3233, 1.3398,\n",
      "        1.2990])\n",
      "test tensor([2.8761, 2.1755, 2.8845, 3.6712, 2.7031, 2.6914, 3.0732, 2.1563, 2.9168,\n",
      "        2.1276])\n"
     ]
    }
   ],
   "source": [
    "from models.models import GradientRandFeatBoostRegression\n",
    "        \n",
    "model = GradientRandFeatBoostRegression(\n",
    "     hidden_dim=32, \n",
    "     bottleneck_dim=512, \n",
    "     out_dim=d, \n",
    "     n_layers=5, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\", \n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(10):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End2End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 14.57it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 14.31it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 13.12it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 13.97it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 14.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(1.4970) std tensor(1.2154)\n",
      "test rmse tensor(2.3131) std tensor(0.9893)\n",
      "train tensor([0.5836, 2.8102, 0.6247, 2.8462, 0.6202])\n",
      "test tensor([1.4892, 3.4227, 1.7266, 3.3623, 1.5646])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from models.models import End2EndMLPResNet\n",
    "\n",
    "model = End2EndMLPResNet(\n",
    "        in_dim=D,\n",
    "        hidden_dim=128,\n",
    "        bottleneck_dim=32,\n",
    "        out_dim=d,\n",
    "        n_blocks=3,\n",
    "        loss = \"mse\",\n",
    "        lr = 0.1,\n",
    "        n_epochs = 30,\n",
    "        end_lr_factor= 0.1,\n",
    "        weight_decay = 0.001,\n",
    "        batch_size = 64,\n",
    "        )\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next model f(x_t, x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse tensor(0.5150) std tensor(0.0308)\n",
      "test rmse tensor(1.6631) std tensor(0.0689)\n",
      "train tensor([0.4694, 0.5369, 0.4967, 0.5335, 0.5384])\n",
      "test tensor([1.6296, 1.7383, 1.5621, 1.6821, 1.7036])\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable, Type\n",
    "import abc\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "\n",
    "from models.ridge_ALOOCV import fit_ridge_ALOOCV\n",
    "from models.sandwiched_least_squares import sandwiched_LS_dense, sandwiched_LS_diag, sandwiched_LS_scalar\n",
    "from models.models import FittableModule, create_layer\n",
    "\n",
    "\n",
    "class GradientRandFeatBoostRegression_fxtx0(FittableModule):\n",
    "    def __init__(self, \n",
    "                 hidden_dim: int = 128,\n",
    "                 bottleneck_dim: int = 128,\n",
    "                 out_dim: int = 1,\n",
    "                 n_layers: int = 5,\n",
    "                 activation: nn.Module = nn.Tanh(),\n",
    "                #  l2_reg: float = 0.01,   #TODO ALOOCV or fixed l2_reg\n",
    "                 feature_type = \"SWIM\", # \"dense\", identity\n",
    "                 boost_lr: float = 1.0,\n",
    "                 upscale: Optional[str] = \"dense\",\n",
    "                 concat_phi_t_x0: bool = False,\n",
    "                 ):\n",
    "        super(GradientRandFeatBoostRegression_fxtx0, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bottleneck_dim = bottleneck_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        # self.l2_reg = l2_reg\n",
    "        self.feature_type = feature_type\n",
    "        self.boost_lr = boost_lr\n",
    "        self.upscale = upscale\n",
    "        self.concat_phi_t_x0 = concat_phi_t_x0\n",
    "\n",
    "\n",
    "    def fit(self, X: Tensor, y: Tensor):\n",
    "        in_dim = X.shape[1]\n",
    "        with torch.no_grad():\n",
    "            X0 = X\n",
    "            #optional upscale\n",
    "            if self.upscale == \"dense\":\n",
    "                self.upscale_fun = create_layer(self.upscale, X.shape[1], self.hidden_dim, None)\n",
    "                X = self.upscale_fun.fit_transform(X, y)\n",
    "            elif self.upscale == \"SWIM\":\n",
    "                self.upscale_fun = create_layer(self.upscale, X.shape[1], self.hidden_dim, self.activation)\n",
    "                X = self.upscale_fun.fit_transform(X, y)\n",
    "\n",
    "            # Create regressor W_0\n",
    "            self.W, self.b, _ = fit_ridge_ALOOCV(X, y)\n",
    "            self.layers = []\n",
    "            self.deltas = []\n",
    "\n",
    "            # Layerwise boosting\n",
    "            N = X.size(0)\n",
    "            for t in range(self.n_layers):\n",
    "                Xt0 = X\n",
    "                #Step 0: do we want f(x_t) or f(x_t, x_0)?\n",
    "                if self.concat_phi_t_x0:\n",
    "                    X = torch.cat([X, X0], dim=1) \n",
    "                    X = (X-X.mean(dim=0, keepdim=True)) / torch.std(X, dim=0, keepdim=True) #TODO keep normalization?\n",
    "                else:\n",
    "                    in_dim = 0 # hack\n",
    "\n",
    "                # Step 1: Create random feature layer\n",
    "                layer = create_layer(self.feature_type, self.hidden_dim+in_dim, self.bottleneck_dim, self.activation)\n",
    "                F = layer.fit_transform(X, y)\n",
    "\n",
    "                # Step 2: Obtain activation gradient and learn Delta\n",
    "                # X shape (N, D) --- ResNet neurons\n",
    "                # F shape (N, p) --- random features\n",
    "                # y shape (N, d) --- target\n",
    "                # W shape (D, d) --- top level classifier\n",
    "                # G shape (N, D) --- gradient of neurons\n",
    "                # r shape (N, d) --- residual at currect boosting iteration\n",
    "\n",
    "                r = y - Xt0 @ self.W - self.b\n",
    "                G = r @ self.W.T\n",
    "                \n",
    "                # fit to negative gradient (finding functional direction)\n",
    "                Delta, Delta_b, _ = fit_ridge_ALOOCV(F, G)\n",
    "                Ghat = F @ Delta + Delta_b\n",
    "\n",
    "                # Line search closed form risk minimization of R(W_t, Phi_{t+1})\n",
    "                linesearch = sandwiched_LS_scalar(r, self.W, Ghat, 0.00001)\n",
    "\n",
    "\n",
    "                # Step 3: Learn top level classifier\n",
    "                X = Xt0 + self.boost_lr * linesearch * Ghat\n",
    "                self.W, self.b, _ = fit_ridge_ALOOCV(X, y)\n",
    "\n",
    "                #update Delta scale\n",
    "                Delta = Delta * linesearch\n",
    "                Delta_b = Delta_b * linesearch\n",
    "\n",
    "                # store\n",
    "                self.layers.append(layer)\n",
    "                self.deltas.append((Delta, Delta_b))\n",
    "\n",
    "            return X @ self.W + self.b\n",
    "        \n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            X0 = X\n",
    "            if self.upscale is not None:\n",
    "                X = self.upscale_fun(X)\n",
    "            for layer, (Delta, Delta_b) in zip(self.layers, self.deltas):\n",
    "                Xt0 = X\n",
    "                if self.concat_phi_t_x0:\n",
    "                    X = torch.cat([X, X0], dim=1) \n",
    "                    X = (X-X.mean(dim=0, keepdim=True)) / torch.std(X, dim=0, keepdim=True) #TODO keep normalization?\n",
    "                X = Xt0 + self.boost_lr * (layer(X) @ Delta + Delta_b)\n",
    "            return X @ self.W + self.b\n",
    "        \n",
    "\n",
    "model = GradientRandFeatBoostRegression_fxtx0(\n",
    "     hidden_dim=32, \n",
    "     bottleneck_dim=1000, \n",
    "     out_dim=d, \n",
    "     n_layers=10, \n",
    "     feature_type=\"SWIM\", \n",
    "     upscale=\"dense\",\n",
    "     concat_phi_t_x0=True,\n",
    "     boost_lr=1.0\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# another option: concat[f(x_t), h(x_0)]. Do this next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO batch normalization??? Wolfe-Franke???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable, Type\n",
    "import abc\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from models.ridge_ALOOCV import fit_ridge_ALOOCV\n",
    "from models.sandwiched_least_squares import sandwiched_LS_dense, sandwiched_LS_diag, sandwiched_LS_scalar\n",
    "\n",
    "X_testing = torch.randn(100, 128)\n",
    "test = nn.Linear(128, 0)\n",
    "\n",
    "out = test(X_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10])\n",
      "torch.Size([1000, 3])\n",
      "train rmse tensor(0.9708) std tensor(0.3264)\n",
      "test rmse tensor(3.2590) std tensor(0.1887)\n",
      "train tensor([0.8672, 0.7679, 0.6892, 0.8056, 0.8406, 1.1227, 1.7298, 1.1538, 0.7605])\n",
      "test tensor([3.3239, 3.1751, 3.0082, 3.1010, 3.1505, 3.2738, 3.5422, 3.5585, 3.1980])\n"
     ]
    }
   ],
   "source": [
    "from models.models import GradientRandFeatBoostReg\n",
    "\n",
    "model = GradientRandFeatBoostReg(\n",
    "     hidden_dim=32, \n",
    "     randfeat_xt_dim = 128,\n",
    "     randfeat_x0_dim = 512,\n",
    "     out_dim=d, \n",
    "     n_layers=5, \n",
    "     feature_type=\"dense\", \n",
    "     upscale=\"dense\",\n",
    "     boost_lr=1.0\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(9):\n",
    "    model.fit(X, y)\n",
    "    out = model(X)\n",
    "    out_test = model(X_test)\n",
    "    rmse = torch.sqrt(F.mse_loss(out, y))\n",
    "    rmse_test = torch.sqrt(F.mse_loss(out_test, y_test))\n",
    "    results.append(torch.tensor([rmse, rmse_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train rmse\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test rmse\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train\", results[:, 0])\n",
    "print(\"test\", results[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO make the GreedyRandFeatReg also use both xtx0. (here it almost makes more sense to concat f([x0,xt]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable, Type\n",
    "import abc\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.ridge_ALOOCV import fit_ridge_ALOOCV\n",
    "from models.sandwiched_least_squares import sandwiched_LS_dense, sandwiched_LS_diag, sandwiched_LS_scalar\n",
    "\n",
    "def acc_from_logits(logits, y, n_classes):\n",
    "    if n_classes==2:\n",
    "        pred = (logits>0).float()\n",
    "    else:\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        y = torch.argmax(y, dim=1)\n",
    "    acc = (pred==y).float().mean()\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def generate_concentric_rings(n_rings_per_class=2, n_classes=3, n_samples=2000, device=\"cpu\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X = []\n",
    "    y = []\n",
    "    n_rings = n_rings_per_class * n_classes\n",
    "    samples_per_ring = n_samples // n_rings\n",
    "    for i in range(n_rings):\n",
    "        radius = i + 1\n",
    "        theta = np.linspace(0, 2 * np.pi, samples_per_ring)\n",
    "        noise_scale = 0.5\n",
    "        x1 = radius * np.cos(theta) + np.random.rand(samples_per_ring) * noise_scale\n",
    "        x2 = radius * np.sin(theta) + np.random.rand(samples_per_ring) * noise_scale\n",
    "        X.append(np.vstack((x1, x2)).T)\n",
    "        y.append(np.full(samples_per_ring, i % n_classes))\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "    perm = np.random.permutation(len(X))\n",
    "    X = X[perm] / np.max(X)\n",
    "    y = y[perm]\n",
    "\n",
    "    #plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(n_classes):\n",
    "        plt.scatter(X[y == i][:, 0], X[y == i][:, 1], label=f'Class {i}')\n",
    "    plt.legend()\n",
    "    plt.title('Concentric Rings Dataset')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.show()\n",
    "\n",
    "    y = torch.tensor(y).float()\n",
    "    X = torch.tensor(X).float()\n",
    "    if n_classes > 2:\n",
    "        y = torch.nn.functional.one_hot(y.to(torch.int64), n_classes).float()\n",
    "    else:\n",
    "        y = y.unsqueeze(1)\n",
    "\n",
    "    r = int(0.5 * len(X))\n",
    "    X_train, y_train, X_test, y_test = X[:r], y[:r], X[r:], y[r:]\n",
    "    return (X_train.to(device),\n",
    "            y_train.to(device), \n",
    "            X_test.to(device), \n",
    "            y_test.to(device) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate dataset\n",
    "# n_classes = 2\n",
    "# D = 10\n",
    "# X_train, y_train, X_test, y_test = make_spirals_dataset_Rd(\n",
    "#     n_samples=1000, n_classes=n_classes, noise=0.1, D=D, train_test_ratio=0.8\n",
    "#     )\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "n_classes = 3\n",
    "D = 2\n",
    "device = \"cpu\"\n",
    "X_train, y_train, X_test, y_test = generate_concentric_rings(\n",
    "    n_rings_per_class=2, n_classes=n_classes, n_samples=2000, device=device\n",
    "    )\n",
    "if n_classes > 2:\n",
    "    loss_fn = nn.functional.cross_entropy\n",
    "else:\n",
    "    loss_fn = nn.functional.binary_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.base import LogisticRegression\n",
    "    \n",
    "model = LogisticRegression(\n",
    "    n_classes, l2_lambda=0.1, max_iter=100\n",
    "     )\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    model.fit(X_train, y_train)\n",
    "    out_train = model(X_train)\n",
    "    out_test = model(X_test)\n",
    "    ce_train = loss_fn(out_train, y_train)\n",
    "    ce_test = loss_fn(out_test, y_test)\n",
    "    acc_train = acc_from_logits(out_train, y_train, n_classes)\n",
    "    acc_test = acc_from_logits(out_test, y_test, n_classes)\n",
    "    results.append(torch.tensor([ce_train, ce_test, acc_train, acc_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train ce\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test ce\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train acc\", results[:, 2].mean(), \"std\", results[:, 2].std())\n",
    "print(\"test acc\", results[:, 3].mean(), \"std\", results[:, 3].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.random_feature_representation_boosting import GradientRFRBoostClassifier\n",
    "\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = GradientRFRBoostClassifier(\n",
    "        in_dim=D,\n",
    "        n_classes=n_classes,\n",
    "        hidden_dim=2,\n",
    "        n_layers=4,\n",
    "        randfeat_xt_dim=128,\n",
    "        randfeat_x0_dim=128,\n",
    "        l2_cls=0.001,\n",
    "        l2_ghat=0.00001,\n",
    "        boost_lr=1.0,\n",
    "        feature_type=\"SWIM\",\n",
    "        upscale_type=\"identity\",\n",
    "        lbfgs_lr=1.0,\n",
    "        lbfgs_max_iter=300,\n",
    "    )\n",
    "\n",
    "results = []\n",
    "for i in range(2):\n",
    "    model.fit(X_train, y_train)\n",
    "    out_train = model(X_train)\n",
    "    out_test = model(X_test)\n",
    "    ce_train = loss_fn(out_train, y_train)\n",
    "    ce_test = loss_fn(out_test, y_test)\n",
    "    acc_train = acc_from_logits(out_train, y_train, n_classes)\n",
    "    acc_test = acc_from_logits(out_test, y_test, n_classes)\n",
    "    results.append(torch.tensor([ce_train, ce_test, acc_train, acc_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train ce\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test ce\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train acc\", results[:, 2].mean(), \"std\", results[:, 2].std())\n",
    "print(\"test acc\", results[:, 3].mean(), \"std\", results[:, 3].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(X_train, y_train, X_test, y_test):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        if y_train.size(1) == 1:\n",
    "            y_train = torch.nn.functional.one_hot(y_train[:, 0].to(torch.int64), 2).float()\n",
    "            y_test = torch.nn.functional.one_hot(y_test[:, 0].to(torch.int64), 2).float()\n",
    "\n",
    "        X_train = X_train.cpu().numpy()\n",
    "        y_train = y_train.cpu().numpy()\n",
    "        X_test = X_test.cpu().numpy()\n",
    "        y_test = y_test.cpu().numpy()\n",
    "\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        for i in range(n_classes):\n",
    "            plt.scatter(X_train[y_train[:, i] == 1][:, 0], X_train[y_train[:, i] == 1][:, 1], label=f'Class {i}')\n",
    "        plt.title('Training Data')\n",
    "        plt.xlabel('x1')\n",
    "        plt.ylabel('x2')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        for i in range(n_classes):\n",
    "            plt.scatter(X_test[y_test[:, i] == 1][:, 0], X_test[y_test[:, i] == 1][:, 1], label=f'Class {i}')\n",
    "        plt.title('Test Data')\n",
    "        plt.xlabel('x1')\n",
    "        plt.ylabel('x2')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def see_results_for_every_layer(X_train, y_train, X_test, y_test, model, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "\n",
    "        X_train = model.upscale(X0_train)\n",
    "        X_test = model.upscale(X0_test)\n",
    "\n",
    "        pred_train = model.top_level_modules[0](X_train)\n",
    "        pred_test = model.top_level_modules[0](X_test)\n",
    "\n",
    "        ce_train = loss_fn(pred_train, y_train)\n",
    "        ce_test = loss_fn(pred_test, y_test)\n",
    "        acc_train = acc_from_logits(pred_train, y_train, n_classes)\n",
    "        acc_test = acc_from_logits(pred_test, y_test, n_classes)\n",
    "        print(f\"Train ce at layer 0: {ce_train}\")\n",
    "        print(f\"Test ce at layer 0: {ce_test}\")\n",
    "        print(f\"Train acc at layer 0: {acc_train}\")\n",
    "        print(f\"Test acc at layer 0: {acc_test}\")\n",
    "        print()\n",
    "        plot_features(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        for t, (feat_layer, ghat_layer, classifier) in enumerate(zip(model.random_feature_layers, \n",
    "                                                                     model.ghat_boosting_layers, \n",
    "                                                                     model.top_level_modules[1:])):\n",
    "            features_train = feat_layer(X_train, X0_train)\n",
    "            features_test = feat_layer(X_test, X0_test)\n",
    "            X_train = X_train + model.boost_lr * ghat_layer(features_train)\n",
    "            X_test = X_test + model.boost_lr * ghat_layer(features_test)\n",
    "            \n",
    "            pred_train = classifier(X_train)\n",
    "            pred_test = classifier(X_test)\n",
    "\n",
    "            ce_train = loss_fn(pred_train, y_train)\n",
    "            ce_test = loss_fn(pred_test, y_test)\n",
    "            acc_train = acc_from_logits(pred_train, y_train, n_classes)\n",
    "            acc_test = acc_from_logits(pred_test, y_test, n_classes)\n",
    "\n",
    "            print(f\"Train ce at layer {t+1}: {ce_train}\")\n",
    "            print(f\"Test ce at layer {t+1}: {ce_test}\")\n",
    "            print(f\"Train acc at layer {t+1}: {acc_train}\")\n",
    "            print(f\"Test acc at layer {t+1}: {acc_test}\")\n",
    "            print()\n",
    "            plot_features(X_train, y_train, X_test, y_test)\n",
    "\n",
    "see_results_for_every_layer(X_train, y_train, X_test, y_test, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End2End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.end2end import End2EndMLPResNet\n",
    "\n",
    "n_blocks = 5\n",
    "model = End2EndMLPResNet(\n",
    "        in_dim=2,\n",
    "        hidden_dim=2,\n",
    "        bottleneck_dim=128,\n",
    "        out_dim= (n_classes if n_classes>2 else 1),\n",
    "        n_blocks=n_blocks,\n",
    "        loss = (\"cce\" if n_classes>2 else \"bce\"),\n",
    "        lr = 0.1,\n",
    "        n_epochs = 30,\n",
    "        end_lr_factor= 0.01,\n",
    "        weight_decay = 0.00001,\n",
    "        batch_size = 64,\n",
    "        upsample = False,\n",
    "        activation = nn.Tanh(),\n",
    "        )\n",
    "\n",
    "results = []\n",
    "for i in range(1):\n",
    "    model.fit(X_train, y_train)\n",
    "    model.eval()\n",
    "    out_train = model(X_train)\n",
    "    out_test = model(X_test)\n",
    "    ce_train = loss_fn(out_train, y_train)\n",
    "    ce_test = loss_fn(out_test, y_test)\n",
    "    acc_train = acc_from_logits(out_train, y_train, n_classes)\n",
    "    acc_test = acc_from_logits(out_test, y_test, n_classes)\n",
    "    results.append(torch.tensor([ce_train, ce_test, acc_train, acc_test]))\n",
    "results = torch.stack(results)\n",
    "print(\"train ce\", results[:, 0].mean(), \"std\", results[:, 0].std())\n",
    "print(\"test ce\", results[:, 1].mean(), \"std\", results[:, 1].std())\n",
    "print(\"train acc\", results[:, 2].mean(), \"std\", results[:, 2].std())\n",
    "print(\"test acc\", results[:, 3].mean(), \"std\", results[:, 3].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resnet(X_train, y_train, X_test, y_test, model):\n",
    "    with torch.no_grad():\n",
    "        # for each layer, calculate the activations, and plot\n",
    "        X_train = model.upsample(X_train)\n",
    "        X_test = model.upsample(X_test)\n",
    "        plot_features(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        for t in range(n_blocks):\n",
    "            X_train = X_train + model.residual_blocks[t](X_train)\n",
    "            X_test = X_test + model.residual_blocks[t](X_test)\n",
    "            plot_features(X_train, y_train, X_test, y_test)\n",
    "plot_resnet(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do i want to set up the experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from models.base import FittableModule\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "class SKLearnWrapper(BaseEstimator):\n",
    "    def __init__(self, modelClass=None, **model_params):\n",
    "        self.modelClass = modelClass\n",
    "        self.model_params = model_params\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.modelClass(**self.model_params)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.modelClass = params.pop('modelClass', self.modelClass)\n",
    "        self.model_params.update(params)\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {'modelClass': self.modelClass}\n",
    "        params.update(self.model_params)\n",
    "        return params\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        logits = self.model(X)\n",
    "        if y.size(1) == 1:\n",
    "            pred = (logits>0).float()\n",
    "        else:\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            y = torch.argmax(y, dim=1)\n",
    "        acc = (pred==y).float().mean()\n",
    "        return acc.detach().cpu().item()\n",
    "    \n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def save_feature_plot(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        model, \n",
    "        n_classes, \n",
    "        file_name: str,\n",
    "        save_dir: str = \"save/ConcentricCircles/\"\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Saves train and test features side by side for visualization at a given layer.\n",
    "    \n",
    "    Args:\n",
    "        X_train (torch.Tensor): Features of the training set (shape: [n_samples, 2]).\n",
    "        y_train (torch.Tensor): Labels of the training set (one-hot or integer-encoded).\n",
    "        X_test (torch.Tensor): Features of the test set (shape: [n_samples, 2]).\n",
    "        y_test (torch.Tensor): Labels of the test set (one-hot or integer-encoded).\n",
    "        n_classes (int): Number of classes in the dataset.\n",
    "        file_name (str): Name of the file to save the plot.\n",
    "        save_dir (str): Directory to save the plot.\n",
    "    \"\"\"\n",
    "    # Obtain the features at each layer\n",
    "    train_features = [X_train]\n",
    "    test_features = [X_test]\n",
    "    if type(model) == GradientRFRBoostClassifier:\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "        for t, (feat_layer, ghat_layer, classifier) in enumerate(zip(model.random_feature_layers, \n",
    "                                                                     model.ghat_boosting_layers, \n",
    "                                                                     model.top_level_modules[1:])):\n",
    "            features_train = feat_layer(X_train, X0_train)\n",
    "            features_test = feat_layer(X_test, X0_test)\n",
    "            X_train = X_train + model.boost_lr * ghat_layer(features_train)\n",
    "            X_test = X_test + model.boost_lr * ghat_layer(features_test)\n",
    "            train_features.append(X_train)\n",
    "            test_features.append(X_test)\n",
    "    elif type(model) == End2EndMLPResNet:\n",
    "        for resblock in model.residual_blocks:\n",
    "            X_train = X_train + resblock(X_train)\n",
    "            X_test = X_test + resblock(X_test)\n",
    "            train_features.append(X_train)\n",
    "            test_features.append(X_test)\n",
    "    else:\n",
    "        raise ValueError(\"Model type not recognized.\")\n",
    "    \n",
    "    # Convert labels to integers if they're one-hot encoded\n",
    "    train_labels_np = y_train.argmax(dim=1).cpu().numpy() if n_classes > 2 else y_train.cpu().numpy()\n",
    "    test_labels_np = y_test.argmax(dim=1).cpu().numpy() if n_classes > 1 else y_test.cpu().numpy()\n",
    "\n",
    "\n",
    "    # Plot the features\n",
    "    T = len(train_features)\n",
    "    for name_train_or_test, features, labels in [(\"train\", train_features, train_labels_np), \n",
    "                                                 (\"test\", test_features, test_labels_np)]:\n",
    "        fig, axes = plt.subplots(1, T, figsize=(T * 4, 4), dpi=100)\n",
    "        for t in range(T):\n",
    "            ax = axes[t]\n",
    "            feat = features[t].detach().cpu().numpy()\n",
    "            for j in range(feat.shape[1]):\n",
    "                feat[:, j] = (feat[:, j] - feat[:, j].min()) / (feat[:, j].max() - feat[:, j].min())\n",
    "            for i in range(n_classes):\n",
    "                ax.scatter(feat[labels == i][:, 0], feat[labels == i][:, 1])\n",
    "\n",
    "            # Remove titles and adjust axis\n",
    "            ax.axis('equal')\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Add black border as a rectangle\n",
    "            rect = patches.Rectangle(\n",
    "                (0, 0), 1, 1, transform=ax.transAxes,  # Set size relative to axes\n",
    "                linewidth=2, edgecolor='black', facecolor='none', zorder=10\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        # Add vertical label for the model name\n",
    "        name = \"MLP ResNet\" if type(model) == End2EndMLPResNet else \"GRFRBoost (ours)\"\n",
    "        if \"GRFRBoost\" in name:\n",
    "            if model.randfeat_x0_dim == 0:\n",
    "                name += \" x0 only\"\n",
    "            if model.feature_type == \"SWIM\":\n",
    "                name += \" SWIM\"\n",
    "        fig.text(0.02, 0.5, name, fontsize=18, rotation='vertical', va='center', ha='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(left=0.03)  # Add space for the model name\n",
    "        plt.savefig(f\"{save_dir}/{file_name}_{name_train_or_test}.png\", bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_concentric_rings_experiment(\n",
    "        seed=0,\n",
    "        k_folds=5,\n",
    "        n_classes=3,\n",
    "        n_rings_per_class=2,\n",
    "        n_samples=2000,\n",
    "        device=\"cuda\",\n",
    "        ):\n",
    "    \"\"\"Runs the concentric circle experiments and saves the plots and results.\"\"\"\n",
    "    # Set seed\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Create dataset\n",
    "    X_train, y_train, X_test, y_test = generate_concentric_rings(\n",
    "        n_rings_per_class, n_classes, n_samples, device\n",
    "    )\n",
    "\n",
    "    # Define models and their hyperparameter grids\n",
    "    n_layers = 4 # TODO CHANGE\n",
    "    model_specs = {\n",
    "        \"GradientRFRBoost\": {\n",
    "            'modelClass': [GradientRFRBoostClassifier],\n",
    "            'l2_cls': np.logspace(-7, -1, 7),\n",
    "            'l2_ghat': np.logspace(-7, -1, 7),\n",
    "            'in_dim': [2],\n",
    "            'n_classes': [n_classes],\n",
    "            'hidden_dim': [2],\n",
    "            'n_layers': [n_layers],\n",
    "            'randfeat_xt_dim': [128],\n",
    "            'randfeat_x0_dim': [128],\n",
    "            'feature_type': [\"iid\"],\n",
    "            'upscale_type': [\"identity\"],\n",
    "        },\n",
    "        \"GradientRFRBoost xt only\": {\n",
    "            'modelClass': [GradientRFRBoostClassifier],\n",
    "            'l2_cls': np.logspace(-7, -1, 7),\n",
    "            'l2_ghat': np.logspace(-7, -1, 7),\n",
    "            'in_dim': [2],\n",
    "            'n_classes': [n_classes],\n",
    "            'hidden_dim': [2],\n",
    "            'n_layers': [n_layers],\n",
    "            'randfeat_xt_dim': [128],\n",
    "            'randfeat_x0_dim': [0],\n",
    "            'feature_type': [\"iid\"],\n",
    "            'upscale_type': [\"identity\"],\n",
    "        },\n",
    "        \"GradientRFRBoost SWIM\": {\n",
    "            'modelClass': [GradientRFRBoostClassifier],\n",
    "            'l2_cls': np.logspace(-7, -1, 7),\n",
    "            'l2_ghat': np.logspace(-7, -1, 7),\n",
    "            'in_dim': [2],\n",
    "            'n_classes': [n_classes],\n",
    "            'hidden_dim': [2],\n",
    "            'n_layers': [n_layers],\n",
    "            'randfeat_xt_dim': [128],\n",
    "            'randfeat_x0_dim': [128],\n",
    "            'feature_type': [\"SWIM\"],\n",
    "            'upscale_type': [\"identity\"],\n",
    "        },\n",
    "        \"GradientRFRBoost xt only SWIM\": {\n",
    "            'modelClass': [GradientRFRBoostClassifier],\n",
    "            'l2_cls': np.logspace(-7, -1, 7),\n",
    "            'l2_ghat': np.logspace(-7, -1, 7),\n",
    "            'in_dim': [2],\n",
    "            'n_classes': [n_classes],\n",
    "            'hidden_dim': [2],\n",
    "            'n_layers': [n_layers],\n",
    "            'randfeat_xt_dim': [128],\n",
    "            'randfeat_x0_dim': [0],\n",
    "            'feature_type': [\"SWIM\"],\n",
    "            'upscale_type': [\"identity\"],\n",
    "        },\n",
    "        \"Logistic Regression\": {\n",
    "            'modelClass': [LogisticRegression],\n",
    "            'l2_lambda': np.logspace(-4, -1, 4),\n",
    "            'n_classes': [n_classes],\n",
    "        },\n",
    "        \"End2EndMLP\": {\n",
    "            'modelClass': [End2EndMLPResNet],\n",
    "            'lr': np.logspace(-4, -1, 4),\n",
    "            'in_dim': [2],\n",
    "            'hidden_dim': [2],\n",
    "            'bottleneck_dim': [128],\n",
    "            'out_dim': [n_classes if n_classes > 2 else 1],\n",
    "            'n_blocks': [n_layers],\n",
    "            'loss': [\"cce\" if n_classes > 2 else \"bce\"],\n",
    "            'n_epochs': [30],\n",
    "            'end_lr_factor': [0.01],\n",
    "            'weight_decay': [0.00001],\n",
    "            'batch_size': [64],\n",
    "            'upsample': [False],\n",
    "            'activation': [nn.Tanh()],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Run experiments\n",
    "    results = {}\n",
    "    results_params = {}\n",
    "    for model_name, param_grid in model_specs.items():\n",
    "        accuracies = []\n",
    "        best_params = []\n",
    "        for i in range(5):\n",
    "            print(i, model_name)\n",
    "            # Perform grid search with k-fold cross-validation\n",
    "            estimator = SKLearnWrapper()\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=estimator,\n",
    "                param_grid=param_grid,\n",
    "                cv=k_folds,\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            best_model = grid_search.best_estimator_\n",
    "            accuracy = best_model.score(X_test, y_test)\n",
    "            accuracies.append(accuracy)\n",
    "            best_params.append(grid_search.best_params_)\n",
    "\n",
    "            if i == 0 and model_name != \"Logistic Regression\":\n",
    "                print(\"plotting and saving feature evolution point cloud\")\n",
    "                save_feature_plot(X_train, y_train, X_test, y_test, best_model.model, n_classes, model_name)\n",
    "        results[model_name] = accuracies\n",
    "        results_params[model_name] = best_params\n",
    "\n",
    "    return results, results_params\n",
    "\n",
    "results, results_params = run_concentric_rings_experiment()\n",
    "\n",
    "for model_name, accs in results.items():\n",
    "    print(model_name, \"acc\", np.mean(accs), \"std\", np.std(accs))\n",
    "    print(accs)\n",
    "    print()\n",
    "for model_name, best_params in results_params.items():\n",
    "    print(model_name, \"best params\", best_params)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: combine the plots into one image for the paper. also combine train vs test plots for appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO add batch norm... should help stabilize gradient\n",
    "\n",
    "# TODO add pointer to prev classifier, for faster training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocessing.stream_transforms import normalize_mean_std_traindata, normalize_streams, augment_time, add_basepoint_zero\n",
    "from utils.utils import print_name, print_shape\n",
    "from models import ResNet, NeuralEulerODE, RidgeClassifierCVModule, E2EResNet, LogisticRegression, RandFeatBoost, GradientRandomFeatureBoostingClassification\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options\n",
    "device = \"cuda\" #torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_loader, model, device):\n",
    "    \"\"\"Function to extract features from a dataset using pre-train model\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, target in tqdm(data_loader):\n",
    "            images = images.to(device)\n",
    "            output = model(images)\n",
    "            output = output.view(output.size(0), -1)\n",
    "            features.append(output.cpu().numpy())\n",
    "            labels.append(target.cpu().numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def load_cifar10(\n",
    "        data_path = \"/home/nikita/hdd/cifar10/\",\n",
    "        train_name = \"resnet18_train_features.csv\",\n",
    "        test_name = \"resnet18_test_features.csv\",\n",
    "        ):\n",
    "    \"\"\"Loads a pretrained ResNet18 model and extracts features from the CIFAR-10 dataset\"\"\"\n",
    "    # see if data has already been processed\n",
    "    if train_name in os.listdir(data_path) and \\\n",
    "            test_name in os.listdir(data_path):\n",
    "        print(\"Loading preprocessed data\")\n",
    "        train_df = pd.read_csv(data_path + train_name)\n",
    "        test_df = pd.read_csv(data_path + test_name)\n",
    "        return train_df.to_numpy(), test_df.to_numpy()\n",
    "\n",
    "    # Define the DataLoaders and transformations for the CIFAR-10 dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),  # Resize images to 224x224 as required by ResNet18\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize with ImageNet mean and std\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Load the pre-trained ResNet18 model, remove classification head\n",
    "    model = resnet18(weights=True)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Extract features for the training and test datasets\n",
    "    train_features, train_labels = extract_features(train_loader, model, device)\n",
    "    test_features, test_labels = extract_features(test_loader, model, device)\n",
    "\n",
    "    # Create a DataFrame to store the features and labels, save to CSV\n",
    "    train_df = pd.DataFrame(train_features)\n",
    "    train_df['target'] = train_labels\n",
    "    test_df = pd.DataFrame(test_features)\n",
    "    test_df['target'] = test_labels\n",
    "    train_df.to_csv(data_path + train_name, index=False)\n",
    "    test_df.to_csv(data_path + test_name, index=False)\n",
    "    return train_df.to_numpy(), test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data\n",
      "Train data shape: torch.Size([50000, 512]), dtype: torch.float32\n",
      "Train labels shape: torch.Size([50000, 10]), dtype: torch.float32\n",
      "Test data shape: torch.Size([10000, 512]), dtype: torch.float32\n",
      "Test labels shape: torch.Size([10000, 10]), dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "train, test = load_cifar10()\n",
    "\n",
    "X_train = torch.from_numpy(train[:, :-1].astype(np.float32)).to(device)\n",
    "y_train_cat = torch.from_numpy(train[:, -1].astype(np.int64)).to(device)\n",
    "X_test = torch.from_numpy(test[:, :-1].astype(np.float32)).to(device)\n",
    "y_test_cat = torch.from_numpy(test[:, -1].astype(np.int64)).to(device)\n",
    "\n",
    "# Convert train and test labels to one-hot encoding\n",
    "y_train = F.one_hot(y_train_cat, num_classes=10).float()\n",
    "y_test = F.one_hot(y_test_cat, num_classes=10).float()\n",
    "print(f\"Train data shape: {X_train.shape}, dtype: {X_train.dtype}\")\n",
    "print(f\"Train labels shape: {y_train.shape}, dtype: {y_train.dtype}\")\n",
    "print(f\"Test data shape: {X_test.shape}, dtype: {X_test.dtype}\")\n",
    "print(f\"Test labels shape: {y_test.shape}, dtype: {y_test.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allmodels_1dataset(\n",
    "        generator: torch.Generator,\n",
    "        X_train: Tensor,\n",
    "        y_train: Tensor,\n",
    "        X_test: Tensor,\n",
    "        y_test: Tensor,\n",
    "        ):\n",
    "    \n",
    "    D = X_train.shape[1]\n",
    "    hidden_size = 512\n",
    "    bottleneck_dim = 1*hidden_size\n",
    "    num_epochs = 40\n",
    "    batch_size = 512\n",
    "    adam_lr = 0.01\n",
    "    \n",
    "    # (name, model, kwargs). kwargs separate to save memory\n",
    "    model_list = [\n",
    "        # [\"T=10 RandFeatureBoost\", RandFeatBoost,\n",
    "        #         {\"generator\": generator,\n",
    "        #          \"in_dim\": D,\n",
    "        #          \"hidden_size\": hidden_size,\n",
    "        #          \"out_dim\": 10,\n",
    "        #          \"n_blocks\": 9,\n",
    "        #          \"activation\": nn.Tanh(),\n",
    "        #          \"loss_fn\": nn.CrossEntropyLoss(),\n",
    "        #          \"adam_lr\": adam_lr,\n",
    "        #          \"boost_lr\": 1.0,\n",
    "        #          \"epochs\": num_epochs,\n",
    "        #          \"batch_size\": batch_size,\n",
    "        #          \"upscale_type\": \"SWIM\",  # \"dense\", \"identity\"\n",
    "        #          }],\n",
    "\n",
    "        # [\"Tabular RidgeClassifier\", RidgeClassifierCVModule, {}],\n",
    "\n",
    "    #     [\"T=1 Dense\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #              \"in_dim\": D,\n",
    "    #              \"hidden_size\": hidden_size,\n",
    "    #              \"bottleneck_dim\": None,\n",
    "    #              \"n_blocks\": 0,\n",
    "    #              \"upsample_layer\": \"dense\",\n",
    "    #              \"output_layer\": \"logistic regression\",\n",
    "    #              }],\n",
    "\n",
    "    #     [\"T=1 SWIM Grad\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": None,\n",
    "    #             \"n_blocks\": 0,\n",
    "    #             \"upsample_layer\": \"SWIM\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }],\n",
    "    ]\n",
    "\n",
    "    # for n_blocks in [3]:\n",
    "    #     model_list += [\n",
    "    #     [f\"T={n_blocks+1} End2End\", E2EResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": bottleneck_dim,\n",
    "    #             \"out_dim\": 10,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"activation\": nn.Tanh(),\n",
    "    #             \"loss\": nn.CrossEntropyLoss(),\n",
    "    #             \"lr\": adam_lr,\n",
    "    #             \"epochs\": num_epochs,\n",
    "    #             \"batch_size\": batch_size}\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResSWIM Grad-dense\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": bottleneck_dim,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"SWIM\",\n",
    "    #             \"res_layer1\": \"SWIM\",\n",
    "    #             \"res_layer2\": \"dense\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResSWIM Grad-id\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": hidden_size,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"SWIM\",\n",
    "    #             \"res_layer1\": \"SWIM\",\n",
    "    #             \"res_layer2\": \"identity\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResDense\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": hidden_size,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"dense\",\n",
    "    #             \"res_layer1\": \"dense\",\n",
    "    #             \"res_layer2\": \"identity\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "    # ]\n",
    "        \n",
    "    for l2_reg in [ 0.00001]:\n",
    "        model_list += [\n",
    "            [f\"Logistic L-BFSG, l2={l2_reg}\", \n",
    "                LogisticRegression, \n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"out_dim\": 10,\n",
    "                \"l2_reg\": l2_reg,\n",
    "                \"lr\": 1.0,\n",
    "                \"max_iter\": 100,\n",
    "                }],\n",
    "        ]\n",
    "    \n",
    "        \n",
    "    for n_blocks in range(0, 10 + 1, 5):\n",
    "        model_list += [\n",
    "            [f\"T={n_blocks} Gradient GRFBoost\", \n",
    "             GradientRandomFeatureBoostingClassification,\n",
    "                {\"generator\": generator,\n",
    "                \"hidden_dim\": hidden_size,\n",
    "                \"bottleneck_dim\": bottleneck_dim,\n",
    "                \"out_dim\": 10,\n",
    "                \"n_layers\": n_blocks,\n",
    "                \"activation\": nn.Tanh(),\n",
    "                \"l2_reg\": 0.0001,\n",
    "                \"feature_type\": \"SWIM\",\n",
    "                \"boost_lr\": 0.5,\n",
    "                \"upscale\": None,\n",
    "                },\n",
    "                ],\n",
    "        ]\n",
    "    \n",
    "    results = []\n",
    "    model_names = []\n",
    "    for name, model, model_args in model_list:\n",
    "        print(name)\n",
    "        torch.cuda.empty_cache()\n",
    "        t0 = time.perf_counter()\n",
    "        model = model(**model_args).to(X_train.device)\n",
    "        pred_train, _ = model.fit(X_train, y_train)\n",
    "        t1 = time.perf_counter()\n",
    "        pred_test = model(X_test)\n",
    "        t2 = time.perf_counter()\n",
    "        \n",
    "        #convert to class predictions:\n",
    "        if len(pred_train.shape) == 2:\n",
    "            pred_train = torch.argmax(pred_train, dim=1)\n",
    "            pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        result = np.array( [acc_train, acc_test, t1-t0, t2-t1] )\n",
    "        results.append( result )\n",
    "        model_names.append( name )\n",
    "\n",
    "    return model_names, results\n",
    "\n",
    "\n",
    "\n",
    "def run_all_experiments(\n",
    "        name_save: str = \"PLACEHOLDER\",\n",
    "        ):\n",
    "    # Fetch and process each dataset\n",
    "    experiments = {}\n",
    "    generator = torch.Generator(device=device).manual_seed(999)\n",
    "    results = run_allmodels_1dataset(\n",
    "        generator, X_train, y_train, X_test, y_test, \n",
    "        )\n",
    "    experiments[\"MNIST\"] = results\n",
    "\n",
    "    # Save results\n",
    "    # Assuming experiments is a dict where keys are dataset names and values are tuples (model_names, results)\n",
    "    attributes = [\"acc_train\", \"acc_test\", \"t_fit\", \"t_feat\"]\n",
    "    data_list = []\n",
    "    # Process the data\n",
    "    for dataset_name, (model_names, results) in experiments.items():\n",
    "        dataset_data = {}\n",
    "        for attr_idx, attribute in enumerate(attributes):\n",
    "            for model_idx, model_name in enumerate(model_names):\n",
    "                dataset_data[(attribute, model_name)] = results[model_idx][attr_idx]\n",
    "        data_list.append(pd.DataFrame(dataset_data, index=[dataset_name]))\n",
    "\n",
    "    # Combine all datasets into a single DataFrame\n",
    "    df = pd.concat(data_list)\n",
    "    df = df.sort_index(axis=1)\n",
    "    df.to_pickle(f\"cifar10_{name_save}.pkl\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic L-BFSG, l2=1e-05\n",
      "T=0 Gradient GRFBoost\n",
      "T=5 Gradient GRFBoost\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 3.584036350250244\n",
      "Gradient hat norm tensor(6.7225, device='cuda:0')\n",
      "alpha 0.02154434658586979\n",
      "linesearch loss tensor(0.4472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.4467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 10.734461784362793\n",
      "Gradient hat norm tensor(33.7338, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 12.39175796508789\n",
      "Gradient hat norm tensor(23.6323, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.3443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 16.293067932128906\n",
      "Gradient hat norm tensor(13.3283, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.3279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 17.734045028686523\n",
      "Gradient hat norm tensor(10.5710, device='cuda:0')\n",
      "T=10 Gradient GRFBoost\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.009189658798277378\n",
      "Gradient hat norm tensor(3.5451, device='cuda:0')\n",
      "alpha 0.02154434658586979\n",
      "linesearch loss tensor(0.4332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.4328, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 11.996099472045898\n",
      "Gradient hat norm tensor(30.8604, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3127, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 14.22761058807373\n",
      "Gradient hat norm tensor(22.6537, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.3445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 13.308355331420898\n",
      "Gradient hat norm tensor(15.3712, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.3298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 14.499673843383789\n",
      "Gradient hat norm tensor(12.1873, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 16.597837448120117\n",
      "Gradient hat norm tensor(9.2791, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 14.97247314453125\n",
      "Gradient hat norm tensor(9.6276, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.3140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 10.807526588439941\n",
      "Gradient hat norm tensor(10.8002, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.3091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 11.71006965637207\n",
      "Gradient hat norm tensor(9.5820, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 13.650449752807617\n",
      "Gradient hat norm tensor(7.7857, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "_ = run_all_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T=0 Gradient GRFBoost        0.8739\n",
       "Logistic L-BFSG, l2=1e-05    0.8738\n",
       "T=10 Gradient GRFBoost       0.8720\n",
       "T=5 Gradient GRFBoost        0.8694\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"cifar10_PLACEHOLDER.pkl\")\n",
    "df[\"acc_test\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T=10 Gradient GRFBoost               0.8757\n",
    "# T=5 Gradient GRFBoost                0.8743\n",
    "# T=0 Gradient GRFBoost                0.8737\n",
    "# T=15 Gradient GRFBoost               0.8737\n",
    "# T=4 End2End                          0.8663\n",
    "# Logistic L-BFSG, l2=0.001 lr=1.0     0.8309\n",
    "# Logistic L-BFSG, l2=1e-05 lr=1.0     0.8304\n",
    "# Logistic L-BFSG, l2=1e-06 lr=1.0     0.8303\n",
    "# Logistic L-BFSG, l2=0.1 lr=1.0       0.8295\n",
    "# Logistic L-BFSG, l2=0.0001 lr=1.0    0.8285\n",
    "# Logistic L-BFSG, l2=0.01 lr=1.0      0.8221\n",
    "# Logistic L-BFSG, l2=1 lr=1.0         0.7912\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Logistic L-BFSG, l2=1e-05    0.90666\n",
       "T=0 Gradient GRFBoost        0.90484\n",
       "T=10 Gradient GRFBoost       0.89434\n",
       "T=5 Gradient GRFBoost        0.88914\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"acc_train\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For T residual blocks, plot the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

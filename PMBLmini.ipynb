{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMLBmini experiments\n",
    "\n",
    "This notebook runs the PMLBmini experiments, and compares RANDOM FEATURE BOOSTING and END2END to the saved PMLBmini models\n",
    "\n",
    "NOTE that we assume tabmini is installed in the cwd https://github.com/RicardoKnauer/TabMini \n",
    "\n",
    "Should take no more than 30 minutes to run this notebook, ie run all models and datasets sequentially on a single CPU core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import tabmini\n",
    "import aeon\n",
    "from aeon.visualisation import plot_critical_difference, plot_significance\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from models.gridsearch_wrapper import SKLearnWrapper\n",
    "from models.random_feature_representation_boosting import GradientRFRBoostClassifier\n",
    "from models.end2end import End2EndMLPResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    save_dir = Path.cwd() / \"results\" / \"PMLBmini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####      Equal/Random Guessing        ######\n",
    "##############################################\n",
    "\n",
    "\n",
    "class EqualGuessing(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Guess probabilty 0.5 for each class\"\"\"\n",
    "        # Guess [0.5, 0.5]\n",
    "        return np.ones((X.shape[0], 2)) * 0.5\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        # Get the probabilities from predict_proba\n",
    "        proba = self.predict_proba(X)\n",
    "        # Calculate the log of ratios for binary classification\n",
    "        decision = np.log((proba[:, 1] + 1e-10) / (proba[:, 0] + 1e-10))\n",
    "        return decision\n",
    "    \n",
    "\n",
    "##################################################\n",
    "############# Grid Search wrapper    #############\n",
    "############# for custom estimators  #############\n",
    "##################################################\n",
    "\n",
    "\n",
    "class WrapperGridSearch(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, param_grid:Dict[str, List], out_name = \"n_classes\"):\n",
    "        self.param_grid = param_grid\n",
    "        self.out_name = out_name # 'n_classes' for GBRFRBoost, 'out_dim' for E2E_MLP_ResNet\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Performs a stratified 3-fold CV for hyperparameter tuning\n",
    "        based on self.param_grid, and fits the best model on the whole dataset\n",
    "        \"\"\"\n",
    "        # MinMaxScaler and convert to torch\n",
    "        self.classes_ = np.unique(y)\n",
    "        N, D = X.values.shape\n",
    "        self.scaler = MinMaxScaler()\n",
    "        X = self.scaler.fit_transform(X.values)\n",
    "        X = torch.tensor(X).float()\n",
    "        y = torch.tensor(y.values)[..., None].float()\n",
    "\n",
    "        # Perform grid search with k-fold cross-validation\n",
    "        param_grid = {**self.param_grid, **{\"seed\": [42]}, **{\"in_dim\": [D]}, **{self.out_name: [2]}}\n",
    "        if self.out_name == 'out_dim': # end2end has other param names\n",
    "            param_grid[\"batch_size\"] = [max(int(N*4/9-1), self.param_grid[\"batch_size\"][0])] # otherwise we can get a batch size of 1, error with batch norm\n",
    "            param_grid[self.out_name] = [1]\n",
    "        estimator = SKLearnWrapper()\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_grid= param_grid,\n",
    "            cv=StratifiedKFold(n_splits=3), #3-fold since PMLBmini uses 3-fold\n",
    "            verbose=1,\n",
    "        )\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        # fit best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(\"Best params:\", grid_search.best_params_)\n",
    "        best_model.set_model_eval()\n",
    "        self.model = best_model\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.scaler.fit_transform(X.values)\n",
    "        X = torch.tensor(X).float()\n",
    "        proba_0 = torch.nn.functional.sigmoid(self.model.predict(X)).cpu().detach().numpy()\n",
    "        return np.concatenate((1 - proba_0, proba_0), axis=1)\n",
    "\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        decision = np.log((proba[:, 1] + 1e-10) / (proba[:, 0] + 1e-10))\n",
    "        return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#### Run given model ####\n",
    "#########################\n",
    "\n",
    "def test_on_PMLBmini(\n",
    "        estimator: BaseEstimator,\n",
    "        estimator_name: str, \n",
    "        dataset_save_path = Config.save_dir / 'PMLBmini_dataset.pkl',\n",
    "        other_saved_methods = {}, #{'XGBoost'},\n",
    "        ):\n",
    "    \n",
    "    #download dataset, cache it\n",
    "    if not os.path.exists(dataset_save_path):\n",
    "        print(\"Dataset not found, downloading\")\n",
    "        dataset = tabmini.load_dataset(reduced=False)\n",
    "        os.makedirs(Config.save_dir, exist_ok=True)\n",
    "        with open(dataset_save_path, 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "    else:\n",
    "        print(\"Dataset found, loading\")\n",
    "        with open(dataset_save_path, 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "\n",
    "    # Perform the comparison\n",
    "    train_results, test_results = tabmini.compare(\n",
    "        estimator_name,\n",
    "        estimator,\n",
    "        dataset,\n",
    "        working_directory = Config.save_dir,\n",
    "        scoring_method=\"roc_auc\",\n",
    "        methods= {},\n",
    "        cv=3,\n",
    "        time_limit=3600,\n",
    "        device=\"cpu\",\n",
    "        n_jobs=1,\n",
    "    )\n",
    "    return train_results, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found, loading\n",
      "Evaluating EqualGuessing against {}...\n",
      "Comparing EqualGuessing on analcatdata_aids\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on analcatdata_asbestos\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on analcatdata_bankruptcy\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on analcatdata_creditscore\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on analcatdata_cyyoung8092\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on analcatdata_cyyoung9302\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on analcatdata_fraud\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on analcatdata_japansolvent\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on labor\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on lupus\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on parity5\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on postoperative_patient_data\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on analcatdata_boxing1\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on analcatdata_boxing2\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on appendicitis\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on backache\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on corral\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on glass2\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on hepatitis\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on molecular_biology_promoters\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on mux6\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on prnn_crabs\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on analcatdata_lawsuit\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on biomed\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on breast_cancer\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on heart_h\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on heart_statlog\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on hungarian\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on prnn_synth\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on sonar\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on spect\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on bupa\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on cleve\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on colic\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on haberman\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on heart_c\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on horse_colic\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on ionosphere\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on spectf\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on clean1\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on house_votes_84\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on irish\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on saheart\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Comparing EqualGuessing on vote\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing EqualGuessing\n",
      "Extracting test and train scores\n"
     ]
    }
   ],
   "source": [
    "train_guessing_and_xgboost, test_guessing_and_xgboost = test_on_PMLBmini(\n",
    "    EqualGuessing(),\n",
    "    'EqualGuessing',\n",
    "    other_saved_methods={\"XGBoost\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_logistic, test_logistic = test_on_PMLBmini(\n",
    "#     WrapperGridSearch(param_grid = {\n",
    "#                 'modelClass': [GradientRFRBoostClassifier],\n",
    "#                 'l2_cls': [1, 0.1, 0.001, 0.0001],\n",
    "#                 'n_layers': [0],\n",
    "#                 'upscale_type': [\"identity\"],\n",
    "#                 'use_batchnorm': [False],\n",
    "#                 'lbfgs_max_iter': [300],\n",
    "#                 'lbfgs_lr': [1.0],\n",
    "#             }),\n",
    "#     'Logistic (mine)',\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix this tomorrow... TODO TODO TODO TODO maybe enable batch norm and do grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_GRFRBoost_exp, test_GRFRBoost_exp = test_on_PMLBmini(\n",
    "#     WrapperGridSearch(param_grid = {\n",
    "#                 'modelClass': [GradientRFRBoostClassifier],\n",
    "#                 'l2_cls': [0.001],\n",
    "#                 'l2_ghat': [0.01],\n",
    "#                 'n_layers': [2],\n",
    "#                 'randfeat_xt_dim': [512],\n",
    "#                 'randfeat_x0_dim': [512],\n",
    "#                 'hidden_dim': [128],\n",
    "#                 # 'SWIM_scale': [1.0],\n",
    "#                 'use_batchnorm': [False],\n",
    "#             }),\n",
    "#     'GRFRBoost exp',\n",
    "#     other_saved_methods={},\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found, loading\n",
      "Evaluating GRFRBoost (ours) against {}...\n",
      "Comparing GRFRBoost (ours) on analcatdata_aids\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on analcatdata_asbestos\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on analcatdata_bankruptcy\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on analcatdata_creditscore\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on analcatdata_cyyoung8092\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on analcatdata_cyyoung9302\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on analcatdata_fraud\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 11, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 11, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 11, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on analcatdata_japansolvent\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on labor\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on lupus\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on parity5\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on postoperative_patient_data\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on analcatdata_boxing1\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on analcatdata_boxing2\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on appendicitis\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on backache\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 32, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 32, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 32, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on corral\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on glass2\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on hepatitis\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 19, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 19, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 19, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on molecular_biology_promoters\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 57, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 57, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 57, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on mux6\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on prnn_crabs\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on analcatdata_lawsuit\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on biomed\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on breast_cancer\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on heart_h\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on heart_statlog\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on hungarian\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on prnn_synth\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 2, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 2, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 2, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on sonar\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 60, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 60, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 60, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on spect\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on bupa\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on cleve\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on colic\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on haberman\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on heart_c\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on horse_colic\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on ionosphere\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 34, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 34, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 34, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on spectf\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 44, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 44, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 44, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on clean1\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 168, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 168, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 168, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on house_votes_84\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on irish\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on saheart\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Comparing GRFRBoost (ours) on vote\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing GRFRBoost (ours)\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'l2_ghat': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 2, 'randfeat_x0_dim': 512, 'randfeat_xt_dim': 512, 'seed': 42, 'use_batchnorm': False}\n",
      "Extracting test and train scores\n"
     ]
    }
   ],
   "source": [
    "train_GRFRBoost, test_GRFRBoost = test_on_PMLBmini(\n",
    "    WrapperGridSearch(param_grid = {\n",
    "                'modelClass': [GradientRFRBoostClassifier],\n",
    "                'l2_cls': [0.001],\n",
    "                'l2_ghat': [0.001],\n",
    "                'n_layers': [2],\n",
    "                'randfeat_xt_dim': [512],\n",
    "                'randfeat_x0_dim': [512],\n",
    "                'hidden_dim': [128],\n",
    "                # 'SWIM_scale': [1.0],\n",
    "                'use_batchnorm': [False],\n",
    "            }),\n",
    "    'GRFRBoost (ours)',\n",
    "    other_saved_methods={},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found, loading\n",
      "Evaluating RFNN against {}...\n",
      "Comparing RFNN on analcatdata_aids\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on analcatdata_asbestos\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on analcatdata_bankruptcy\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on analcatdata_creditscore\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on analcatdata_cyyoung8092\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on analcatdata_cyyoung9302\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 10, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on analcatdata_fraud\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 11, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 11, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 11, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on analcatdata_japansolvent\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on labor\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on lupus\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on parity5\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on postoperative_patient_data\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on analcatdata_boxing1\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on analcatdata_boxing2\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on appendicitis\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on backache\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 32, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 32, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 32, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on corral\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on glass2\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on hepatitis\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 19, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 19, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 19, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on molecular_biology_promoters\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 57, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 57, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 57, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on mux6\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 6, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on prnn_crabs\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 7, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on analcatdata_lawsuit\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 4, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on biomed\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 8, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on breast_cancer\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on heart_h\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on heart_statlog\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on hungarian\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on prnn_synth\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 2, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 2, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 2, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on sonar\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 60, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 60, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 60, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on spect\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on bupa\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on cleve\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on colic\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on haberman\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 3, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on heart_c\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 13, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on horse_colic\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 22, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on ionosphere\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 34, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 34, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 34, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on spectf\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 44, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 44, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 44, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on clean1\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 168, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 168, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 168, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on house_votes_84\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on irish\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 5, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on saheart\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 9, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Comparing RFNN on vote\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing RFNN\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best params: {'hidden_dim': 128, 'in_dim': 16, 'l2_cls': 0.001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42}\n",
      "Extracting test and train scores\n"
     ]
    }
   ],
   "source": [
    "# Random feature Neural Network\n",
    "train_RFNN, test_RFNN = test_on_PMLBmini(\n",
    "    WrapperGridSearch(param_grid = {\n",
    "                'modelClass': [GradientRFRBoostClassifier],\n",
    "                'l2_cls': [1, 0.1, 0.001, 0.0001],\n",
    "                'hidden_dim': [128],\n",
    "                'n_layers': [0],\n",
    "            }),\n",
    "    'RFNN',\n",
    "    other_saved_methods={},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found, loading\n",
      "Evaluating E2E_MLP_ResNet against {}...\n",
      "Comparing E2E_MLP_ResNet on analcatdata_aids\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1658.20it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2194.44it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1742.81it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2661.36it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1679.78it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2473.35it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2551.44it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1933.81it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2437.84it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2066.53it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2197.35it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1975.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1311.77it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1888.28it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2129.16it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 617.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 4, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1211.89it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1981.75it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2617.08it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2268.01it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2403.84it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2577.62it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2441.72it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2673.46it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2448.85it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2162.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2109.88it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2496.46it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2406.88it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2466.56it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2549.88it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1709.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 4, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1289.64it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1512.22it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1671.66it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1836.84it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1885.45it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1670.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2546.48it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1991.82it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2375.03it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2713.71it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2063.86it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2610.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1783.85it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2156.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2109.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1659.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 4, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on analcatdata_asbestos\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1548.88it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1714.32it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1152.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1701.80it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1506.43it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1631.77it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1887.97it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1974.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1942.44it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2065.96it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2057.54it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2020.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1981.72it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2044.64it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1680.30it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 667.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1738.36it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2037.22it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2046.50it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2046.20it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2165.70it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1911.40it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2228.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1680.19it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1593.70it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1288.39it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1709.59it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1596.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1395.56it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1388.78it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1461.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 841.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1320.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1905.72it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1842.76it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2131.54it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1562.20it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1857.34it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2084.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2082.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1843.30it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1967.34it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1916.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1857.94it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1871.40it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1936.96it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1927.82it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1528.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on analcatdata_bankruptcy\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1508.47it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2435.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2152.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2509.96it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2508.81it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1402.43it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1846.95it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2528.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2636.21it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2261.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2362.28it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2811.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2508.81it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2455.78it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2254.96it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2279.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2283.53it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2629.11it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2457.70it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2077.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2440.20it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2117.05it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2252.22it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2631.25it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2796.70it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2613.11it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2437.51it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1645.71it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2000.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2715.70it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2602.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1911.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1676.51it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2545.19it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2363.48it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1897.36it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1764.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2343.62it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2334.75it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2777.07it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2676.48it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2438.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2510.66it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2545.55it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2794.46it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2478.02it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2520.82it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1998.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on analcatdata_creditscore\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1419.97it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1197.08it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1265.31it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1699.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1812.03it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1767.24it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1772.62it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1780.74it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1849.34it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2090.71it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1821.21it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1902.61it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1854.05it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1750.42it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1872.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1909.77it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2145.61it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1953.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1928.45it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1839.55it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1800.21it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1755.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1191.81it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1412.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1804.39it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 954.03it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1239.82it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2150.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2044.24it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1762.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 179.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1859.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1999.95it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1957.64it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2028.10it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1772.52it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1908.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2063.25it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2069.15it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1810.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1925.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1886.01it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1823.00it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1821.39it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1805.58it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1793.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on analcatdata_cyyoung8092\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1550.75it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1027.58it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1865.07it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2097.78it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1948.48it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2019.53it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1867.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2084.78it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2100.62it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1859.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1806.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1937.14it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1777.77it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1904.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1928.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 174.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 10, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1872.40it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1849.37it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1974.94it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2119.76it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1985.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2021.12it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2229.27it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1912.12it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1774.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 915.83it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1054.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1919.30it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1897.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1893.16it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1928.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 10, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1902.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2145.39it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1966.60it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2278.85it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1953.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1878.27it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1945.35it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1853.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1768.80it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1811.53it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1809.19it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1716.02it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1864.00it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1769.08it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1786.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 10, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on analcatdata_cyyoung9302\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2125.42it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1990.46it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2079.96it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1906.96it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2059.19it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1914.10it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1896.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1954.20it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1911.74it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2195.01it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1865.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1794.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1930.90it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1762.91it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1784.35it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1446.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 10, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1844.84it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1905.75it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1922.55it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1536.45it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1224.55it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1503.82it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1866.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1895.22it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1831.68it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1806.23it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2079.61it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1979.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1974.97it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1828.35it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1990.37it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1360.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 10, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2014.56it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2115.31it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1954.08it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2013.97it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1911.48it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2103.64it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1895.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2041.29it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1107.29it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1105.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1881.50it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1915.03it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1832.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1929.86it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1892.74it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1665.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 10, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on analcatdata_fraud\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3038.03it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2642.58it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2303.85it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2617.95it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2652.61it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2493.30it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2174.64it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2709.15it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3014.95it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2707.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2549.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2941.03it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2639.42it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2638.31it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2945.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1364.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 11, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1910.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2348.48it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2497.01it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2567.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2702.11it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2878.59it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2212.22it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2942.89it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2032.71it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2959.85it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2823.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2631.03it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2859.36it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2565.27it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2673.40it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2140.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 11, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2492.50it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2343.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2565.95it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2513.06it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2836.16it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2612.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1500.88it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1999.54it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2409.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2532.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2613.06it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2381.37it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2699.56it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2107.76it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2420.95it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2223.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 11, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on analcatdata_japansolvent\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2772.36it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2483.40it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2619.48it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2604.83it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2558.39it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2356.75it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2152.25it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2418.77it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2606.83it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2481.20it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2481.30it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2688.94it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2513.11it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1449.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1464.83it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1818.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2474.76it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2234.46it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2114.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2352.08it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2808.56it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2562.55it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2572.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2694.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2447.42it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2471.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2525.83it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2457.60it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2557.40it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2606.61it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2728.53it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2068.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2275.14it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2275.31it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2332.20it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1609.95it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1982.15it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2588.49it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2398.02it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2436.00it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2570.98it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2755.66it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2450.76it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2528.31it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2409.46it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2762.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2630.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1891.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on labor\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2231.01it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1902.32it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1755.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1890.29it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2297.12it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2466.80it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2331.77it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2637.98it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1512.48it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1239.24it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2208.69it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2445.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2451.85it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2451.52it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2321.23it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1948.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 16, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2449.94it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2997.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2378.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2253.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2473.69it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2559.95it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2524.91it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2507.66it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2376.56it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2567.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2152.62it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1937.53it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2308.20it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2641.42it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2753.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1108.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 16, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1876.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1870.34it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2261.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2138.39it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2273.75it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2465.55it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2437.23it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2532.03it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2567.21it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2033.86it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2365.34it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2250.44it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2291.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2226.47it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2343.23it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1789.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 16, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on lupus\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2034.16it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2048.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1880.24it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2071.40it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1875.08it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1367.95it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1436.50it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2325.60it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2057.58it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2055.76it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1979.01it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2120.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2305.91it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2155.94it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2336.49it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1541.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1860.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2073.14it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2151.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2052.88it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2026.40it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2002.56it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2097.15it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1882.12it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1719.80it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1958.46it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1650.33it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2066.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1279.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 941.74it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2000.30it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1730.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1774.74it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1982.28it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1647.19it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2135.85it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1914.27it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1925.49it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2083.75it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1839.71it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2052.81it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1931.43it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2027.80it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1867.51it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1979.50it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2029.83it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2068.91it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1112.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on parity5\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2516.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1939.71it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1609.48it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2823.24it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3260.24it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2694.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2518.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3092.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3097.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2540.72it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3357.14it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2700.49it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2696.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2984.70it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3178.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2353.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 5, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3424.01it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3088.97it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2749.46it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2953.39it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3028.74it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2625.10it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3141.17it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3227.05it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2241.78it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 883.32it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1615.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1731.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3215.75it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2923.13it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2825.02it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2700.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 5, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3359.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2616.81it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2848.04it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3084.27it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3373.89it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2864.31it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2860.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2573.25it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3176.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3027.07it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2706.47it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2498.94it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2591.53it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3047.82it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 3235.76it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1562.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 5, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on postoperative_patient_data\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1093.05it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1854.14it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1949.75it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2123.95it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2101.88it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2343.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2115.59it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2283.07it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2120.12it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2003.94it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1896.76it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2187.88it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2165.85it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1895.59it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2164.54it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1702.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 8, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2149.82it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1930.13it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1902.29it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1949.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1849.50it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1908.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2134.54it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1581.05it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1145.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1515.17it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1922.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1718.16it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1964.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2249.44it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1888.70it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1606.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 8, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2024.86it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2150.78it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1980.97it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2123.81it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1883.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2222.23it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1947.76it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2007.80it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2073.96it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1790.75it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2063.69it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1941.03it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1399.30it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1371.36it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1893.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1511.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 8, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on analcatdata_boxing1\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1462.43it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1099.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1497.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1754.40it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1710.36it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1723.88it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1875.00it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1690.59it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1898.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1537.33it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1512.10it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 829.25it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1539.08it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 837.85it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 989.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1578.69it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1801.68it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1727.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1650.39it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1886.13it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1817.87it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1651.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1757.00it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1737.52it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1768.43it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1536.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1762.76it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1732.56it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1709.50it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1646.68it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1615.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1742.98it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1027.49it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1320.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1966.82it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1650.98it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1746.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1706.13it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1698.69it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1640.30it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1946.04it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1654.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1771.47it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1733.59it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1705.12it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on analcatdata_boxing2\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1574.93it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1747.85it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1710.54it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1721.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1718.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1949.15it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1725.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1699.22it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1612.24it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1685.59it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1626.06it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1546.14it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 739.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1022.03it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1818.52it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1576.23it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1650.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1906.91it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1615.66it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1729.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1784.91it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1557.33it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1573.45it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1571.08it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1588.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1532.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1607.61it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1659.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1636.44it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1568.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1747.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1735.55it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 994.15it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1020.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1651.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1794.66it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1593.44it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1666.35it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1769.97it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1765.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1427.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1488.05it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1547.83it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1539.42it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1603.39it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on appendicitis\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1992.80it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1913.05it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2001.86it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1991.94it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1943.25it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1965.68it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1926.47it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2020.90it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1915.53it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 811.32it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1199.02it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1704.66it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1889.84it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1697.71it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1883.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 7, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1735.60it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1731.25it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1856.16it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2093.17it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1789.96it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1863.61it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1876.45it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1871.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1872.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1898.19it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1803.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1673.57it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1650.37it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1793.49it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1836.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 126.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 7, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1812.81it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2070.89it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1913.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1932.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2024.21it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2133.02it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1856.24it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1963.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1842.84it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1688.01it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1799.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1751.18it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1696.47it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1882.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1676.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 7, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on backache\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 173.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 170.13it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 174.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 180.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 179.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 165.29it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 172.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 32, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 161.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 83.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.99it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.08it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 93.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 32, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 121.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 119.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.49it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.22it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 165.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 32, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on corral\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 116.63it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.58it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 174.29it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 171.29it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 179.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 181.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.39it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.63it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 151.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 178.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 178.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.92it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 175.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.95it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 174.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 170.22it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 172.09it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 174.41it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.29it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 181.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 181.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.58it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 165.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 170.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 181.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 182.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 170.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on glass2\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 179.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 185.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 186.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.70it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 169.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 182.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 180.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 172.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 181.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 169.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.42it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 178.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 181.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.56it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 171.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 170.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 173.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 169.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 166.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 166.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.56it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 166.08it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.96it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on hepatitis\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 139.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.96it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.66it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 101.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 19, 'loss': 'bce', 'lr': 1e-05, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 119.63it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.86it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.56it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 19, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 104.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.52it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 178.70it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.41it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.92it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 126.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 19, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on molecular_biology_promoters\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1184.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1351.64it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1516.49it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1989.30it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1918.33it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1867.23it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1928.33it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2045.87it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1549.43it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1501.74it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1599.54it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1819.34it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1934.14it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1810.54it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1963.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 57, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1975.22it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1906.39it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1017.20it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1161.04it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1512.51it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1773.72it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2021.35it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1935.42it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1917.02it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1758.94it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1920.56it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1787.09it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1765.53it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1735.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1717.99it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 57, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1728.23it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2216.78it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2004.73it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1759.72it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1896.02it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1857.80it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 2000.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1762.91it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1733.47it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1756.14it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1725.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1624.38it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1316.92it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1139.72it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1424.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 57, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on mux6\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1356.82it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1783.07it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1681.33it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1863.80it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1652.19it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1803.12it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1094.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1490.21it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1358.00it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1628.01it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1754.43it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1643.60it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1605.17it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1524.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 628.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1039.22it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1700.37it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1848.80it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1360.27it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1219.71it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1264.65it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1775.27it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1645.94it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1517.59it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1444.29it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 837.29it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1242.87it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1275.41it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1635.06it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1653.93it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1409.28it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1596.88it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1462.99it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1938.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1760.86it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1588.63it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1259.98it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1161.37it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 718.83it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1512.35it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1830.83it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1593.24it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1753.96it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1693.37it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|██████████| 30/30 [00:00<00:00, 1363.93it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 6, 'loss': 'bce', 'lr': 1e-06, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on prnn_crabs\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 150.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 180.56it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.68it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 74.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 7, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 111.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.86it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 111.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.49it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 104.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 66.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 7, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 151.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 65.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 7, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on analcatdata_lawsuit\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 100.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.10it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.95it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 126.93it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.96it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 65.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 77, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 4, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 113.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.13it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 126.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.56it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 65.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 77, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 4, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 123.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.12it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 113.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 77.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 77, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 4, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on biomed\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 147.66it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 113.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 97.54it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 124.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.39it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.09it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 72.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 8, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 132.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.42it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.68it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.09it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 119.09it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.49it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.31it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 78.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 8, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 157.09it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 95.47it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 137.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 126.12it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.62it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 58.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 8, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on breast_cancer\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 145.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.56it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.95it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 119.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.66it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.66it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 73.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 83, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 111.86it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.86it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 74.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 83, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 115.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.22it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.31it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.08it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.58it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.39it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 67.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 83, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on heart_h\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 86.70it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.12it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 119.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.31it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 57.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 86, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 129.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 119.41it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.09it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 104.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 66.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 86, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 128.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 119.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 103.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.31it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.42it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 66.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 86, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on heart_statlog\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 142.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 106.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.58it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.99it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 80.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 79, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 141.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 106.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.86it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.13it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.31it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 80.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 79, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 146.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 108.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 126.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.49it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 105.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 71.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 79, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on hungarian\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 97.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.93it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.63it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 70.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 86, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 104.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.49it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 97.49it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 103.56it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 108.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.12it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.66it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.96it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 92.38it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 126.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 75.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 86, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 1e-05, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 151.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.31it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.10it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 85.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 86, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on prnn_synth\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 154.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 169.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 171.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.70it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 170.41it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 126.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.63it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 70.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 72, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 2, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 111.22it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.10it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.06it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.66it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.31it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 92.50it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 144.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 102.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 86.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 73, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 2, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 165.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 169.96it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 166.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.70it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 88.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 73, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 2, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on sonar\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 141.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 169.96it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 170.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.68it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.06it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 92.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 60, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 148.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 175.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 175.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 171.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 86.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 60, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 171.93it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.52it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 177.31it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 176.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 171.58it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 173.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 178.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 174.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 93.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 64, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 60, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on spect\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 120.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.39it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.41it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 165.52it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 88.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 78, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 22, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 158.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 166.99it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 165.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 165.06it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 77.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 78, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 22, 'loss': 'bce', 'lr': 1e-05, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 154.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 165.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 166.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 167.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 73.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 78, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 22, 'loss': 'bce', 'lr': 1e-05, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on bupa\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 151.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.93it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.52it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 79.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 78.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 101, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 5, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 130.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.29it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.08it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.09it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.99it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 80.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 101, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 5, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 152.62it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.92it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.29it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.42it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.49it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 71.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 101, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 5, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on cleve\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 139.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.08it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.70it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 65.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 88, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 98.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.22it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 93.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 109.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.92it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.13it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 69.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 88, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 115.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 106.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.06it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 111.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 50.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 88, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on colic\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 100.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 99.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 106.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 113.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.95it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 105.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 113.63it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.62it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 104.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 104.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.22it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 66.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 107, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 22, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 132.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 103.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 113.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 126.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 102.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 60.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 107, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 22, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 116.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 126.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.63it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 108.10it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 108.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 107.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 61.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 108, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 22, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on haberman\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 120.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.22it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.13it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 65.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 89, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 134.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.86it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.68it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 126.29it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.41it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 103.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 69.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 89, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 144.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 165.68it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 165.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 166.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 109.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 69.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 89, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 3, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on heart_c\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 156.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.08it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.09it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.10it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.52it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.92it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 80.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 88, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 169.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 165.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.93it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.06it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.39it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.92it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 83.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 88, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 124.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.22it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.90it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 166.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 168.31it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 73.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 88, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 13, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on horse_colic\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 153.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.68it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.70it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 109.10it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 105.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 105.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 119.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 98.87it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 81.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 49.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 107, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 22, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 92.68it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 67.44it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 75.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 93.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.62it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 105.41it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 65.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 107, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 22, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 91.69it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 118.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 105.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.80it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 108.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 96.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 99.51it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 102.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 111.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 88.51it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 78.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 41.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 108, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 22, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on ionosphere\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 122.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 111.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 102.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.89it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.52it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.66it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.12it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.22it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.39it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.99it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 60.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 103, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 34, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 126.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.62it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.66it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.58it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.16it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 61.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 103, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 34, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 121.28it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 159.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.62it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 164.49it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 86.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 103, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 34, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on spectf\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 116.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.08it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 156.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.95it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 157.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.30it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 84.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 102, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 44, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 129.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 158.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 155.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 73.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 102, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 44, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 160.66it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 154.13it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 163.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.10it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.10it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 160.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 162.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 161.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 85.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 102, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 44, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on clean1\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 138.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.19it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.95it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.23it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.41it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.31it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 77.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 139, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 168, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 139.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.29it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.63it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.42it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.86it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 128.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.10it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.95it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 77.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 139, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 168, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 121.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.12it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.63it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.68it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 109.09it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.92it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.59it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.12it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 71.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 140, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 168, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on house_votes_84\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 150.10it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.92it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.96it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.06it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 131.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.06it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.58it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.49it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 75.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 127, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 16, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 149.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 148.86it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 147.42it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 150.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.58it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 151.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 76.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 127, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 16, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 114.42it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 143.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 152.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 149.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 153.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 69.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 127, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 16, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on irish\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 136.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 104.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 146.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.02it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 96.73it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 111.22it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 133.92it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 129.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 72.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 147, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 5, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 140.92it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 102.39it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.47it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 109.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 108.67it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 135.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.97it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 50.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 147, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 5, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 81.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 106.68it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 95.40it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 97.25it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 88.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 66.17it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.41it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 104.29it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 94.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 121.25it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 107.82it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 109.62it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 127.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.95it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 63.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 147, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 5, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on saheart\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 93.08it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 83.13it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 100.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 108.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 103.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 119.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 90.98it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 99.49it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 82.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.99it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 101.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 113.32it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 119.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 63.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 135, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 120.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 103.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 105.61it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 84.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.11it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 106.57it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.42it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 95.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 108.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 122.08it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.24it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 62.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 135, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 113.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 113.60it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 116.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.75it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 99.34it/s] \n",
      "100%|██████████| 30/30 [00:00<00:00, 71.37it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 103.74it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 108.01it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 113.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 111.00it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 103.55it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 120.51it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 102.20it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 55.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 135, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 9, 'loss': 'bce', 'lr': 0.0001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Comparing E2E_MLP_ResNet on vote\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 84.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 101.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 105.86it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 111.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 110.87it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 134.09it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.85it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.81it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 119.96it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 108.45it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 49.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 127, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 16, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 104.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.44it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 130.91it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.36it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 117.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 113.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 113.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 107.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 92.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 103.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 106.72it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 59.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 127, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 16, 'loss': 'bce', 'lr': 0.01, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 107.03it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 102.78it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 102.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 115.83it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 101.46it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 98.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 92.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 112.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 124.95it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 125.84it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 132.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 118.54it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 114.35it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 111.43it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 67.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'activation': ReLU(), 'batch_size': 127, 'bottleneck_dim': 512, 'end_lr_factor': 0.01, 'hidden_dim': 128, 'in_dim': 16, 'loss': 'bce', 'lr': 0.001, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Extracting test and train scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_E2E, test_E2E = test_on_PMLBmini(\n",
    "    WrapperGridSearch(param_grid = {\n",
    "            'modelClass': [End2EndMLPResNet],\n",
    "            'lr': np.logspace(-6, -2, 5),\n",
    "            'hidden_dim': [128],\n",
    "            'bottleneck_dim': [512],\n",
    "            'n_blocks': [2],\n",
    "            'loss': [\"bce\"],\n",
    "            'n_epochs': [30],\n",
    "            'end_lr_factor': [0.01],\n",
    "            'weight_decay': [0.00001],\n",
    "            'batch_size': [64],\n",
    "            'activation': [nn.ReLU()],\n",
    "            },\n",
    "            out_name='out_dim',),\n",
    "    'E2E_MLP_ResNet',\n",
    "    other_saved_methods={},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AutoPrognosis</th>\n",
       "      <th>AutoGluon</th>\n",
       "      <th>TabPFN</th>\n",
       "      <th>Logistic regression</th>\n",
       "      <th>HyperFast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>parity5</th>\n",
       "      <td>0.27</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_fraud</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_aids</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_bankruptcy</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_japansolvent</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labor</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_asbestos</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lupus</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postoperative_patient_data</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_cyyoung9302</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_cyyoung8092</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_creditscore</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appendicitis</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>molecular_biology_promoters</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_boxing1</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mux6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_boxing2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hepatitis</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corral</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass2</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backache</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prnn_crabs</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sonar</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biomed</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prnn_synth</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_lawsuit</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spect</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_statlog</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_cancer</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_h</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hungarian</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleve</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_c</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haberman</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bupa</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectf</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ionosphere</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colic</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horse_colic</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house_votes_84</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vote</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saheart</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irish</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             AutoPrognosis  AutoGluon  TabPFN  \\\n",
       "parity5                               0.27       1.00    0.02   \n",
       "analcatdata_fraud                     0.86       0.80    0.79   \n",
       "analcatdata_aids                      0.73       0.77    0.63   \n",
       "analcatdata_bankruptcy                0.98       0.98    0.96   \n",
       "analcatdata_japansolvent              0.85       0.88    0.91   \n",
       "labor                                 0.88       0.94    0.99   \n",
       "analcatdata_asbestos                  0.87       0.84    0.85   \n",
       "lupus                                 0.84       0.79    0.82   \n",
       "postoperative_patient_data            0.49       0.55    0.44   \n",
       "analcatdata_cyyoung9302               0.89       0.85    0.87   \n",
       "analcatdata_cyyoung8092               0.73       0.90    0.85   \n",
       "analcatdata_creditscore               1.00       0.99    1.00   \n",
       "appendicitis                          0.78       0.84    0.82   \n",
       "molecular_biology_promoters           0.88       0.93    0.88   \n",
       "analcatdata_boxing1                   0.89       0.89    0.76   \n",
       "mux6                                  1.00       1.00    1.00   \n",
       "analcatdata_boxing2                   0.82       0.78    0.71   \n",
       "hepatitis                             0.85       0.82    0.85   \n",
       "corral                                1.00       1.00    1.00   \n",
       "glass2                                0.89       0.91    0.89   \n",
       "backache                              0.60       0.72    0.75   \n",
       "prnn_crabs                            1.00       1.00    1.00   \n",
       "sonar                                 0.88       0.93    0.92   \n",
       "biomed                                1.00       0.96    0.95   \n",
       "prnn_synth                            0.94       0.95    0.95   \n",
       "analcatdata_lawsuit                   0.99       0.99    1.00   \n",
       "spect                                 0.84       0.82    0.83   \n",
       "heart_statlog                         0.91       0.89    0.90   \n",
       "breast_cancer                         0.69       0.69    0.73   \n",
       "heart_h                               0.87       0.89    0.88   \n",
       "hungarian                             0.86       0.86    0.86   \n",
       "cleve                                 0.90       0.89    0.89   \n",
       "heart_c                               0.91       0.91    0.91   \n",
       "haberman                              0.70       0.71    0.72   \n",
       "bupa                                  0.66       0.64    0.68   \n",
       "spectf                                0.91       0.94    0.93   \n",
       "ionosphere                            0.97       0.98    0.98   \n",
       "colic                                 0.87       0.86    0.87   \n",
       "horse_colic                           0.88       0.85    0.84   \n",
       "house_votes_84                        0.99       0.99    0.99   \n",
       "vote                                  1.00       0.99    1.00   \n",
       "saheart                               0.77       0.76    0.77   \n",
       "clean1                                0.93       1.00    0.99   \n",
       "irish                                 1.00       1.00    1.00   \n",
       "\n",
       "                             Logistic regression  HyperFast  \n",
       "parity5                                     0.17       0.02  \n",
       "analcatdata_fraud                           0.77       0.73  \n",
       "analcatdata_aids                            0.61       0.53  \n",
       "analcatdata_bankruptcy                      0.97       0.88  \n",
       "analcatdata_japansolvent                    0.85       0.91  \n",
       "labor                                       0.97       0.98  \n",
       "analcatdata_asbestos                        0.86       0.87  \n",
       "lupus                                       0.85       0.79  \n",
       "postoperative_patient_data                  0.38       0.34  \n",
       "analcatdata_cyyoung9302                     0.87       0.84  \n",
       "analcatdata_cyyoung8092                     0.79       0.84  \n",
       "analcatdata_creditscore                     0.94       0.87  \n",
       "appendicitis                                0.84       0.87  \n",
       "molecular_biology_promoters                 0.88       0.89  \n",
       "analcatdata_boxing1                         0.67       0.67  \n",
       "mux6                                        0.70       0.95  \n",
       "analcatdata_boxing2                         0.68       0.70  \n",
       "hepatitis                                   0.84       0.83  \n",
       "corral                                      0.96       1.00  \n",
       "glass2                                      0.72       0.79  \n",
       "backache                                    0.72       0.78  \n",
       "prnn_crabs                                  1.00       0.81  \n",
       "sonar                                       0.85       0.89  \n",
       "biomed                                      0.94       0.93  \n",
       "prnn_synth                                  0.94       0.94  \n",
       "analcatdata_lawsuit                         1.00       0.98  \n",
       "spect                                       0.82       0.83  \n",
       "heart_statlog                               0.89       0.89  \n",
       "breast_cancer                               0.70       0.69  \n",
       "heart_h                                     0.86       0.85  \n",
       "hungarian                                   0.85       0.84  \n",
       "cleve                                       0.88       0.88  \n",
       "heart_c                                     0.91       0.89  \n",
       "haberman                                    0.66       0.58  \n",
       "bupa                                        0.67       0.66  \n",
       "spectf                                      0.88       0.87  \n",
       "ionosphere                                  0.90       0.97  \n",
       "colic                                       0.86       0.86  \n",
       "horse_colic                                 0.82       0.83  \n",
       "house_votes_84                              0.99       0.98  \n",
       "vote                                        0.99       0.99  \n",
       "saheart                                     0.77       0.76  \n",
       "clean1                                      1.00       0.96  \n",
       "irish                                       0.83       0.97  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV and set the index\n",
    "saved_results = pd.read_csv('https://raw.githubusercontent.com/RicardoKnauer/TabMini/master/plotting/results/test_scores_wide_3600.csv', delimiter=\";\", index_col=0)\n",
    "saved_results.index.name = None\n",
    "saved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = saved_results.copy()\n",
    "# combined_results = pd.read_csv(Config.save_dir / \"combined_results.csv\", index_col=0)\n",
    "for df in [test_guessing_and_xgboost, \n",
    "           test_RFNN, \n",
    "           test_E2E, \n",
    "           test_GRFRBoost,\n",
    "           ]:\n",
    "    combined_results = combined_results.join(df, how='inner')\n",
    "combined_results = combined_results.round(2) # since PMLBmini's resulst are rounded to 2 decimals, for fair comparison\n",
    "combined_results\n",
    "\n",
    "#save\n",
    "combined_results.to_csv(Config.save_dir / \"combined_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AutoPrognosis</th>\n",
       "      <th>AutoGluon</th>\n",
       "      <th>TabPFN</th>\n",
       "      <th>Logistic regression</th>\n",
       "      <th>HyperFast</th>\n",
       "      <th>EqualGuessing</th>\n",
       "      <th>RFNN</th>\n",
       "      <th>E2E_MLP_ResNet</th>\n",
       "      <th>GRFRBoost (ours)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>parity5</th>\n",
       "      <td>0.27</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_fraud</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_aids</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_bankruptcy</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_japansolvent</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labor</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_asbestos</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lupus</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postoperative_patient_data</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_cyyoung9302</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_cyyoung8092</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_creditscore</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appendicitis</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>molecular_biology_promoters</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_boxing1</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mux6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_boxing2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hepatitis</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corral</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass2</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backache</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prnn_crabs</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sonar</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biomed</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prnn_synth</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_lawsuit</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spect</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_statlog</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_cancer</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_h</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hungarian</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleve</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_c</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haberman</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bupa</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectf</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ionosphere</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colic</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horse_colic</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house_votes_84</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vote</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saheart</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irish</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             AutoPrognosis  AutoGluon  TabPFN  \\\n",
       "parity5                               0.27       1.00    0.02   \n",
       "analcatdata_fraud                     0.86       0.80    0.79   \n",
       "analcatdata_aids                      0.73       0.77    0.63   \n",
       "analcatdata_bankruptcy                0.98       0.98    0.96   \n",
       "analcatdata_japansolvent              0.85       0.88    0.91   \n",
       "labor                                 0.88       0.94    0.99   \n",
       "analcatdata_asbestos                  0.87       0.84    0.85   \n",
       "lupus                                 0.84       0.79    0.82   \n",
       "postoperative_patient_data            0.49       0.55    0.44   \n",
       "analcatdata_cyyoung9302               0.89       0.85    0.87   \n",
       "analcatdata_cyyoung8092               0.73       0.90    0.85   \n",
       "analcatdata_creditscore               1.00       0.99    1.00   \n",
       "appendicitis                          0.78       0.84    0.82   \n",
       "molecular_biology_promoters           0.88       0.93    0.88   \n",
       "analcatdata_boxing1                   0.89       0.89    0.76   \n",
       "mux6                                  1.00       1.00    1.00   \n",
       "analcatdata_boxing2                   0.82       0.78    0.71   \n",
       "hepatitis                             0.85       0.82    0.85   \n",
       "corral                                1.00       1.00    1.00   \n",
       "glass2                                0.89       0.91    0.89   \n",
       "backache                              0.60       0.72    0.75   \n",
       "prnn_crabs                            1.00       1.00    1.00   \n",
       "sonar                                 0.88       0.93    0.92   \n",
       "biomed                                1.00       0.96    0.95   \n",
       "prnn_synth                            0.94       0.95    0.95   \n",
       "analcatdata_lawsuit                   0.99       0.99    1.00   \n",
       "spect                                 0.84       0.82    0.83   \n",
       "heart_statlog                         0.91       0.89    0.90   \n",
       "breast_cancer                         0.69       0.69    0.73   \n",
       "heart_h                               0.87       0.89    0.88   \n",
       "hungarian                             0.86       0.86    0.86   \n",
       "cleve                                 0.90       0.89    0.89   \n",
       "heart_c                               0.91       0.91    0.91   \n",
       "haberman                              0.70       0.71    0.72   \n",
       "bupa                                  0.66       0.64    0.68   \n",
       "spectf                                0.91       0.94    0.93   \n",
       "ionosphere                            0.97       0.98    0.98   \n",
       "colic                                 0.87       0.86    0.87   \n",
       "horse_colic                           0.88       0.85    0.84   \n",
       "house_votes_84                        0.99       0.99    0.99   \n",
       "vote                                  1.00       0.99    1.00   \n",
       "saheart                               0.77       0.76    0.77   \n",
       "clean1                                0.93       1.00    0.99   \n",
       "irish                                 1.00       1.00    1.00   \n",
       "\n",
       "                             Logistic regression  HyperFast  EqualGuessing  \\\n",
       "parity5                                     0.17       0.02            0.5   \n",
       "analcatdata_fraud                           0.77       0.73            0.5   \n",
       "analcatdata_aids                            0.61       0.53            0.5   \n",
       "analcatdata_bankruptcy                      0.97       0.88            0.5   \n",
       "analcatdata_japansolvent                    0.85       0.91            0.5   \n",
       "labor                                       0.97       0.98            0.5   \n",
       "analcatdata_asbestos                        0.86       0.87            0.5   \n",
       "lupus                                       0.85       0.79            0.5   \n",
       "postoperative_patient_data                  0.38       0.34            0.5   \n",
       "analcatdata_cyyoung9302                     0.87       0.84            0.5   \n",
       "analcatdata_cyyoung8092                     0.79       0.84            0.5   \n",
       "analcatdata_creditscore                     0.94       0.87            0.5   \n",
       "appendicitis                                0.84       0.87            0.5   \n",
       "molecular_biology_promoters                 0.88       0.89            0.5   \n",
       "analcatdata_boxing1                         0.67       0.67            0.5   \n",
       "mux6                                        0.70       0.95            0.5   \n",
       "analcatdata_boxing2                         0.68       0.70            0.5   \n",
       "hepatitis                                   0.84       0.83            0.5   \n",
       "corral                                      0.96       1.00            0.5   \n",
       "glass2                                      0.72       0.79            0.5   \n",
       "backache                                    0.72       0.78            0.5   \n",
       "prnn_crabs                                  1.00       0.81            0.5   \n",
       "sonar                                       0.85       0.89            0.5   \n",
       "biomed                                      0.94       0.93            0.5   \n",
       "prnn_synth                                  0.94       0.94            0.5   \n",
       "analcatdata_lawsuit                         1.00       0.98            0.5   \n",
       "spect                                       0.82       0.83            0.5   \n",
       "heart_statlog                               0.89       0.89            0.5   \n",
       "breast_cancer                               0.70       0.69            0.5   \n",
       "heart_h                                     0.86       0.85            0.5   \n",
       "hungarian                                   0.85       0.84            0.5   \n",
       "cleve                                       0.88       0.88            0.5   \n",
       "heart_c                                     0.91       0.89            0.5   \n",
       "haberman                                    0.66       0.58            0.5   \n",
       "bupa                                        0.67       0.66            0.5   \n",
       "spectf                                      0.88       0.87            0.5   \n",
       "ionosphere                                  0.90       0.97            0.5   \n",
       "colic                                       0.86       0.86            0.5   \n",
       "horse_colic                                 0.82       0.83            0.5   \n",
       "house_votes_84                              0.99       0.98            0.5   \n",
       "vote                                        0.99       0.99            0.5   \n",
       "saheart                                     0.77       0.76            0.5   \n",
       "clean1                                      1.00       0.96            0.5   \n",
       "irish                                       0.83       0.97            0.5   \n",
       "\n",
       "                             RFNN  E2E_MLP_ResNet  GRFRBoost (ours)  \n",
       "parity5                      0.75            0.50              1.00  \n",
       "analcatdata_fraud            0.88            0.62              1.00  \n",
       "analcatdata_aids             0.82            0.29              1.00  \n",
       "analcatdata_bankruptcy       0.99            0.07              1.00  \n",
       "analcatdata_japansolvent     0.95            0.66              1.00  \n",
       "labor                        1.00            0.35              1.00  \n",
       "analcatdata_asbestos         0.86            0.28              1.00  \n",
       "lupus                        0.85            0.67              1.00  \n",
       "postoperative_patient_data   0.77            0.53              0.99  \n",
       "analcatdata_cyyoung9302      0.92            0.38              1.00  \n",
       "analcatdata_cyyoung8092      0.93            0.56              1.00  \n",
       "analcatdata_creditscore      1.00            0.64              1.00  \n",
       "appendicitis                 0.86            0.71              1.00  \n",
       "molecular_biology_promoters  0.99            0.67              1.00  \n",
       "analcatdata_boxing1          0.69            0.38              1.00  \n",
       "mux6                         1.00            0.53              1.00  \n",
       "analcatdata_boxing2          0.76            0.44              0.99  \n",
       "hepatitis                    0.92            0.93              1.00  \n",
       "corral                       1.00            1.00              1.00  \n",
       "glass2                       0.82            0.92              1.00  \n",
       "backache                     0.82            1.00              1.00  \n",
       "prnn_crabs                   1.00            1.00              1.00  \n",
       "sonar                        0.96            1.00              1.00  \n",
       "biomed                       0.97            1.00              1.00  \n",
       "prnn_synth                   0.94            0.97              0.99  \n",
       "analcatdata_lawsuit          1.00            1.00              1.00  \n",
       "spect                        0.87            0.90              0.98  \n",
       "heart_statlog                0.93            0.99              1.00  \n",
       "breast_cancer                0.75            0.96              0.96  \n",
       "heart_h                      0.88            0.99              0.99  \n",
       "hungarian                    0.89            0.96              0.99  \n",
       "cleve                        0.91            1.00              1.00  \n",
       "heart_c                      0.93            1.00              1.00  \n",
       "haberman                     0.73            0.79              0.96  \n",
       "bupa                         0.69            0.85              0.97  \n",
       "spectf                       0.93            1.00              1.00  \n",
       "ionosphere                   0.97            1.00              1.00  \n",
       "colic                        0.90            1.00              1.00  \n",
       "horse_colic                  0.90            1.00              1.00  \n",
       "house_votes_84               0.99            1.00              1.00  \n",
       "vote                         1.00            1.00              1.00  \n",
       "saheart                      0.80            0.91              0.95  \n",
       "clean1                       1.00            1.00              1.00  \n",
       "irish                        0.96            1.00              1.00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results = pd.read_csv(Config.save_dir / \"combined_results.csv\", index_col=0)\n",
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x280 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = plot_critical_difference(combined_results.values,\n",
    "                                combined_results.columns.tolist(), \n",
    "                                alpha=0.05, \n",
    "                                lower_better=False)\n",
    "plot[0].savefig(Config.save_dir / \"PMLBmini_critical_difference.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do i want an AUC plot? table? or just the critical difference plot?\n",
    "\n",
    "\n",
    "\n",
    "# TODO YES I WANT TABLE LIKE CONCENTRIC CIRCLES\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

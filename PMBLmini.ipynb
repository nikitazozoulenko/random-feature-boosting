{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMLBmini experiments\n",
    "\n",
    "This notebook runs the PMLBmini experiments, and compares RANDOM FEATURE BOOSTING and END2END to the saved PMLBmini models\n",
    "\n",
    "NOTE that we assume tabmini is installed in the cwd https://github.com/RicardoKnauer/TabMini \n",
    "\n",
    "Should take no more than 30 minutes to run this notebook, ie run all models and datasets sequentially on a single CPU core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import tabmini\n",
    "import aeon\n",
    "from aeon.visualisation import plot_critical_difference, plot_significance\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from models.gridsearch_wrapper import SKLearnWrapper\n",
    "from models.random_feature_representation_boosting import GradientRFRBoostClassifier\n",
    "from models.end2end import End2EndMLPResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    save_dir = Path.cwd() / \"results\" / \"PMLBmini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####      Equal/Random Guessing        ######\n",
    "##############################################\n",
    "\n",
    "\n",
    "class EqualGuessing(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Guess probabilty 0.5 for each class\"\"\"\n",
    "        # Guess [0.5, 0.5]\n",
    "        return np.ones((X.shape[0], 2)) * 0.5\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        # Get the probabilities from predict_proba\n",
    "        proba = self.predict_proba(X)\n",
    "        # Calculate the log of ratios for binary classification\n",
    "        decision = np.log((proba[:, 1] + 1e-10) / (proba[:, 0] + 1e-10))\n",
    "        return decision\n",
    "    \n",
    "\n",
    "##################################################\n",
    "############# Grid Search wrapper    #############\n",
    "############# for custom estimators  #############\n",
    "##################################################\n",
    "\n",
    "\n",
    "class WrapperGridSearch(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, param_grid:Dict[str, List], out_name = \"n_classes\", verbose=0):\n",
    "        self.param_grid = param_grid\n",
    "        self.out_name = out_name # 'n_classes' for GBRFRBoost, 'out_dim' for E2E_MLP_ResNet\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Performs a stratified 3-fold CV for hyperparameter tuning\n",
    "        based on self.param_grid, and fits the best model on the whole dataset\n",
    "        \"\"\"\n",
    "        # MinMaxScaler and convert to torch\n",
    "        self.classes_ = np.unique(y)\n",
    "        N, D = X.values.shape\n",
    "        self.scaler = MinMaxScaler()\n",
    "        X = self.scaler.fit_transform(X.values)\n",
    "        X = torch.tensor(X).float()\n",
    "        y = torch.tensor(y.values)[..., None].float()\n",
    "\n",
    "        # Perform grid search with k-fold cross-validation\n",
    "        param_grid = {**self.param_grid, **{\"seed\": [42]}, **{\"in_dim\": [D]}, **{self.out_name: [2]}}\n",
    "        if self.out_name == 'out_dim': # end2end has other param names\n",
    "            param_grid[\"batch_size\"] = [max(int(N*4/9-1), self.param_grid[\"batch_size\"][0])] # otherwise we can get a batch size of 1, error with batch norm\n",
    "            param_grid[self.out_name] = [1]\n",
    "        estimator = SKLearnWrapper()\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_grid= param_grid,\n",
    "            cv=StratifiedKFold(n_splits=3), #3-fold since PMLBmini uses 3-fold\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        # fit best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(\"Best params:\", grid_search.best_params_)\n",
    "        best_model.set_model_eval()\n",
    "        self.model = best_model\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.scaler.fit_transform(X.values)\n",
    "        X = torch.tensor(X).float()\n",
    "        proba_0 = torch.nn.functional.sigmoid(self.model.predict(X)).cpu().detach().numpy()\n",
    "        return np.concatenate((1 - proba_0, proba_0), axis=1)\n",
    "\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        decision = np.log((proba[:, 1] + 1e-10) / (proba[:, 0] + 1e-10))\n",
    "        return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#### Run given model ####\n",
    "#########################\n",
    "\n",
    "\n",
    "def test_on_PMLBmini(\n",
    "        estimator: BaseEstimator,\n",
    "        estimator_name: str, \n",
    "        dataset_save_path = Config.save_dir / 'PMLBmini_dataset.pkl',\n",
    "        other_saved_methods = {}, #{'XGBoost'},\n",
    "        ):\n",
    "    \n",
    "    #download dataset, cache it\n",
    "    if not os.path.exists(dataset_save_path):\n",
    "        print(\"Dataset not found, downloading\")\n",
    "        dataset = tabmini.load_dataset(reduced=False)\n",
    "        os.makedirs(Config.save_dir, exist_ok=True)\n",
    "        with open(dataset_save_path, 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "    else:\n",
    "        print(\"Dataset found, loading\")\n",
    "        with open(dataset_save_path, 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "\n",
    "    # Perform the comparison\n",
    "    test_results, train_results = tabmini.compare(\n",
    "        estimator_name,\n",
    "        estimator,\n",
    "        dataset,\n",
    "        working_directory = Config.save_dir,\n",
    "        scoring_method=\"roc_auc\",\n",
    "        methods= other_saved_methods,\n",
    "        cv=5,\n",
    "        time_limit=3600,\n",
    "        device=\"cpu\",\n",
    "        n_jobs=1,\n",
    "    )\n",
    "    return train_results, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_guessing_and_xgboost, test_guessing_and_xgboost = test_on_PMLBmini(\n",
    "    EqualGuessing(),\n",
    "    'EqualGuessing',\n",
    "    other_saved_methods={\"XGBoost\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_logistic, test_logistic = test_on_PMLBmini(\n",
    "#     WrapperGridSearch(param_grid = {\n",
    "#                 'modelClass': [GradientRFRBoostClassifier],\n",
    "#                 'l2_cls': [1, 0.1, 0.001, 0.0001],\n",
    "#                 'n_layers': [0],\n",
    "#                 'upscale_type': [\"identity\"],\n",
    "#                 'use_batchnorm': [False],\n",
    "#                 'lbfgs_max_iter': [300],\n",
    "#                 'lbfgs_lr': [1.0],\n",
    "#             },\n",
    "#             verbose=3),\n",
    "#     'Logistic (mine)',\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix this tomorrow... TODO TODO TODO TODO maybe enable batch norm and do grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_GRFRBoost_exp, test_GRFRBoost_exp = test_on_PMLBmini(\n",
    "#     WrapperGridSearch(param_grid = {\n",
    "#                 'modelClass': [GradientRFRBoostClassifier],\n",
    "#                 'l2_cls': [0.001],\n",
    "#                 'l2_ghat': [0.01],\n",
    "#                 'n_layers': [2],\n",
    "#                 'randfeat_xt_dim': [512],\n",
    "#                 'randfeat_x0_dim': [512],\n",
    "#                 'hidden_dim': [128],\n",
    "#                 # 'SWIM_scale': [1.0],\n",
    "#                 'use_batchnorm': [False],\n",
    "#             }),\n",
    "#     'GRFRBoost exp',\n",
    "#     other_saved_methods={},\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_GRFRBoostID, test_GRFRBoostID = test_on_PMLBmini(\n",
    "    WrapperGridSearch(param_grid = {\n",
    "                'modelClass': [GradientRFRBoostClassifier],\n",
    "                'l2_cls': [10, 1, 0.1, 0.001, 0.0001],\n",
    "                'l2_ghat': [10, 1, 0.1, 0.01],\n",
    "                'n_layers': [1],\n",
    "                'randfeat_xt_dim': [512],\n",
    "                'randfeat_x0_dim': [512],\n",
    "                # 'hidden_dim': [512],\n",
    "                # 'upscale_type': [\"identity\"],\n",
    "                # 'feature_type': [\"iid\"],\n",
    "                'upscale_type': [\"identity\"],\n",
    "                'feature_type': [\"SWIM\"],\n",
    "                # 'hidden_dim': [128],\n",
    "                # 'SWIM_scale': [1.0],\n",
    "                'use_batchnorm': [False],\n",
    "                'boost_lr': [1.0],\n",
    "                #'activation': ['relu'],\n",
    "                \n",
    "            },\n",
    "            verbose=3,),\n",
    "    'GRFRBoostID',\n",
    "    other_saved_methods={},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_GRFRBoost, test_GRFRBoost = test_on_PMLBmini(\n",
    "    WrapperGridSearch(param_grid = {\n",
    "                'modelClass': [GradientRFRBoostClassifier],\n",
    "                'l2_cls': [10, 1, 0.1, 0.001, 0.0001],\n",
    "                'l2_ghat': [10, 1, 0.1, 0.01],\n",
    "                'n_layers': [1],\n",
    "                'randfeat_xt_dim': [512],\n",
    "                'randfeat_x0_dim': [512],\n",
    "                'hidden_dim': [512],\n",
    "                # 'upscale_type': [\"identity\"],\n",
    "                # 'feature_type': [\"iid\"],\n",
    "                'upscale_type': [\"iid\"],\n",
    "                'iid_scale': [10.0],\n",
    "                'feature_type': [\"SWIM\"],\n",
    "                # 'hidden_dim': [128],\n",
    "                # 'SWIM_scale': [1.0],\n",
    "                'use_batchnorm': [False],\n",
    "                'boost_lr': [1.0],\n",
    "                #'activation': ['relu'],\n",
    "                \n",
    "            },\n",
    "            verbose=3,),\n",
    "    'GRFRBoost',\n",
    "    other_saved_methods={},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random feature Neural Network\n",
    "train_RFNN, test_RFNN = test_on_PMLBmini(\n",
    "    WrapperGridSearch(param_grid = {\n",
    "                'modelClass': [GradientRFRBoostClassifier],\n",
    "                'l2_cls': [1, 0.1, 0.001, 0.0001],\n",
    "                'hidden_dim': [512],\n",
    "                'n_layers': [0],\n",
    "            }),\n",
    "    'RFNN',\n",
    "    other_saved_methods={},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_GRFRBoost.join(test_RFNN).join(test_GRFRBoost, lsuffix='_train', rsuffix='_test').join(test_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_E2E, test_E2E = test_on_PMLBmini(\n",
    "    WrapperGridSearch(param_grid = {\n",
    "            'modelClass': [End2EndMLPResNet],\n",
    "            'lr': np.logspace(-2, -1, 2),\n",
    "            'hidden_dim': [32],\n",
    "            'bottleneck_dim': [32],\n",
    "            'n_blocks': [2],\n",
    "            'loss': [\"bce\"],\n",
    "            'n_epochs': [30],\n",
    "            'end_lr_factor': [0.01],\n",
    "            'weight_decay': [0.00001],\n",
    "            'batch_size': [32],\n",
    "            'activation': [nn.ReLU()],\n",
    "            },\n",
    "            out_name='out_dim',),\n",
    "    'E2E_MLP_ResNet',\n",
    "    other_saved_methods={},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV and set the index\n",
    "saved_results = pd.read_csv('https://raw.githubusercontent.com/RicardoKnauer/TabMini/master/plotting/results/test_scores_wide_3600.csv', delimiter=\";\", index_col=0)\n",
    "saved_results.index.name = None\n",
    "saved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = saved_results.copy()\n",
    "# combined_results = pd.read_csv(Config.save_dir / \"combined_results.csv\", index_col=0)\n",
    "for df in [test_guessing_and_xgboost, \n",
    "           test_RFNN, \n",
    "           #test_E2E, \n",
    "           test_GRFRBoost,\n",
    "           #test_logistic,\n",
    "           ]:\n",
    "    combined_results = combined_results.join(df, how='inner')\n",
    "combined_results = combined_results.round(2) # since PMLBmini's resulst are rounded to 2 decimals, for fair comparison\n",
    "combined_results\n",
    "\n",
    "#save\n",
    "combined_results.to_csv(Config.save_dir / \"combined_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = pd.read_csv(Config.save_dir / \"combined_results.csv\", index_col=0)\n",
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_results[[\"GRFRBoost (ours)\", \"Logistic (mine)\", \"RFNN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the plot\n",
    "plot = plot_critical_difference(combined_results.values,\n",
    "                                combined_results.columns.tolist(), \n",
    "                                alpha=0.05, \n",
    "                                lower_better=False)\n",
    "\n",
    "# Retrieve the figure and axes from the plot\n",
    "fig = plot[0].figure\n",
    "ax = plot[0]\n",
    "\n",
    "# Adjust figure size\n",
    "fig.set_size_inches(6, 3)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figures\n",
    "fig.savefig(Config.save_dir / \"PMLBmini_critical_difference.eps\", bbox_inches='tight')\n",
    "fig.savefig(Config.save_dir / \"PMLBmini_critical_difference.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_latex_table(df):\n",
    "    table = \"\"\"\n",
    "\\\\begin{table}[t]\n",
    "\\\\caption{Test accuracies on the concentric circles task.}\n",
    "\\\\label{tab:concentric-circles}\n",
    "\\\\vskip 0.15in\n",
    "\\\\begin{center}\n",
    "\\\\begin{small}\n",
    "\\\\begin{sc}\n",
    "\\\\begin{tabular}{lcc}\n",
    "\\\\toprule\n",
    "Model & Mean Acc & Std Dev \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "    for model_name in df.columns:\n",
    "        accs = df[model_name]\n",
    "        mean_acc = np.mean(accs)\n",
    "        std_acc = np.std(accs)\n",
    "        table += f\"{model_name} & {mean_acc:.4f} & {std_acc:.4f} \\\\\\\\\\n\"\n",
    "    \n",
    "    table += \"\"\"\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{sc}\n",
    "\\\\end{small}\n",
    "\\\\end{center}\n",
    "\\\\vskip -0.1in\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "    return table\n",
    "\n",
    "# Example usage\n",
    "# Assuming `results_df` is your pandas DataFrame\n",
    "latex_table = create_latex_table(combined_results)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment on single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import tabmini\n",
    "import aeon\n",
    "from aeon.visualisation import plot_critical_difference, plot_significance\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from models.base import LogisticRegression\n",
    "from models.gridsearch_wrapper import SKLearnWrapper\n",
    "from models.random_feature_representation_boosting import GradientRFRBoostClassifier\n",
    "from models.end2end import End2EndMLPResNet\n",
    "\n",
    "class Config:\n",
    "    save_dir = Path.cwd() / \"results\" / \"PMLBmini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment on a single dataset\n",
    "\n",
    "#download dataset, cache it\n",
    "dataset_save_path = Config.save_dir / 'PMLBmini_dataset.pkl'\n",
    "if not os.path.exists(dataset_save_path):\n",
    "    print(\"Dataset not found, downloading\")\n",
    "    dataset = tabmini.load_dataset(reduced=False)\n",
    "    os.makedirs(Config.save_dir, exist_ok=True)\n",
    "    with open(dataset_save_path, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "else:\n",
    "    print(\"Dataset found, loading\")\n",
    "    with open(dataset_save_path, 'rb') as f:\n",
    "        dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 10: parity5\n",
    "X, y = dataset[\"analcatdata_asbestos\"]\n",
    "X = torch.tensor(X.values).float()\n",
    "y = torch.tensor(y.values)[..., None].float()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(\n",
    "#     n_classes=2,\n",
    "#     l2_lambda=0.0001,\n",
    "#     max_iter = 300,\n",
    "# )\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = GradientRFRBoostClassifier(\n",
    "    in_dim=X.shape[1],\n",
    "    n_classes=2,\n",
    "    l2_cls=0.1,\n",
    "    l2_ghat=10000,\n",
    "    n_layers=1,\n",
    "    randfeat_xt_dim=128,\n",
    "    randfeat_x0_dim=128,\n",
    "    hidden_dim=128,\n",
    "    upscale_type=\"SWIM\",\n",
    "    feature_type=\"SWIM\",\n",
    "    use_batchnorm=False,\n",
    "    boost_lr=1.0,\n",
    "    activation=\"relu\"\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "logits = model(X_test)\n",
    "print(\"logits\", logits)\n",
    "probs = nn.functional.sigmoid(logits)\n",
    "print(\"out and y\", torch.cat([logits, y_test], dim=1))\n",
    "print(\"binary class pred and y\", torch.cat([probs > 0.5, y_test], dim=1))\n",
    "auc = roc_auc_score(y_test.numpy(), probs.detach().numpy())\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "#train\n",
    "logits = model(X_train)\n",
    "probs = nn.functional.sigmoid(logits)\n",
    "auc = roc_auc_score(y_train.numpy(), probs.detach().numpy())\n",
    "print(\"train AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

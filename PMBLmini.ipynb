{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMLBmini experiments\n",
    "\n",
    "This notebook runs the PMLBmini experiments, and compares RANDOM FEATURE BOOSTING and END2END to the saved PMLBmini models\n",
    "\n",
    "NOTE that we assume tabmini is installed in the cwd https://github.com/RicardoKnauer/TabMini \n",
    "\n",
    "Should take no more than 30 minutes to run this notebook, ie run all models and datasets sequentially on a single CPU core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import tabmini\n",
    "import aeon\n",
    "from aeon.visualisation import plot_critical_difference, plot_significance\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from models.gridsearch_wrapper import SKLearnWrapper\n",
    "from models.random_feature_representation_boosting import GradientRFRBoostClassifier\n",
    "from models.end2end import End2EndMLPResNet\n",
    "from PMLBmini import test_on_PMLBmini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#####      Equal/Random Guessing        ######\n",
    "##############################################\n",
    "\n",
    "\n",
    "class EqualGuessing(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Guess probabilty 0.5 for each class\"\"\"\n",
    "        # Guess [0.5, 0.5]\n",
    "        return np.ones((X.shape[0], 2)) * 0.5\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        # Get the probabilities from predict_proba\n",
    "        proba = self.predict_proba(X)\n",
    "        # Calculate the log of ratios for binary classification\n",
    "        decision = np.log((proba[:, 1] + 1e-10) / (proba[:, 0] + 1e-10))\n",
    "        return decision\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/nikita/Code/random-feature-boosting/results/PMLBmini/\"\n",
    "\n",
    "train_guessing_and_xgboost, test_guessing_and_xgboost = test_on_PMLBmini(\n",
    "    EqualGuessing(),\n",
    "    'EqualGuessing',\n",
    "    [i for i in range(44)],\n",
    "    save_dir, \n",
    "    other_saved_methods={\"XGBoost\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found, loading\n",
      "Evaluating E2E_MLP_ResNet against {}...\n",
      "Comparing E2E_MLP_ResNet on 0: analcatdata_aids\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing E2E_MLP_ResNet\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1576.06it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.000 total time=   0.7s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1857.80it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2176.00it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2202.74it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.222 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2529.43it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2679.21it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2394.78it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.417 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2292.39it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2305.15it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2094.92it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2355.07it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2810.69it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2764.44it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2414.50it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.222 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2227.81it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2135.63it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2042.65it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.417 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2006.94it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1761.13it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1866.57it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1843.76it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2075.98it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1777.02it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1861.65it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.222 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2391.42it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1343.15it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1818.02it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.417 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1647.88it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2383.17it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2023.20it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2306.72it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2440.77it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2634.01it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1816.42it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.222 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2636.10it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2419.14it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2770.29it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.417 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2914.33it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2410.43it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2681.21it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2777.38it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2609.43it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2616.21it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2769.19it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.222 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1788.34it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1808.36it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1438.10it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.417 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1611.42it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1685.90it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2113.64it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "100%|██████████████████████████████████████████| 30/30 [00:00<00:00, 192.30it/s]\n",
      "Best params: {'activation': ReLU(), 'batch_size': 32, 'bottleneck_dim': 32, 'end_lr_factor': 0.01, 'hidden_dim': 32, 'in_dim': 4, 'loss': 'bce', 'lr': 1e-05, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2006.75it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2542.05it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2426.60it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2008.38it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.222 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2006.81it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2340.96it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.833 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1923.64it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2030.35it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.250 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2335.58it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1877.43it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2445.94it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2303.09it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2406.88it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2516.78it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.222 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2337.61it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1871.54it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.833 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2013.01it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1462.84it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.250 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2063.52it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1990.21it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2086.06it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2086.93it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2296.40it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2101.74it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.222 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1924.35it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1753.30it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.833 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1988.76it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2472.52it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.250 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2386.70it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2179.09it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2175.66it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2227.42it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2428.06it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2529.28it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.222 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1945.32it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2400.77it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.833 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2702.05it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2282.12it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.250 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2218.50it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2288.80it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2601.71it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2782.23it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2663.50it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2760.74it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.222 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2850.49it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.667 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2766.33it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.833 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2960.62it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2657.09it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.250 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2819.45it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2576.88it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.333 total time=   0.0s\n",
      "100%|██████████████████████████████████████████| 30/30 [00:00<00:00, 272.08it/s]\n",
      "Best params: {'activation': ReLU(), 'batch_size': 32, 'bottleneck_dim': 32, 'end_lr_factor': 0.01, 'hidden_dim': 32, 'in_dim': 4, 'loss': 'bce', 'lr': 1e-05, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2331.12it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.083 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2937.39it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2725.17it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2274.32it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1966.30it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.444 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2417.93it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=1.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2640.58it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2543.80it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2695.22it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2554.75it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=1e-05, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.778 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2482.18it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.083 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2854.04it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1879.95it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2271.61it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1875.95it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.444 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1996.65it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=1.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1996.81it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2030.98it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2058.55it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2034.39it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.0001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.778 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1400.51it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.083 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1566.52it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1487.25it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1429.88it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1530.21it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.444 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2447.32it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=1.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1856.87it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2296.65it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2267.64it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1880.63it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.001, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.778 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1943.64it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.083 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1959.59it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2270.96it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2023.07it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1647.78it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.444 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1945.20it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=1.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1937.71it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1374.36it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1709.52it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2167.30it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.01, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.778 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1934.85it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.083 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2054.25it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1833.39it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1804.18it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2129.92it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=1, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.444 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 2016.59it/s]\n",
      "[CV 1/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=1.000 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1994.56it/s]\n",
      "[CV 2/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.500 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1588.25it/s]\n",
      "[CV 3/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1610.67it/s]\n",
      "[CV 4/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.583 total time=   0.0s\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1544.22it/s]\n",
      "[CV 5/5] END activation=ReLU(), batch_size=32, bottleneck_dim=32, end_lr_factor=0.01, hidden_dim=32, in_dim=4, loss=bce, lr=0.1, modelClass=<class 'models.end2end.End2EndMLPResNet'>, n_blocks=2, n_epochs=30, out_dim=1, seed=42, weight_decay=1e-05;, score=0.778 total time=   0.0s\n",
      "100%|██████████████████████████████████████████| 30/30 [00:00<00:00, 202.29it/s]\n",
      "Best params: {'activation': ReLU(), 'batch_size': 32, 'bottleneck_dim': 32, 'end_lr_factor': 0.01, 'hidden_dim': 32, 'in_dim': 4, 'loss': 'bce', 'lr': 1e-05, 'modelClass': <class 'models.end2end.End2EndMLPResNet'>, 'n_blocks': 2, 'n_epochs': 30, 'out_dim': 1, 'seed': 42, 'weight_decay': 1e-05}\n",
      "Extracting test and train scores\n"
     ]
    }
   ],
   "source": [
    "!python PMLBmini.py \\\n",
    "    --models E2E_MLP_ResNet \\\n",
    "    --dataset_indices 0 \\\n",
    "    --save_dir /home/nikita/Code/random-feature-boosting/results/PMLBmini/ \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found, loading\n",
      "Evaluating Logistic(ours) against {}...\n",
      "Comparing Logistic(ours) on 0: analcatdata_aids\n",
      "estimated name is not in methods: AutoGluon\n",
      "estimated name is not in methods: AutoPrognosis\n",
      "estimated name is not in methods: TabPFN\n",
      "estimated name is not in methods: HyperFast\n",
      "estimated name is not in methods: LightGBM\n",
      "estimated name is not in methods: XGBoost\n",
      "estimated name is not in methods: CatBoost\n",
      "Testing Logistic(ours)\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.6s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.444 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.444 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.444 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.444 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.444 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.556 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.833 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.444 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.778 total time=   0.0s\n",
      "Best params: {'hidden_dim': 32, 'in_dim': 4, 'l2_cls': 100, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42, 'upscale_type': 'identity'}\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.917 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.556 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.917 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.556 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.917 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.556 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=1.000 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.556 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.833 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.833 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.556 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.833 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.556 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.500 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.556 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.889 total time=   0.0s\n",
      "Best params: {'hidden_dim': 32, 'in_dim': 4, 'l2_cls': 0.1, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42, 'upscale_type': 'identity'}\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.500 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.333 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=100, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.444 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.500 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.333 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=10, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.444 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.500 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.333 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.444 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.333 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.1, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.444 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.333 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.01, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.556 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.750 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.333 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.917 total time=   0.0s\n",
      "[CV 3/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.417 total time=   0.0s\n",
      "[CV 4/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.333 total time=   0.0s\n",
      "[CV 5/5] END hidden_dim=32, in_dim=4, l2_cls=0.0001, modelClass=<class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, n_classes=2, n_layers=0, seed=42, upscale_type=identity;, score=0.667 total time=   0.0s\n",
      "Best params: {'hidden_dim': 32, 'in_dim': 4, 'l2_cls': 0.0001, 'modelClass': <class 'models.random_feature_representation_boosting.GradientRFRBoostClassifier'>, 'n_classes': 2, 'n_layers': 0, 'seed': 42, 'upscale_type': 'identity'}\n",
      "Extracting test and train scores\n"
     ]
    }
   ],
   "source": [
    "!python PMLBmini.py \\\n",
    "    --models \"Logistic(ours)\" \\\n",
    "    --dataset_indices 0 \\\n",
    "    --save_dir /home/nikita/Code/random-feature-boosting/results/PMLBmini/ \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AutoPrognosis</th>\n",
       "      <th>AutoGluon</th>\n",
       "      <th>TabPFN</th>\n",
       "      <th>Logistic regression</th>\n",
       "      <th>HyperFast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>parity5</th>\n",
       "      <td>0.27</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_fraud</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_aids</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_bankruptcy</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_japansolvent</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labor</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_asbestos</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lupus</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postoperative_patient_data</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_cyyoung9302</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_cyyoung8092</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_creditscore</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appendicitis</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>molecular_biology_promoters</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_boxing1</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mux6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_boxing2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hepatitis</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corral</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glass2</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backache</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prnn_crabs</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sonar</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biomed</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prnn_synth</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analcatdata_lawsuit</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spect</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_statlog</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breast_cancer</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_h</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hungarian</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleve</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_c</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haberman</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bupa</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectf</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ionosphere</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colic</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horse_colic</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house_votes_84</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vote</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saheart</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irish</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             AutoPrognosis  AutoGluon  TabPFN  \\\n",
       "parity5                               0.27       1.00    0.02   \n",
       "analcatdata_fraud                     0.86       0.80    0.79   \n",
       "analcatdata_aids                      0.73       0.77    0.63   \n",
       "analcatdata_bankruptcy                0.98       0.98    0.96   \n",
       "analcatdata_japansolvent              0.85       0.88    0.91   \n",
       "labor                                 0.88       0.94    0.99   \n",
       "analcatdata_asbestos                  0.87       0.84    0.85   \n",
       "lupus                                 0.84       0.79    0.82   \n",
       "postoperative_patient_data            0.49       0.55    0.44   \n",
       "analcatdata_cyyoung9302               0.89       0.85    0.87   \n",
       "analcatdata_cyyoung8092               0.73       0.90    0.85   \n",
       "analcatdata_creditscore               1.00       0.99    1.00   \n",
       "appendicitis                          0.78       0.84    0.82   \n",
       "molecular_biology_promoters           0.88       0.93    0.88   \n",
       "analcatdata_boxing1                   0.89       0.89    0.76   \n",
       "mux6                                  1.00       1.00    1.00   \n",
       "analcatdata_boxing2                   0.82       0.78    0.71   \n",
       "hepatitis                             0.85       0.82    0.85   \n",
       "corral                                1.00       1.00    1.00   \n",
       "glass2                                0.89       0.91    0.89   \n",
       "backache                              0.60       0.72    0.75   \n",
       "prnn_crabs                            1.00       1.00    1.00   \n",
       "sonar                                 0.88       0.93    0.92   \n",
       "biomed                                1.00       0.96    0.95   \n",
       "prnn_synth                            0.94       0.95    0.95   \n",
       "analcatdata_lawsuit                   0.99       0.99    1.00   \n",
       "spect                                 0.84       0.82    0.83   \n",
       "heart_statlog                         0.91       0.89    0.90   \n",
       "breast_cancer                         0.69       0.69    0.73   \n",
       "heart_h                               0.87       0.89    0.88   \n",
       "hungarian                             0.86       0.86    0.86   \n",
       "cleve                                 0.90       0.89    0.89   \n",
       "heart_c                               0.91       0.91    0.91   \n",
       "haberman                              0.70       0.71    0.72   \n",
       "bupa                                  0.66       0.64    0.68   \n",
       "spectf                                0.91       0.94    0.93   \n",
       "ionosphere                            0.97       0.98    0.98   \n",
       "colic                                 0.87       0.86    0.87   \n",
       "horse_colic                           0.88       0.85    0.84   \n",
       "house_votes_84                        0.99       0.99    0.99   \n",
       "vote                                  1.00       0.99    1.00   \n",
       "saheart                               0.77       0.76    0.77   \n",
       "clean1                                0.93       1.00    0.99   \n",
       "irish                                 1.00       1.00    1.00   \n",
       "\n",
       "                             Logistic regression  HyperFast  \n",
       "parity5                                     0.17       0.02  \n",
       "analcatdata_fraud                           0.77       0.73  \n",
       "analcatdata_aids                            0.61       0.53  \n",
       "analcatdata_bankruptcy                      0.97       0.88  \n",
       "analcatdata_japansolvent                    0.85       0.91  \n",
       "labor                                       0.97       0.98  \n",
       "analcatdata_asbestos                        0.86       0.87  \n",
       "lupus                                       0.85       0.79  \n",
       "postoperative_patient_data                  0.38       0.34  \n",
       "analcatdata_cyyoung9302                     0.87       0.84  \n",
       "analcatdata_cyyoung8092                     0.79       0.84  \n",
       "analcatdata_creditscore                     0.94       0.87  \n",
       "appendicitis                                0.84       0.87  \n",
       "molecular_biology_promoters                 0.88       0.89  \n",
       "analcatdata_boxing1                         0.67       0.67  \n",
       "mux6                                        0.70       0.95  \n",
       "analcatdata_boxing2                         0.68       0.70  \n",
       "hepatitis                                   0.84       0.83  \n",
       "corral                                      0.96       1.00  \n",
       "glass2                                      0.72       0.79  \n",
       "backache                                    0.72       0.78  \n",
       "prnn_crabs                                  1.00       0.81  \n",
       "sonar                                       0.85       0.89  \n",
       "biomed                                      0.94       0.93  \n",
       "prnn_synth                                  0.94       0.94  \n",
       "analcatdata_lawsuit                         1.00       0.98  \n",
       "spect                                       0.82       0.83  \n",
       "heart_statlog                               0.89       0.89  \n",
       "breast_cancer                               0.70       0.69  \n",
       "heart_h                                     0.86       0.85  \n",
       "hungarian                                   0.85       0.84  \n",
       "cleve                                       0.88       0.88  \n",
       "heart_c                                     0.91       0.89  \n",
       "haberman                                    0.66       0.58  \n",
       "bupa                                        0.67       0.66  \n",
       "spectf                                      0.88       0.87  \n",
       "ionosphere                                  0.90       0.97  \n",
       "colic                                       0.86       0.86  \n",
       "horse_colic                                 0.82       0.83  \n",
       "house_votes_84                              0.99       0.98  \n",
       "vote                                        0.99       0.99  \n",
       "saheart                                     0.77       0.76  \n",
       "clean1                                      1.00       0.96  \n",
       "irish                                       0.83       0.97  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV and set the index\n",
    "saved_results = pd.read_csv('https://raw.githubusercontent.com/RicardoKnauer/TabMini/master/plotting/results/test_scores_wide_3600.csv', delimiter=\";\", index_col=0)\n",
    "saved_results.index.name = None\n",
    "saved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E2E_MLP_ResNet</th>\n",
       "      <th>Logistic(ours)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>analcatdata_aids</th>\n",
       "      <td>0.531829</td>\n",
       "      <td>0.585069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  E2E_MLP_ResNet  Logistic(ours)\n",
       "analcatdata_aids        0.531829        0.585069"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = pd.read_csv(\"results/PMLBmini/E2E_MLP_ResNet/test_0.csv\", index_col=0)\n",
    "t2 = pd.read_csv(\"results/PMLBmini/Logistic(ours)/test_0.csv\", index_col=0)\n",
    "\n",
    "t1.join(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = saved_results.copy()\n",
    "# combined_results = pd.read_csv(Config.save_dir / \"combined_results.csv\", index_col=0)\n",
    "for df in [test_guessing_and_xgboost, \n",
    "           test_RFNN,\n",
    "           #test_RFNN_iid,\n",
    "           test_E2E, \n",
    "           test_GRFRBoost,\n",
    "           #test_GRFRBoostID,\n",
    "           #test_GRFRBoost_iid,\n",
    "           #test_GRFRBoostID_iid,\n",
    "           #test_logistic,\n",
    "           ]:\n",
    "    combined_results = combined_results.join(df, how='inner')\n",
    "combined_results = combined_results.round(2) # since PMLBmini's resulst are rounded to 2 decimals, for fair comparison\n",
    "combined_results\n",
    "\n",
    "#save\n",
    "combined_results.to_csv(Config.save_dir / \"combined_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = pd.read_csv(Config.save_dir / \"combined_results.csv\", index_col=0)\n",
    "# combined_results.drop(columns=['GRFRBoostID', 'GRFRBoost iid', 'GRFRBoostID iid', 'RFNN'], inplace=True)\n",
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_results[[\"GRFRBoost (ours)\", \"Logistic (mine)\", \"RFNN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the plot\n",
    "plot = plot_critical_difference(combined_results.values,\n",
    "                                combined_results.columns.tolist(), \n",
    "                                alpha=0.05, \n",
    "                                lower_better=False)\n",
    "\n",
    "# Retrieve the figure and axes from the plot\n",
    "fig = plot[0].figure\n",
    "ax = plot[0]\n",
    "\n",
    "# Adjust figure size\n",
    "fig.set_size_inches(6, 3)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figures\n",
    "fig.savefig(Config.save_dir / \"PMLBmini_critical_difference.eps\", bbox_inches='tight')\n",
    "fig.savefig(Config.save_dir / \"PMLBmini_critical_difference.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_latex_table(df):\n",
    "    table = \"\"\"\n",
    "\\\\begin{table}[t]\n",
    "\\\\caption{Test accuracies on the concentric circles task.}\n",
    "\\\\label{tab:concentric-circles}\n",
    "\\\\vskip 0.15in\n",
    "\\\\begin{center}\n",
    "\\\\begin{small}\n",
    "\\\\begin{sc}\n",
    "\\\\begin{tabular}{lcc}\n",
    "\\\\toprule\n",
    "Model & Mean Acc & Std Dev \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "    for model_name in df.columns:\n",
    "        accs = df[model_name]\n",
    "        mean_acc = np.mean(accs)\n",
    "        std_acc = np.std(accs)\n",
    "        table += f\"{model_name} & {mean_acc:.4f} & {std_acc:.4f} \\\\\\\\\\n\"\n",
    "    \n",
    "    table += \"\"\"\n",
    "\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{sc}\n",
    "\\\\end{small}\n",
    "\\\\end{center}\n",
    "\\\\vskip -0.1in\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "    return table\n",
    "\n",
    "# Example usage\n",
    "# Assuming `results_df` is your pandas DataFrame\n",
    "latex_table = create_latex_table(combined_results)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment on single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import tabmini\n",
    "import aeon\n",
    "from aeon.visualisation import plot_critical_difference, plot_significance\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from models.base import LogisticRegression\n",
    "from models.gridsearch_wrapper import SKLearnWrapper\n",
    "from models.random_feature_representation_boosting import GradientRFRBoostClassifier\n",
    "from models.end2end import End2EndMLPResNet\n",
    "\n",
    "class Config:\n",
    "    save_dir = Path.cwd() / \"results\" / \"PMLBmini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment on a single dataset\n",
    "\n",
    "#download dataset, cache it\n",
    "dataset_save_path = Config.save_dir / 'PMLBmini_dataset.pkl'\n",
    "if not os.path.exists(dataset_save_path):\n",
    "    print(\"Dataset not found, downloading\")\n",
    "    dataset = tabmini.load_dataset(reduced=False)\n",
    "    os.makedirs(Config.save_dir, exist_ok=True)\n",
    "    with open(dataset_save_path, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "else:\n",
    "    print(\"Dataset found, loading\")\n",
    "    with open(dataset_save_path, 'rb') as f:\n",
    "        dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 10: parity5\n",
    "X, y = dataset[\"analcatdata_asbestos\"]\n",
    "# X, y = dataset[\"parity5\"]\n",
    "minmax = MinMaxScaler()\n",
    "X = minmax.fit_transform(X)\n",
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y.values)[..., None].float()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(\n",
    "#     n_classes=2,\n",
    "#     l2_lambda=0.0001,\n",
    "#     max_iter = 300,\n",
    "# )\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = GradientRFRBoostClassifier(\n",
    "    in_dim=X.shape[1],\n",
    "    n_classes=2,\n",
    "    l2_cls=0.01,\n",
    "    l2_ghat=0.001,\n",
    "    n_layers=1,\n",
    "    randfeat_xt_dim=128,\n",
    "    randfeat_x0_dim=128,\n",
    "    hidden_dim=128,\n",
    "    upscale_type=\"identity\",\n",
    "    feature_type=\"SWIM\",\n",
    "    use_batchnorm=False,\n",
    "    boost_lr=1e-0,\n",
    "    activation=\"tanh\"\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "logits = model(X_test)\n",
    "print(\"logits\", logits)\n",
    "probs = nn.functional.sigmoid(logits)\n",
    "print(\"out and y\", torch.cat([logits, y_test], dim=1))\n",
    "print(\"binary class pred and y\", torch.cat([probs > 0.5, y_test], dim=1))\n",
    "auc = roc_auc_score(y_test.numpy(), probs.detach().numpy())\n",
    "print(\"test AUC:\", auc)\n",
    "\n",
    "#train\n",
    "logits = model(X_train)\n",
    "probs = nn.functional.sigmoid(logits)\n",
    "auc = roc_auc_score(y_train.numpy(), probs.detach().numpy())\n",
    "print(\"train AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import openml\n",
    "\n",
    "#from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options\n",
    "device = \"cuda\" # torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([60000, 784])\n",
      "Train labels shape: torch.Size([60000, 10])\n",
      "Test data shape: torch.Size([10000, 784])\n",
      "Test labels shape: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def normalize_mean_std_traindata(X_train: Tensor, X_test: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    mean = X_train.mean(dim=0)\n",
    "    std = X_train.std(dim=0)\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "\n",
    "    X_train = torch.clip(X_train, -5, 5)\n",
    "    X_test = torch.clip(X_test, -5, 5)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_path = \"/home/nikita/hdd/MNIST\"\n",
    "trainset = datasets.MNIST(mnist_path, download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST(mnist_path, download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "# Flatten the data\n",
    "X_train, y_train_cat = next(iter(trainloader))\n",
    "X_train = X_train.view(len(trainset), -1).to(device)\n",
    "X_test, y_test_cat = next(iter(testloader))\n",
    "X_test = X_test.view(len(testset), -1).to(device)\n",
    "\n",
    "# Convert train and test labels to one-hot encoding\n",
    "y_train = nn.functional.one_hot(y_train_cat, num_classes=10).float().to(device)\n",
    "y_test = nn.functional.one_hot(y_test_cat, num_classes=10).float().to(device)\n",
    "y_train_cat = y_train_cat.to(device)\n",
    "y_test_cat = y_test_cat.to(device)\n",
    "\n",
    "# Normalize by mean and std\n",
    "X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_pred tensor([[ -0.2628, -10.2585,   0.3933,  ...,  11.1302,   0.0346,   3.3285],\n",
      "        [  5.8857,   1.3818,  13.1877,  ..., -18.7712,   4.5571, -11.9242],\n",
      "        [ -5.7085,   6.3627,   1.9223,  ...,   0.8333,   0.3595,  -1.5743],\n",
      "        ...,\n",
      "        [ -7.5470,  -7.3161,  -2.6649,  ...,   2.3265,   4.0623,   4.8242],\n",
      "        [ -2.7713,  -1.8789,  -3.1412,  ...,  -4.0734,   6.4926,  -3.2449],\n",
      "        [  2.6914, -10.5134,   4.8309,  ...,  -7.0523,  -0.4899,  -4.2300]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train accuracy: 0.9334999918937683\n",
      "Test accuracy: 0.9266999959945679\n"
     ]
    }
   ],
   "source": [
    "from models.base import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(\n",
    "        n_classes = 10,\n",
    "        l2_lambda = 0.001,\n",
    "        max_iter = 300,\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train_cat)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientRFBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linesearch loss tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(1.0485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "X_test_pred tensor([[ 1.4929e-01, -8.3261e+00, -2.1640e+00,  ...,  1.4248e+01,\n",
      "          9.2703e-02,  3.2637e+00],\n",
      "        [ 1.2329e+00,  6.9448e+00,  1.5439e+01,  ..., -1.5902e+01,\n",
      "          2.9198e+00, -1.0967e+01],\n",
      "        [-4.3724e+00,  8.7558e+00,  7.7390e-01,  ...,  6.1706e-01,\n",
      "          4.6697e-03, -1.3317e+00],\n",
      "        ...,\n",
      "        [-6.2275e+00, -8.8456e+00, -6.6136e+00,  ...,  1.2347e+00,\n",
      "          3.4575e+00,  7.6944e+00],\n",
      "        [-2.1819e+00, -1.3737e+00, -6.4966e+00,  ..., -1.9346e+00,\n",
      "          7.0670e+00, -6.4096e+00],\n",
      "        [ 7.7807e+00, -1.1599e+01,  1.8111e+00,  ..., -7.8876e+00,\n",
      "         -9.8179e-01, -4.2553e+00]], device='cuda:0')\n",
      "Train accuracy: 0.9836166501045227\n",
      "Test accuracy: 0.9668999910354614\n"
     ]
    }
   ],
   "source": [
    "from models.random_feature_representation_boosting import GradientRFRBoostClassifier\n",
    "\n",
    "model = GradientRFRBoostClassifier(\n",
    "    in_dim = 784,\n",
    "    hidden_dim = 128,\n",
    "    n_classes = 10,\n",
    "    randfeat_xt_dim = 256,\n",
    "    randfeat_x0_dim = 256,\n",
    "    n_layers = 5,\n",
    "    l2_cls = 0.000001,\n",
    "    l2_ghat = 0.000001,\n",
    "    feature_type=\"SWIM\",\n",
    "    upscale_type = \"SWIM\",\n",
    "    lbfgs_max_iter = 300,\n",
    "    boost_lr = 1.0,\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "#TODO NEXT: add xtx0 to the classification case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ce at layer 0: 0.2827766239643097\n",
      "Test ce at layer 0: 0.28456902503967285\n",
      "Train acc at layer 0: 0.9189833402633667\n",
      "Test acc at layer 0: 0.9185000061988831\n",
      "\n",
      "Train ce at layer 1: 0.14964938163757324\n",
      "Test ce at layer 1: 0.16896314918994904\n",
      "Train acc at layer 1: 0.9573500156402588\n",
      "Test acc at layer 1: 0.9526000022888184\n",
      "\n",
      "Train ce at layer 2: 0.10423363745212555\n",
      "Test ce at layer 2: 0.13513188064098358\n",
      "Train acc at layer 2: 0.9700333476066589\n",
      "Test acc at layer 2: 0.9598999619483948\n",
      "\n",
      "Train ce at layer 3: 0.08233209699392319\n",
      "Test ce at layer 3: 0.11896318942308426\n",
      "Train acc at layer 3: 0.9764500260353088\n",
      "Test acc at layer 3: 0.9642999768257141\n",
      "\n",
      "Train ce at layer 4: 0.0681803748011589\n",
      "Test ce at layer 4: 0.11446826905012131\n",
      "Train acc at layer 4: 0.9803500175476074\n",
      "Test acc at layer 4: 0.965999960899353\n",
      "\n",
      "Train ce at layer 5: 0.058496713638305664\n",
      "Test ce at layer 5: 0.10933632403612137\n",
      "Train acc at layer 5: 0.9836166501045227\n",
      "Test acc at layer 5: 0.9668999910354614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def see_results_for_every_layer(X_train, y_train, X_test, y_test, model, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "\n",
    "        X_train = model.upscale(X0_train)\n",
    "        X_test = model.upscale(X0_test)\n",
    "\n",
    "        pred_train = model.top_level_modules[0](X_train)\n",
    "        pred_test = model.top_level_modules[0](X_test)\n",
    "\n",
    "        ce = loss_fn(pred_train, y_train)\n",
    "        ce_test = loss_fn(pred_test, y_test)\n",
    "        acc = (pred_train.argmax(1) == y_train.argmax(1)).float().mean()\n",
    "        acc_test = (pred_test.argmax(1) == y_test.argmax(1)).float().mean()\n",
    "        print(f\"Train ce at layer 0: {ce}\")\n",
    "        print(f\"Test ce at layer 0: {ce_test}\")\n",
    "        print(f\"Train acc at layer 0: {acc}\")\n",
    "        print(f\"Test acc at layer 0: {acc_test}\")\n",
    "        print()\n",
    "        \n",
    "        for t, (feat_layer, ghat_layer, classifier) in enumerate(zip(model.random_feature_layers, \n",
    "                                                                     model.ghat_boosting_layers, \n",
    "                                                                     model.top_level_modules[1:])):\n",
    "            features_train = feat_layer(X_train, X0_train)\n",
    "            features_test = feat_layer(X_test, X0_test)\n",
    "            X_train += model.boost_lr * ghat_layer(features_train)\n",
    "            X_test  += model.boost_lr * ghat_layer(features_test)\n",
    "            \n",
    "            pred_train = classifier(X_train)\n",
    "            pred_test = classifier(X_test)\n",
    "\n",
    "            ce = loss_fn(pred_train, y_train)\n",
    "            ce_test = loss_fn(pred_test, y_test)\n",
    "            acc = (pred_train.argmax(1) == y_train.argmax(1)).float().mean()\n",
    "            acc_test = (pred_test.argmax(1) == y_test.argmax(1)).float().mean()\n",
    "            print(f\"Train ce at layer {t+1}: {ce}\")\n",
    "            print(f\"Test ce at layer {t+1}: {ce_test}\")\n",
    "            print(f\"Train acc at layer {t+1}: {acc}\")\n",
    "            print(f\"Test acc at layer {t+1}: {acc_test}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "see_results_for_every_layer(X_train, y_train, X_test, y_test, model, nn.functional.cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End2End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models import End2EndMLPResNet\n",
    "\n",
    "model = End2EndMLPResNet(\n",
    "    in_dim = X_train.shape[1],\n",
    "    hidden_dim = 128,\n",
    "    bottleneck_dim = 32,\n",
    "    out_dim = 10,\n",
    "    n_blocks = 4,\n",
    "    lr = 0.01,\n",
    "    end_lr_factor = 0.01,\n",
    "    n_epochs = 20,\n",
    "    weight_decay = 0.001,\n",
    "    batch_size = 512\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

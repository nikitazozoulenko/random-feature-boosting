{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import openml\n",
    "\n",
    "#from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options\n",
    "device = \"cuda\" # torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([60000, 784])\n",
      "Train labels shape: torch.Size([60000, 10])\n",
      "Test data shape: torch.Size([10000, 784])\n",
      "Test labels shape: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def normalize_mean_std_traindata(X_train: Tensor, X_test: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    mean = X_train.mean(dim=0)\n",
    "    std = X_train.std(dim=0)\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "\n",
    "    X_train = torch.clip(X_train, -5, 5)\n",
    "    X_test = torch.clip(X_test, -5, 5)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_path = \"/home/nikita/hdd/MNIST\"\n",
    "trainset = datasets.MNIST(mnist_path, download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST(mnist_path, download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "# Flatten the data\n",
    "X_train, y_train_cat = next(iter(trainloader))\n",
    "X_train = X_train.view(len(trainset), -1).to(device)\n",
    "X_test, y_test_cat = next(iter(testloader))\n",
    "X_test = X_test.view(len(testset), -1).to(device)\n",
    "\n",
    "# Convert train and test labels to one-hot encoding\n",
    "y_train = nn.functional.one_hot(y_train_cat, num_classes=10).float().to(device)\n",
    "y_test = nn.functional.one_hot(y_test_cat, num_classes=10).float().to(device)\n",
    "y_train_cat = y_train_cat.to(device)\n",
    "y_test_cat = y_test_cat.to(device)\n",
    "\n",
    "# Normalize by mean and std\n",
    "X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_pred tensor([[ -0.2510, -10.2403,   0.3849,  ...,  11.1271,   0.0377,   3.3296],\n",
      "        [  5.8885,   1.3842,  13.1973,  ..., -18.7710,   4.5807, -11.9262],\n",
      "        [ -5.7043,   6.3781,   1.9346,  ...,   0.8382,   0.3691,  -1.5643],\n",
      "        ...,\n",
      "        [ -7.5464,  -7.3031,  -2.6623,  ...,   2.3346,   4.0713,   4.8315],\n",
      "        [ -2.7700,  -1.8704,  -3.1229,  ...,  -4.0766,   6.4907,  -3.2410],\n",
      "        [  2.6960, -10.5125,   4.8481,  ...,  -7.0335,  -0.4837,  -4.2359]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train accuracy: 0.9335166811943054\n",
      "Test accuracy: 0.9264000058174133\n"
     ]
    }
   ],
   "source": [
    "from models.models import LogisticRegression, FittableModule\n",
    "\n",
    "\n",
    "class LogisticRegression(FittableModule):\n",
    "    def __init__(self, \n",
    "                 in_dim: int,\n",
    "                 out_dim: int = 10,\n",
    "                 l2_reg: float = 0.001,\n",
    "                 lr: float = 1.0,\n",
    "                 max_iter: int = 100,\n",
    "                 ):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.l2_reg = l2_reg\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        if out_dim > 1:\n",
    "            self.loss = nn.functional.cross_entropy #this is with logits\n",
    "        else:\n",
    "            self.loss = nn.functional.binary_cross_entropy_with_logits\n",
    "\n",
    "\n",
    "    def fit(self, \n",
    "            X: Tensor, \n",
    "            y: Tensor,\n",
    "            init_W_b: Optional[Tuple[Tensor, Tensor]] = None,\n",
    "            ):\n",
    "        \n",
    "        # No onehot encoding\n",
    "        if y.dim() > 1:\n",
    "            y_labels = torch.argmax(y, dim=1)\n",
    "        else:\n",
    "            y_labels = y\n",
    "\n",
    "        # Put model on device\n",
    "        device = X.device\n",
    "        self.to(device)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        if init_W_b is not None:\n",
    "            W, b = init_W_b\n",
    "            self.linear.weight.data = W\n",
    "            self.linear.bias.data = b\n",
    "        else:\n",
    "            nn.init.kaiming_normal_(self.linear.weight)\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "            \n",
    "        with torch.enable_grad():\n",
    "            # Optimize\n",
    "            optimizer = torch.optim.LBFGS(self.linear.parameters(), lr=self.lr, max_iter=self.max_iter)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.linear(X)\n",
    "                loss = self.loss(logits, y_labels)\n",
    "                loss += self.l2_reg * torch.linalg.norm(self.linear.weight)**2\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        return self.linear(X)\n",
    "\n",
    "\n",
    "model = LogisticRegression(\n",
    "        in_dim = 784,\n",
    "        out_dim = 10,\n",
    "        l2_reg = 0.001,\n",
    "        max_iter = 300,\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train_cat)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientRFBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_pred tensor([[ -0.4578,  -7.3349,  -3.2638,  ...,  19.2302,  -3.6720,   8.1709],\n",
      "        [  1.8385,   4.3136,  15.2803,  ..., -14.3408,   8.0894, -12.2165],\n",
      "        [ -6.2760,  12.3083,  -0.7033,  ...,   0.5448,  -0.3209,  -2.4590],\n",
      "        ...,\n",
      "        [ -6.0342,  -7.2200, -11.2291,  ...,   3.2755,   6.3079,   8.0130],\n",
      "        [ -4.2226,  -1.5429,  -9.6328,  ...,  -0.4453,  10.5957,  -5.8550],\n",
      "        [ 11.1178, -13.2252,   8.9488,  ..., -10.6908,  -1.5249,  -5.1460]],\n",
      "       device='cuda:0')\n",
      "Train accuracy: 0.9985499978065491\n",
      "Test accuracy: 0.9706999659538269\n"
     ]
    }
   ],
   "source": [
    "from models.models import FittableModule, create_layer, fit_ridge_ALOOCV, Identity\n",
    "\n",
    "def line_search_cross_entropy(cls, X, y, G_hat):\n",
    "    \"\"\"Solves the line search risk minimizatin problem\n",
    "    R(W, X + a * g) for mutliclass cross entropy loss\"\"\"\n",
    "    # No onehot encoding\n",
    "    if y.dim() > 1:\n",
    "        y_labels = torch.argmax(y, dim=1)\n",
    "    else:\n",
    "        y_labels = y\n",
    "\n",
    "    # Optimize\n",
    "    with torch.enable_grad():\n",
    "        alpha = torch.tensor([0.0], requires_grad=True, device=X.device, dtype=X.dtype)\n",
    "        optimizer = torch.optim.LBFGS([alpha])\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            logits = cls(X + alpha * G_hat)\n",
    "            loss = nn.functional.cross_entropy(logits, y_labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return alpha.detach().item()\n",
    "\n",
    "\n",
    "class GradientRFBoostClassifier(FittableModule):\n",
    "    def __init__(self, \n",
    "                 hidden_dim: int = 128, # TODO\n",
    "                 randfeat_xt_dim: int = 128,\n",
    "                 randfeat_x0_dim: int = 128,\n",
    "                 out_dim: int = 10,\n",
    "                 n_layers: int = 5,\n",
    "                 activation: nn.Module = nn.Tanh(),\n",
    "                 l2_reg: float = 1,\n",
    "                 feature_type = \"SWIM\", # \"dense\", identity\n",
    "                 boost_lr: float = 1.0,\n",
    "                 upscale: Optional[str] = \"dense\",\n",
    "                 max_iter: int = 100,\n",
    "                 ridge_l2: float = 0.001,\n",
    "                 ):\n",
    "        super(GradientRFBoostClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.randfeat_xt_dim = randfeat_xt_dim\n",
    "        self.randfeat_x0_dim = randfeat_x0_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.l2_reg = l2_reg\n",
    "        self.feature_type = feature_type\n",
    "        self.boost_lr = boost_lr\n",
    "        self.upscale = upscale\n",
    "        self.max_iter = max_iter\n",
    "        self.ridge_l2 = ridge_l2\n",
    "    \n",
    "\n",
    "    def fit_transform(self, X: Tensor, y: Tensor):\n",
    "        with torch.no_grad():\n",
    "            X0 = X\n",
    "\n",
    "            #optional upscale/downscale\n",
    "            if self.upscale == \"dense\":\n",
    "                self.upscale_fun = create_layer(self.upscale, X.shape[1], self.hidden_dim, None)\n",
    "                X = self.upscale_fun.fit_transform(X, y)\n",
    "            elif self.upscale == \"SWIM\":\n",
    "                self.upscale_fun = create_layer(self.upscale, X.shape[1], self.hidden_dim, self.activation)\n",
    "                X = self.upscale_fun.fit_transform(X, y)\n",
    "            elif self.upscale == \"identity\":\n",
    "                self.upscale_fun = Identity()\n",
    "                self.hidden_dim = X.size(1)\n",
    "            else:\n",
    "                raise ValueError(f\"Parameter not recoginized. Given: {self.upscale}\")\n",
    "\n",
    "\n",
    "            # Create classifier W_0\n",
    "            cls = LogisticRegression(\n",
    "                in_dim = self.hidden_dim,\n",
    "                out_dim = self.out_dim,\n",
    "                l2_reg = self.l2_reg,\n",
    "                max_iter = self.max_iter,\n",
    "            ).to(X.device)\n",
    "            cls.fit(X, y)\n",
    "            # save for now. for more memory efficient implementation, we can remove a lot of this\n",
    "            self.classifiers = [cls]\n",
    "            self.layers_fxt = []\n",
    "            self.layers_fx0 = []\n",
    "            self.deltas = []\n",
    "\n",
    "            # Layerwise boosting\n",
    "            N = X.size(0)\n",
    "            prev_cls = None if self.upscale != \"identity\" else cls\n",
    "            for t in range(self.n_layers):\n",
    "                # Step 2: Obtain activation gradient\n",
    "                # X shape (N, D) --- ResNet neurons\n",
    "                # F shape (N, p) --- random features\n",
    "                # y shape (N, d) --- one-hot target\n",
    "                # r shape (N, D) --- residual at currect boosting iteration\n",
    "                # W shape (D, d) --- top level classifier\n",
    "                # probs shape (N, d) --- predicted probabilities\n",
    "\n",
    "                # Step 1: Create random feature layer   \n",
    "                fxt_fun = create_layer(self.feature_type, self.hidden_dim, self.randfeat_xt_dim, self.activation)\n",
    "                fx0_fun = create_layer(self.feature_type, X0.size(1), self.randfeat_x0_dim, self.activation)\n",
    "                Fxt = fxt_fun.fit_transform(X, y)\n",
    "                Fx0 = fx0_fun.fit_transform(X0, y)\n",
    "                F = torch.cat([Fxt, Fx0], dim=1)\n",
    "\n",
    "\n",
    "                # Step 2: Obtain activation gradient and learn Delta\n",
    "                probs = nn.functional.softmax(cls(X), dim=1)\n",
    "                G = (y - probs) @ cls.linear.weight #negative gradient TODO divide by N?\n",
    "                G = G / torch.norm(G) * N**0.5 #normalize to unit L2(mu) norm?\n",
    "                # fit Least Squares to negative gradient (finding functional direction)\n",
    "                Delta, Delta_b, _ = fit_ridge_ALOOCV(F, G, alphas=[self.ridge_l2])\n",
    "                # Line search for risk minimization of R(W_t, Phi_t + linesearch * G_hat)\n",
    "                G_hat = F @ Delta + Delta_b\n",
    "                linesearch = line_search_cross_entropy(cls, X, y, G_hat)\n",
    "\n",
    "\n",
    "                # Step 3: Learn top level classifier\n",
    "                X = X + self.boost_lr * linesearch * G_hat\n",
    "                cls = LogisticRegression(\n",
    "                    in_dim = self.hidden_dim,\n",
    "                    out_dim = self.out_dim,\n",
    "                    l2_reg = self.l2_reg,\n",
    "                    max_iter = self.max_iter,\n",
    "                ).to(X.device)\n",
    "                cls.fit(\n",
    "                    X, \n",
    "                    y, \n",
    "                    init_W_b = (\n",
    "                        (prev_cls.linear.weight.detach().clone(), \n",
    "                        prev_cls.linear.bias.detach().clone())\n",
    "                        if prev_cls is not None else None\n",
    "                        ) \n",
    "                )\n",
    "                prev_cls = cls\n",
    "\n",
    "                #update Delta scale\n",
    "                Delta = Delta * linesearch\n",
    "                Delta_b = Delta_b * linesearch\n",
    "\n",
    "                # store\n",
    "                self.layers_fxt.append(fxt_fun)\n",
    "                self.layers_fx0.append(fx0_fun)\n",
    "                self.deltas.append((Delta, Delta_b))\n",
    "                self.classifiers.append(cls)\n",
    "\n",
    "        return cls(X)\n",
    "\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            X0 = X\n",
    "            if self.upscale is not None:\n",
    "                X = self.upscale_fun(X)\n",
    "            for fxt_fun, fx0_fun, (Delta, Delta_b) in zip(self.layers_fxt, self.layers_fx0, self.deltas):\n",
    "                features = torch.cat([fxt_fun(X), fx0_fun(X0)], dim=1)\n",
    "                X = X + self.boost_lr * (features @ Delta + Delta_b)\n",
    "            return self.classifiers[-1](X)\n",
    "        \n",
    "\n",
    "model = GradientRFBoostClassifier(\n",
    "        hidden_dim = 128,\n",
    "        randfeat_xt_dim = 256,\n",
    "        randfeat_x0_dim = 256,\n",
    "        out_dim = 10,\n",
    "        n_layers = 20,\n",
    "        l2_reg = 0.000001,\n",
    "        ridge_l2 = 0.000000001,\n",
    "        feature_type=\"SWIM\",\n",
    "        upscale = \"SWIM\",\n",
    "        max_iter = 300,\n",
    "        boost_lr = 1.0,\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "#TODO NEXT: add xtx0 to the classification case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc at layer 0: 0.9192166924476624\n",
      "Test acc at layer 0: 0.9194999933242798\n",
      "\n",
      "Train acc at layer 1: 0.9477666616439819\n",
      "Test acc at layer 1: 0.9449999928474426\n",
      "\n",
      "Train acc at layer 2: 0.9585166573524475\n",
      "Test acc at layer 2: 0.9540999531745911\n",
      "\n",
      "Train acc at layer 3: 0.9649333357810974\n",
      "Test acc at layer 3: 0.9585999846458435\n",
      "\n",
      "Train acc at layer 4: 0.9692833423614502\n",
      "Test acc at layer 4: 0.960099995136261\n",
      "\n",
      "Train acc at layer 5: 0.9713667035102844\n",
      "Test acc at layer 5: 0.9629999995231628\n",
      "\n",
      "Train acc at layer 6: 0.9738500118255615\n",
      "Test acc at layer 6: 0.964199960231781\n",
      "\n",
      "Train acc at layer 7: 0.9765666723251343\n",
      "Test acc at layer 7: 0.965399980545044\n",
      "\n",
      "Train acc at layer 8: 0.9784500002861023\n",
      "Test acc at layer 8: 0.9656999707221985\n",
      "\n",
      "Train acc at layer 9: 0.9797999858856201\n",
      "Test acc at layer 9: 0.9646999835968018\n",
      "\n",
      "Train acc at layer 10: 0.9814500212669373\n",
      "Test acc at layer 10: 0.9651999473571777\n",
      "\n",
      "Train acc at layer 11: 0.982699990272522\n",
      "Test acc at layer 11: 0.9660999774932861\n",
      "\n",
      "Train acc at layer 12: 0.9840500354766846\n",
      "Test acc at layer 12: 0.965999960899353\n",
      "\n",
      "Train acc at layer 13: 0.9851500391960144\n",
      "Test acc at layer 13: 0.967799961566925\n",
      "\n",
      "Train acc at layer 14: 0.9859833717346191\n",
      "Test acc at layer 14: 0.9664999842643738\n",
      "\n",
      "Train acc at layer 15: 0.9868500232696533\n",
      "Test acc at layer 15: 0.9668999910354614\n",
      "\n",
      "Train acc at layer 16: 0.987500011920929\n",
      "Test acc at layer 16: 0.9661999940872192\n",
      "\n",
      "Train acc at layer 17: 0.9884333610534668\n",
      "Test acc at layer 17: 0.9661999940872192\n",
      "\n",
      "Train acc at layer 18: 0.9892833232879639\n",
      "Test acc at layer 18: 0.9666999578475952\n",
      "\n",
      "Train acc at layer 19: 0.9899166822433472\n",
      "Test acc at layer 19: 0.9662999510765076\n",
      "\n",
      "Train acc at layer 20: 0.9903500080108643\n",
      "Test acc at layer 20: 0.9670999646186829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def see_results_for_every_layer(X_train, X_test):\n",
    "    with torch.no_grad():\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "\n",
    "        if model.upscale is not None:\n",
    "            X_train = model.upscale_fun(X0_train)\n",
    "            X_test = model.upscale_fun(X0_test)\n",
    "\n",
    "        y_pred_train = model.classifiers[0](X_train)\n",
    "        y_pred_test = model.classifiers[0](X_test)\n",
    "        print(f\"Train acc at layer 0: {torch.argmax(y_pred_train, dim=1).eq(y_train_cat).float().mean()}\")\n",
    "        print(f\"Test acc at layer 0: {torch.argmax(y_pred_test, dim=1).eq(y_test_cat).float().mean()}\")\n",
    "        print()\n",
    "        \n",
    "        for t, (fxt_fun, fx0_fun, (Delta, Delta_b)) in enumerate(zip(model.layers_fxt, model.layers_fx0, model.deltas)):\n",
    "            features_train = torch.cat([fxt_fun(X_train), fx0_fun(X0_train)], dim=1)\n",
    "            features_test = torch.cat([fxt_fun(X_test), fx0_fun(X0_test)], dim=1)\n",
    "            X_train = X_train + model.boost_lr * (features_train @ Delta + Delta_b)\n",
    "            X_test = X_test + model.boost_lr * (features_test @ Delta + Delta_b)\n",
    "            \n",
    "            y_pred_train = model.classifiers[t+1](X_train)\n",
    "            y_pred_test = model.classifiers[t+1](X_test)\n",
    "\n",
    "            print(f\"Train acc at layer {t+1}: {torch.argmax(y_pred_train, dim=1).eq(y_train_cat).float().mean()}\")\n",
    "            print(f\"Test acc at layer {t+1}: {torch.argmax(y_pred_test, dim=1).eq(y_test_cat).float().mean()}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "see_results_for_every_layer(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End2End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:10<00:15,  1.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m End2EndMLPResNet\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m End2EndMLPResNet(\n\u001b[1;32m      4\u001b[0m     in_dim \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      5\u001b[0m     hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m X_train_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m X_test_pred \u001b[38;5;241m=\u001b[39m model(X_test)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_test_pred)\n",
      "File \u001b[0;32m~/Code/random-feature-boosting/models/models.py:50\u001b[0m, in \u001b[0;36mFittableModule.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Tensor, y: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the module and return the transformed data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(X)\n",
      "File \u001b[0;32m~/Code/random-feature-boosting/models/models.py:657\u001b[0m, in \u001b[0;36mEnd2EndMLPResNet.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs)):\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m batch_X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip smaller batches since i sometimes have dataset size of 257 leading to training errors with batch norm\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:211\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:211\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.models import End2EndMLPResNet\n",
    "\n",
    "model = End2EndMLPResNet(\n",
    "    in_dim = X_train.shape[1],\n",
    "    hidden_dim = 128,\n",
    "    bottleneck_dim = 32,\n",
    "    out_dim = 10,\n",
    "    n_blocks = 4,\n",
    "    lr = 0.01,\n",
    "    end_lr_factor = 0.01,\n",
    "    n_epochs = 20,\n",
    "    weight_decay = 0.001,\n",
    "    batch_size = 512\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

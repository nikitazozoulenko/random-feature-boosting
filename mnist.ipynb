{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import openml\n",
    "\n",
    "#from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options\n",
    "device = \"cuda\" # torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([60000, 784])\n",
      "Train labels shape: torch.Size([60000, 10])\n",
      "Test data shape: torch.Size([10000, 784])\n",
      "Test labels shape: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def normalize_mean_std_traindata(X_train: Tensor, X_test: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    mean = X_train.mean(dim=0)\n",
    "    std = X_train.std(dim=0)\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "\n",
    "    X_train = torch.clip(X_train, -5, 5)\n",
    "    X_test = torch.clip(X_test, -5, 5)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_path = \"/home/nikita/hdd/MNIST\"\n",
    "trainset = datasets.MNIST(mnist_path, download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST(mnist_path, download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "# Flatten the data\n",
    "X_train, y_train_cat = next(iter(trainloader))\n",
    "X_train = X_train.view(len(trainset), -1).to(device)\n",
    "X_test, y_test_cat = next(iter(testloader))\n",
    "X_test = X_test.view(len(testset), -1).to(device)\n",
    "\n",
    "# Convert train and test labels to one-hot encoding\n",
    "y_train = nn.functional.one_hot(y_train_cat, num_classes=10).float().to(device)\n",
    "y_test = nn.functional.one_hot(y_test_cat, num_classes=10).float().to(device)\n",
    "y_train_cat = y_train_cat.to(device)\n",
    "y_test_cat = y_test_cat.to(device)\n",
    "\n",
    "# Normalize by mean and std\n",
    "X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_pred tensor([[ -0.2361, -10.2318,   0.4055,  ...,  11.1450,   0.0553,   3.3455],\n",
      "        [  5.9069,   1.3996,  13.2116,  ..., -18.7633,   4.5858, -11.9112],\n",
      "        [ -5.6929,   6.3866,   1.9411,  ...,   0.8476,   0.3768,  -1.5534],\n",
      "        ...,\n",
      "        [ -7.5355,  -7.2969,  -2.6505,  ...,   2.3439,   4.0803,   4.8392],\n",
      "        [ -2.7582,  -1.8546,  -3.1137,  ...,  -4.0667,   6.5052,  -3.2216],\n",
      "        [  2.7034, -10.4991,   4.8433,  ...,  -7.0398,  -0.4861,  -4.2308]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train accuracy: 0.9334999918937683\n",
      "Test accuracy: 0.9265999794006348\n"
     ]
    }
   ],
   "source": [
    "from models.base import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(\n",
    "        n_classes = 10,\n",
    "        l2_lambda = 0.001,\n",
    "        max_iter = 300,\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientRFBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.994533360004425\n",
      "Test accuracy: 0.9603999853134155\n"
     ]
    }
   ],
   "source": [
    "from models.random_feature_representation_boosting import GradientRFRBoostClassifier\n",
    "\n",
    "model = GradientRFRBoostClassifier(\n",
    "    in_dim = 784,\n",
    "    hidden_dim = 512,\n",
    "    n_classes = 10,\n",
    "    randfeat_xt_dim = 512,\n",
    "    randfeat_x0_dim = 512,\n",
    "    n_layers = 3,\n",
    "    l2_cls =  0.00001,\n",
    "    l2_ghat = 0.0001,\n",
    "    feature_type=\"SWIM\",\n",
    "    upscale_type = \"SWIM\",\n",
    "    lbfgs_max_iter = 300,\n",
    "    boost_lr = 1.0,\n",
    "    use_batchnorm=True,\n",
    "    do_linesearch= False,\n",
    "    SWIM_scale=1.0,\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "#TODO NEXT: add xtx0 to the classification case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ce at layer 0: 0.3866678476333618\n",
      "Test ce at layer 0: 0.3729083836078644\n",
      "Train acc at layer 0: 0.8935500383377075\n",
      "Test acc at layer 0: 0.8983999490737915\n",
      "\n",
      "Train ce at layer 1: 0.24427926540374756\n",
      "Test ce at layer 1: 0.2442425787448883\n",
      "Train acc at layer 1: 0.9316666722297668\n",
      "Test acc at layer 1: 0.9311999678611755\n",
      "\n",
      "Train ce at layer 2: 0.23991598188877106\n",
      "Test ce at layer 2: 0.24039645493030548\n",
      "Train acc at layer 2: 0.9330166578292847\n",
      "Test acc at layer 2: 0.932699978351593\n",
      "\n",
      "Train ce at layer 3: 0.23564143478870392\n",
      "Test ce at layer 3: 0.23655599355697632\n",
      "Train acc at layer 3: 0.9344333410263062\n",
      "Test acc at layer 3: 0.933899998664856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def see_results_for_every_layer(X_train, y_train, X_test, y_test, model, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "\n",
    "        X_train = model.upscale(X0_train)\n",
    "        X_test = model.upscale(X0_test)\n",
    "\n",
    "        pred_train = model.top_level_modules[0](X_train)\n",
    "        pred_test = model.top_level_modules[0](X_test)\n",
    "\n",
    "        ce = loss_fn(pred_train, y_train)\n",
    "        ce_test = loss_fn(pred_test, y_test)\n",
    "        acc = (pred_train.argmax(1) == y_train.argmax(1)).float().mean()\n",
    "        acc_test = (pred_test.argmax(1) == y_test.argmax(1)).float().mean()\n",
    "        print(f\"Train ce at layer 0: {ce}\")\n",
    "        print(f\"Test ce at layer 0: {ce_test}\")\n",
    "        print(f\"Train acc at layer 0: {acc}\")\n",
    "        print(f\"Test acc at layer 0: {acc_test}\")\n",
    "        print()\n",
    "        \n",
    "        for t, (feat_layer, ghat_layer, classifier, batchnorm) in enumerate(zip(model.random_feature_layers, \n",
    "                                                                     model.ghat_boosting_layers, \n",
    "                                                                     model.top_level_modules[1:],\n",
    "                                                                     model.batchnorms)):\n",
    "            features_train = feat_layer(X_train, X0_train)\n",
    "            features_test = feat_layer(X_test, X0_test)\n",
    "            X_train += model.boost_lr * ghat_layer(features_train)\n",
    "            X_train = batchnorm(X_train)\n",
    "            X_test  += model.boost_lr * ghat_layer(features_test)\n",
    "            X_test = batchnorm(X_test)\n",
    "            \n",
    "            pred_train = classifier(X_train)\n",
    "            pred_test = classifier(X_test)\n",
    "\n",
    "            ce = loss_fn(pred_train, y_train)\n",
    "            ce_test = loss_fn(pred_test, y_test)\n",
    "            acc = (pred_train.argmax(1) == y_train.argmax(1)).float().mean()\n",
    "            acc_test = (pred_test.argmax(1) == y_test.argmax(1)).float().mean()\n",
    "            print(f\"Train ce at layer {t+1}: {ce}\")\n",
    "            print(f\"Test ce at layer {t+1}: {ce_test}\")\n",
    "            print(f\"Train acc at layer {t+1}: {acc}\")\n",
    "            print(f\"Test acc at layer {t+1}: {acc_test}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "see_results_for_every_layer(X_train, y_train, X_test, y_test, model, nn.functional.cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End2End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:21<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_pred tensor([[-7.71678984165191650391e-04, -7.49208033084869384766e-03,\n",
      "         -1.24663859605789184570e-03,  ...,\n",
      "          1.00676012039184570312e+00, -1.60436332225799560547e-03,\n",
      "         -8.16347450017929077148e-03],\n",
      "        [-1.68493315577507019043e-02, -1.36995688080787658691e-02,\n",
      "          1.01810526847839355469e+00,  ...,\n",
      "          1.14518180489540100098e-02,  4.63806092739105224609e-04,\n",
      "          2.97597795724868774414e-03],\n",
      "        [ 2.02718377113342285156e-03,  9.92150068283081054688e-01,\n",
      "         -2.91625410318374633789e-03,  ...,\n",
      "         -7.71909952163696289062e-03,  4.19247895479202270508e-03,\n",
      "         -1.08579769730567932129e-02],\n",
      "        ...,\n",
      "        [ 6.20144605636596679688e-03, -4.16323542594909667969e-04,\n",
      "          7.67238438129425048828e-04,  ...,\n",
      "         -8.91387462615966796875e-05,  8.60729813575744628906e-03,\n",
      "         -2.18964368104934692383e-03],\n",
      "        [ 4.13244962692260742188e-03,  8.91607254743576049805e-03,\n",
      "         -3.58318537473678588867e-03,  ...,\n",
      "          1.03406831622123718262e-02,  2.40520387887954711914e-03,\n",
      "         -2.01981663703918457031e-02],\n",
      "        [ 1.78483128547668457031e-03, -6.35444372892379760742e-03,\n",
      "          3.57230752706527709961e-03,  ...,\n",
      "         -2.01522558927536010742e-03, -3.00440937280654907227e-03,\n",
      "         -5.69839030504226684570e-03]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Train accuracy: 0.9958166480064392\n",
      "Test accuracy: 0.9817999601364136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from models.end2end import End2EndMLPResNet\n",
    "\n",
    "model = End2EndMLPResNet(\n",
    "    in_dim = X_train.shape[1],\n",
    "    hidden_dim = 128,\n",
    "    bottleneck_dim = 32,\n",
    "    out_dim = 10,\n",
    "    n_blocks = 4,\n",
    "    lr = 0.01,\n",
    "    end_lr_factor = 0.01,\n",
    "    n_epochs = 20,\n",
    "    weight_decay = 0.001,\n",
    "    batch_size = 512\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import openml\n",
    "\n",
    "#from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options\n",
    "device = \"cuda\" # torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([60000, 784])\n",
      "Train labels shape: torch.Size([60000, 10])\n",
      "Test data shape: torch.Size([10000, 784])\n",
      "Test labels shape: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def normalize_mean_std_traindata(X_train: Tensor, X_test: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    mean = X_train.mean(dim=0)\n",
    "    std = X_train.std(dim=0)\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "\n",
    "    X_train = torch.clip(X_train, -5, 5)\n",
    "    X_test = torch.clip(X_test, -5, 5)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_path = \"/home/nikita/hdd/MNIST\"\n",
    "trainset = datasets.MNIST(mnist_path, download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST(mnist_path, download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "# Flatten the data\n",
    "X_train, y_train_cat = next(iter(trainloader))\n",
    "X_train = X_train.view(len(trainset), -1).to(device)\n",
    "X_test, y_test_cat = next(iter(testloader))\n",
    "X_test = X_test.view(len(testset), -1).to(device)\n",
    "\n",
    "# Convert train and test labels to one-hot encoding\n",
    "y_train = nn.functional.one_hot(y_train_cat, num_classes=10).float().to(device)\n",
    "y_test = nn.functional.one_hot(y_test_cat, num_classes=10).float().to(device)\n",
    "y_train_cat = y_train_cat.to(device)\n",
    "y_test_cat = y_test_cat.to(device)\n",
    "\n",
    "# Normalize by mean and std\n",
    "X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_pred tensor([[  0.4175, -11.0949,   1.2466,  ...,  12.8043,   1.6586,   5.1637],\n",
      "        [  8.1309,   3.6030,  16.4524,  ..., -24.1260,   5.9731, -13.4601],\n",
      "        [ -6.7998,   7.4932,   3.1322,  ...,   1.7728,   1.4733,  -1.5069],\n",
      "        ...,\n",
      "        [ -8.2448,  -8.1426,  -2.6560,  ...,   3.7084,   5.7502,   6.2365],\n",
      "        [ -2.6307,  -1.3099,  -2.6204,  ...,  -4.6006,   9.3278,  -2.5876],\n",
      "        [  4.1184, -11.7016,   6.9620,  ...,  -6.8436,   2.0377,  -5.2548]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train accuracy: 0.9338833689689636\n",
      "Test accuracy: 0.9279999732971191\n"
     ]
    }
   ],
   "source": [
    "from models.models import LogisticRegression, FittableModule\n",
    "\n",
    "\n",
    "class LogisticRegression(FittableModule):\n",
    "    def __init__(self, \n",
    "                 in_dim: int,\n",
    "                 out_dim: int = 10,\n",
    "                 l2_reg: float = 0.001,\n",
    "                 lr: float = 1.0,\n",
    "                 max_iter: int = 100,\n",
    "                 ):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.l2_reg = l2_reg\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        if out_dim > 1:\n",
    "            self.loss = nn.functional.cross_entropy #this is with logits\n",
    "        else:\n",
    "            self.loss = nn.functional.binary_cross_entropy_with_logits\n",
    "\n",
    "\n",
    "    def fit(self, \n",
    "            X: Tensor, \n",
    "            y: Tensor,\n",
    "            init_W_b: Optional[Tuple[Tensor, Tensor]] = None,\n",
    "            ):\n",
    "        \n",
    "        # No onehot encoding\n",
    "        if y.dim() > 1:\n",
    "            y_labels = torch.argmax(y, dim=1)\n",
    "        else:\n",
    "            y_labels = y\n",
    "\n",
    "        # Put model on device\n",
    "        device = X.device\n",
    "        self.to(device)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        if init_W_b is not None:\n",
    "            W, b = init_W_b\n",
    "            self.linear.weight.data = W\n",
    "            self.linear.bias.data = b\n",
    "        else:\n",
    "            nn.init.kaiming_normal_(self.linear.weight)\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "            \n",
    "        with torch.enable_grad():\n",
    "            # Optimize\n",
    "            optimizer = torch.optim.LBFGS(self.linear.parameters(), lr=self.lr, max_iter=self.max_iter)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.linear(X)\n",
    "                loss = self.loss(logits, y_labels)\n",
    "                loss += self.l2_reg * torch.linalg.norm(self.linear.weight)**2\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        return self.linear(X)\n",
    "\n",
    "\n",
    "model = LogisticRegression(\n",
    "        in_dim = 784,\n",
    "        out_dim = 10,\n",
    "        l2_reg = 0.0001,\n",
    "        max_iter = 100,\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train_cat)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientRFBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linesearch 0.04402843490242958\n",
      "t 0 Gradient hat norm tensor(254.2389, device='cuda:0')\n",
      "Linesearch 0.007105377968400717\n",
      "t 1 Gradient hat norm tensor(459.0419, device='cuda:0')\n",
      "Linesearch 0.13384537398815155\n",
      "t 2 Gradient hat norm tensor(67.2621, device='cuda:0')\n",
      "Linesearch 0.1739414483308792\n",
      "t 3 Gradient hat norm tensor(46.2600, device='cuda:0')\n",
      "Linesearch 0.23741832375526428\n",
      "t 4 Gradient hat norm tensor(33.9673, device='cuda:0')\n",
      "Linesearch 0.2939264476299286\n",
      "t 5 Gradient hat norm tensor(26.8308, device='cuda:0')\n",
      "Linesearch 0.299819678068161\n",
      "t 6 Gradient hat norm tensor(23.1804, device='cuda:0')\n",
      "Linesearch 0.20965401828289032\n",
      "t 7 Gradient hat norm tensor(26.7779, device='cuda:0')\n",
      "Linesearch 0.2635953724384308\n",
      "t 8 Gradient hat norm tensor(22.8784, device='cuda:0')\n",
      "Linesearch 0.33549538254737854\n",
      "t 9 Gradient hat norm tensor(18.4807, device='cuda:0')\n",
      "Linesearch 0.23749461770057678\n",
      "t 10 Gradient hat norm tensor(20.3547, device='cuda:0')\n",
      "Linesearch 0.07007607817649841\n",
      "t 11 Gradient hat norm tensor(53.4891, device='cuda:0')\n",
      "Linesearch 0.10344744473695755\n",
      "t 12 Gradient hat norm tensor(34.8128, device='cuda:0')\n",
      "Linesearch 0.26903197169303894\n",
      "t 13 Gradient hat norm tensor(21.2565, device='cuda:0')\n",
      "Linesearch 0.4263806939125061\n",
      "t 14 Gradient hat norm tensor(15.8604, device='cuda:0')\n",
      "Linesearch 0.3431670367717743\n",
      "t 15 Gradient hat norm tensor(17.0082, device='cuda:0')\n",
      "Linesearch 0.35030072927474976\n",
      "t 16 Gradient hat norm tensor(15.6782, device='cuda:0')\n",
      "Linesearch 0.32923611998558044\n",
      "t 17 Gradient hat norm tensor(15.4382, device='cuda:0')\n",
      "Linesearch 0.31423714756965637\n",
      "t 18 Gradient hat norm tensor(15.7123, device='cuda:0')\n",
      "Linesearch 0.3361256718635559\n",
      "t 19 Gradient hat norm tensor(14.8580, device='cuda:0')\n",
      "X_test_pred tensor([[  1.2128,  -7.1920,  -5.7210,  ...,  19.0348,  -4.0923,   9.4132],\n",
      "        [  0.9200,   4.9302,  20.3026,  ..., -12.5443,   5.6175, -14.8596],\n",
      "        [ -4.6473,  10.9855,  -0.3874,  ...,  -1.4418,   0.6440,  -2.3280],\n",
      "        ...,\n",
      "        [ -5.4169,  -7.8547, -10.4121,  ...,   3.2970,   5.5927,   9.0729],\n",
      "        [ -2.2083,  -0.9220,  -5.0057,  ...,   0.0895,   9.4561,  -8.6585],\n",
      "        [  9.6546, -12.3778,   3.4326,  ...,  -8.0771,  -4.5801,  -5.2508]],\n",
      "       device='cuda:0')\n",
      "Train accuracy: 0.9982333183288574\n",
      "Test accuracy: 0.9716999530792236\n"
     ]
    }
   ],
   "source": [
    "from models.models import FittableModule, create_layer, fit_ridge_ALOOCV, Identity\n",
    "\n",
    "def line_search_cross_entropy(cls, X, y, G_hat):\n",
    "    \"\"\"Solves the line search risk minimizatin problem\n",
    "    R(W, X + a * g) for mutliclass cross entropy loss\"\"\"\n",
    "    # No onehot encoding\n",
    "    if y.dim() > 1:\n",
    "        y_labels = torch.argmax(y, dim=1)\n",
    "    else:\n",
    "        y_labels = y\n",
    "\n",
    "    # Optimize\n",
    "    with torch.enable_grad():\n",
    "        alpha = torch.tensor([0.0], requires_grad=True, device=X.device, dtype=X.dtype)\n",
    "        optimizer = torch.optim.LBFGS([alpha])\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            logits = cls(X + alpha * G_hat)\n",
    "            loss = nn.functional.cross_entropy(logits, y_labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return alpha.detach().item()\n",
    "\n",
    "\n",
    "class GradientRFBoostClassifier(FittableModule):\n",
    "    def __init__(self, \n",
    "                 hidden_dim: int = 128, # TODO\n",
    "                 randfeat_xt_dim: int = 128,\n",
    "                 randfeat_x0_dim: int = 128,\n",
    "                 out_dim: int = 10,\n",
    "                 n_layers: int = 5,\n",
    "                 activation: nn.Module = nn.Tanh(),\n",
    "                 l2_reg: float = 1,\n",
    "                 feature_type = \"SWIM\", # \"dense\", identity\n",
    "                 boost_lr: float = 1.0,\n",
    "                 upscale: Optional[str] = \"dense\",\n",
    "                 max_iter: int = 100,\n",
    "                 ridge_l2: float = 0.001,\n",
    "                 ):\n",
    "        super(GradientRFBoostClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.randfeat_xt_dim = randfeat_xt_dim\n",
    "        self.randfeat_x0_dim = randfeat_x0_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.l2_reg = l2_reg\n",
    "        self.feature_type = feature_type\n",
    "        self.boost_lr = boost_lr\n",
    "        self.upscale = upscale\n",
    "        self.max_iter = max_iter\n",
    "        self.ridge_l2 = ridge_l2\n",
    "    \n",
    "\n",
    "    def fit_transform(self, X: Tensor, y: Tensor):\n",
    "        with torch.no_grad():\n",
    "            X0 = X\n",
    "\n",
    "            #optional upscale\n",
    "            if self.upscale == \"dense\":\n",
    "                self.upscale_fun = create_layer(self.upscale, X.shape[1], self.hidden_dim, None)\n",
    "                X = self.upscale_fun.fit_transform(X, y)\n",
    "            elif self.upscale == \"SWIM\":\n",
    "                self.upscale_fun = create_layer(self.upscale, X.shape[1], self.hidden_dim, self.activation)\n",
    "                X = self.upscale_fun.fit_transform(X, y)\n",
    "            elif self.upscale == \"identity\":\n",
    "                self.upscale_fun = Identity()\n",
    "                self.hidden_dim = X.size(1)\n",
    "            else:\n",
    "                raise ValueError(f\"Parameter not recoginized. Given: {self.upscale}\")\n",
    "\n",
    "\n",
    "            # Create classifier W_0\n",
    "            cls = LogisticRegression(\n",
    "                in_dim = self.hidden_dim,\n",
    "                out_dim = self.out_dim,\n",
    "                l2_reg = self.l2_reg,\n",
    "                max_iter = self.max_iter,\n",
    "            ).to(X.device)\n",
    "            cls.fit(X, y)\n",
    "            # save for now. for more memory efficient implementation, we can remove a lot of this\n",
    "            self.classifiers = [cls]\n",
    "            self.layers_fxt = []\n",
    "            self.layers_fx0 = []\n",
    "            self.deltas = []\n",
    "\n",
    "            # Layerwise boosting\n",
    "            N = X.size(0)\n",
    "            prev_cls = None if self.upscale != \"identity\" else cls\n",
    "            for t in range(self.n_layers):\n",
    "                # Step 2: Obtain activation gradient\n",
    "                # X shape (N, D) --- ResNet neurons\n",
    "                # F shape (N, p) --- random features\n",
    "                # y shape (N, d) --- one-hot target\n",
    "                # r shape (N, D) --- residual at currect boosting iteration\n",
    "                # W shape (D, d) --- top level classifier\n",
    "                # probs shape (N, d) --- predicted probabilities\n",
    "\n",
    "\n",
    "                # Step 1: Create random feature layer   \n",
    "                fxt_fun = create_layer(self.feature_type, self.hidden_dim, self.randfeat_xt_dim, self.activation)\n",
    "                fx0_fun = create_layer(self.feature_type, X0.size(1), self.randfeat_x0_dim, self.activation)\n",
    "                Fxt = fxt_fun.fit_transform(X, y)\n",
    "                Fx0 = fx0_fun.fit_transform(X0, y)\n",
    "                F = torch.cat([Fxt, Fx0], dim=1)\n",
    "\n",
    "\n",
    "                # Step 2: Obtain activation gradient and learn Delta\n",
    "                probs = nn.functional.softmax(cls(X), dim=1)\n",
    "                G = (y - probs) @ cls.linear.weight #negative gradient TODO divide by N?\n",
    "                G = G / torch.norm(G) * N**0.5 #normalize to unit L2(mu) norm?\n",
    "                # fit Least Squares to negative gradient (finding functional direction)\n",
    "                Delta, Delta_b, _ = fit_ridge_ALOOCV(F, G, alphas=[self.ridge_l2])\n",
    "                # Line search for risk minimization of R(W_t, Phi_t + linesearch * G_hat)\n",
    "                G_hat = F @ Delta + Delta_b\n",
    "                linesearch = line_search_cross_entropy(cls, X, y, G_hat)\n",
    "                print(\"Linesearch\", linesearch)\n",
    "                print(\"t\", t, \"Gradient hat norm\", torch.linalg.norm(G_hat))\n",
    "\n",
    "\n",
    "                # Step 3: Learn top level classifier\n",
    "                X = X + self.boost_lr * linesearch * G_hat\n",
    "                cls = LogisticRegression(\n",
    "                    in_dim = self.hidden_dim,\n",
    "                    out_dim = self.out_dim,\n",
    "                    l2_reg = self.l2_reg,\n",
    "                    max_iter = self.max_iter,\n",
    "                ).to(X.device)\n",
    "                cls.fit(\n",
    "                    X, \n",
    "                    y, \n",
    "                    init_W_b = (\n",
    "                        (prev_cls.linear.weight.detach().clone(), \n",
    "                        prev_cls.linear.bias.detach().clone())\n",
    "                        if prev_cls is not None else None\n",
    "                        ) \n",
    "                )\n",
    "                prev_cls = cls\n",
    "\n",
    "                #update Delta scale\n",
    "                Delta = Delta * linesearch\n",
    "                Delta_b = Delta_b * linesearch\n",
    "\n",
    "                # store\n",
    "                self.layers_fxt.append(fxt_fun)\n",
    "                self.layers_fx0.append(fx0_fun)\n",
    "                self.deltas.append((Delta, Delta_b))\n",
    "                self.classifiers.append(cls)\n",
    "\n",
    "        return cls(X)\n",
    "\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            X0 = X\n",
    "            if self.upscale is not None:\n",
    "                X = self.upscale_fun(X)\n",
    "            for fxt_fun, fx0_fun, (Delta, Delta_b) in zip(self.layers_fxt, self.layers_fx0, self.deltas):\n",
    "                features = torch.cat([fxt_fun(X), fx0_fun(X0)], dim=1)\n",
    "                X = X + self.boost_lr * (features @ Delta + Delta_b)\n",
    "            return self.classifiers[-1](X)\n",
    "        \n",
    "\n",
    "model = GradientRFBoostClassifier(\n",
    "        hidden_dim = 128,\n",
    "        randfeat_xt_dim = 256,\n",
    "        randfeat_x0_dim = 256,\n",
    "        out_dim = 10,\n",
    "        n_layers = 20,\n",
    "        l2_reg = 0.000001,\n",
    "        ridge_l2 = 0.000000001,\n",
    "        feature_type=\"SWIM\",\n",
    "        upscale = \"SWIM\",\n",
    "        max_iter = 300,\n",
    "        boost_lr = 1.0,\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "#TODO NEXT: add xtx0 to the classification case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc at layer 0: 0.9188666939735413\n",
      "Test acc at layer 0: 0.91839998960495\n",
      "\n",
      "Train acc at layer 1: 0.9416000247001648\n",
      "Test acc at layer 1: 0.9378999471664429\n",
      "\n",
      "Train acc at layer 2: 0.9431833624839783\n",
      "Test acc at layer 2: 0.9405999779701233\n",
      "\n",
      "Train acc at layer 3: 0.9627000093460083\n",
      "Test acc at layer 3: 0.9580999612808228\n",
      "\n",
      "Train acc at layer 4: 0.9702666997909546\n",
      "Test acc at layer 4: 0.9627999663352966\n",
      "\n",
      "Train acc at layer 5: 0.9768500328063965\n",
      "Test acc at layer 5: 0.9673999547958374\n",
      "\n",
      "Train acc at layer 6: 0.9818500280380249\n",
      "Test acc at layer 6: 0.9684000015258789\n",
      "\n",
      "Train acc at layer 7: 0.9851000308990479\n",
      "Test acc at layer 7: 0.9702000021934509\n",
      "\n",
      "Train acc at layer 8: 0.987416684627533\n",
      "Test acc at layer 8: 0.9699999690055847\n",
      "\n",
      "Train acc at layer 9: 0.9891000390052795\n",
      "Test acc at layer 9: 0.9703999757766724\n",
      "\n",
      "Train acc at layer 10: 0.9913666844367981\n",
      "Test acc at layer 10: 0.9720999598503113\n",
      "\n",
      "Train acc at layer 11: 0.9924499988555908\n",
      "Test acc at layer 11: 0.9718999862670898\n",
      "\n",
      "Train acc at layer 12: 0.9931333661079407\n",
      "Test acc at layer 12: 0.9721999764442444\n",
      "\n",
      "Train acc at layer 13: 0.9934666752815247\n",
      "Test acc at layer 13: 0.9721999764442444\n",
      "\n",
      "Train acc at layer 14: 0.9939500093460083\n",
      "Test acc at layer 14: 0.9726999998092651\n",
      "\n",
      "Train acc at layer 15: 0.9954666495323181\n",
      "Test acc at layer 15: 0.9709999561309814\n",
      "\n",
      "Train acc at layer 16: 0.996150016784668\n",
      "Test acc at layer 16: 0.973099946975708\n",
      "\n",
      "Train acc at layer 17: 0.9969833493232727\n",
      "Test acc at layer 17: 0.9705999493598938\n",
      "\n",
      "Train acc at layer 18: 0.9974666833877563\n",
      "Test acc at layer 18: 0.9713999629020691\n",
      "\n",
      "Train acc at layer 19: 0.99795001745224\n",
      "Test acc at layer 19: 0.9698999524116516\n",
      "\n",
      "Train acc at layer 20: 0.9982333183288574\n",
      "Test acc at layer 20: 0.9716999530792236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def see_results_for_every_layer(X_train, X_test):\n",
    "    with torch.no_grad():\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "\n",
    "        if model.upscale is not None:\n",
    "            X_train = model.upscale_fun(X0_train)\n",
    "            X_test = model.upscale_fun(X0_test)\n",
    "\n",
    "        y_pred_train = model.classifiers[0](X_train)\n",
    "        y_pred_test = model.classifiers[0](X_test)\n",
    "        print(f\"Train acc at layer 0: {torch.argmax(y_pred_train, dim=1).eq(y_train_cat).float().mean()}\")\n",
    "        print(f\"Test acc at layer 0: {torch.argmax(y_pred_test, dim=1).eq(y_test_cat).float().mean()}\")\n",
    "        print()\n",
    "        \n",
    "        for t, (fxt_fun, fx0_fun, (Delta, Delta_b)) in enumerate(zip(model.layers_fxt, model.layers_fx0, model.deltas)):\n",
    "            features_train = torch.cat([fxt_fun(X_train), fx0_fun(X0_train)], dim=1)\n",
    "            features_test = torch.cat([fxt_fun(X_test), fx0_fun(X0_test)], dim=1)\n",
    "            X_train = X_train + model.boost_lr * (features_train @ Delta + Delta_b)\n",
    "            X_test = X_test + model.boost_lr * (features_test @ Delta + Delta_b)\n",
    "            \n",
    "            y_pred_train = model.classifiers[t+1](X_train)\n",
    "            y_pred_test = model.classifiers[t+1](X_test)\n",
    "\n",
    "            print(f\"Train acc at layer {t+1}: {torch.argmax(y_pred_train, dim=1).eq(y_train_cat).float().mean()}\")\n",
    "            print(f\"Test acc at layer {t+1}: {torch.argmax(y_pred_test, dim=1).eq(y_test_cat).float().mean()}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "see_results_for_every_layer(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End2End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:17<00:04,  1.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m End2EndMLPResNet\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m End2EndMLPResNet(\n\u001b[1;32m      4\u001b[0m     in_dim \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      5\u001b[0m     hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m X_train_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m X_test_pred \u001b[38;5;241m=\u001b[39m model(X_test)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_test_pred)\n",
      "File \u001b[0;32m~/Code/random-feature-boosting/models/models.py:50\u001b[0m, in \u001b[0;36mFittableModule.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Tensor, y: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the module and return the transformed data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(X)\n",
      "File \u001b[0;32m~/Code/random-feature-boosting/models/models.py:663\u001b[0m, in \u001b[0;36mEnd2EndMLPResNet.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    661\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(batch_X)\n\u001b[1;32m    662\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(outputs, batch_y)\n\u001b[0;32m--> 663\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.models import End2EndMLPResNet\n",
    "\n",
    "model = End2EndMLPResNet(\n",
    "    in_dim = X_train.shape[1],\n",
    "    hidden_dim = 128,\n",
    "    bottleneck_dim = 32,\n",
    "    out_dim = 10,\n",
    "    n_blocks = 4,\n",
    "    lr = 0.01,\n",
    "    end_lr_factor = 0.01,\n",
    "    n_epochs = 20,\n",
    "    weight_decay = 0.001,\n",
    "    batch_size = 512\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_all_experiments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_all_experiments\u001b[49m(name_save\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTESTING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_all_experiments' is not defined"
     ]
    }
   ],
   "source": [
    "_ = run_all_experiments(name_save=\"TESTING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T=16 Gradient GRFBoost               0.9690\n",
       "T=11 Gradient GRFBoost               0.9684\n",
       "T=6 Gradient GRFBoost                0.9649\n",
       "Logistic L-BFSG, l2=1e-05 lr=1.0     0.9243\n",
       "Logistic L-BFSG, l2=0.0001 lr=1.0    0.9238\n",
       "Logistic L-BFSG, l2=0.001 lr=1.0     0.9231\n",
       "T=1 Gradient GRFBoost                0.9226\n",
       "Logistic L-BFSG, l2=1e-06 lr=1.0     0.9220\n",
       "Logistic L-BFSG, l2=0.01 lr=1.0      0.9174\n",
       "Logistic L-BFSG, l2=0.1 lr=1.0       0.8991\n",
       "Logistic L-BFSG, l2=1 lr=1.0         0.8480\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"MNIST_TESTING.pkl\")\n",
    "df[\"acc_test\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T=5 End2End        0.9717\n",
    "# T=1 Dense          0.9215\n",
    "# T=1 SWIM Unif      0.9207\n",
    "# T=1 SWIM Grad      0.9204\n",
    "# Logistic SGD       0.8990\n",
    "# Tabular Ridge      0.8606\n",
    "# Logistic L-BFSG    0.8480\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T=16 Gradient GRFBoost               0.978417\n",
       "T=11 Gradient GRFBoost               0.976083\n",
       "T=6 Gradient GRFBoost                0.973017\n",
       "Logistic L-BFSG, l2=1e-05 lr=1.0     0.931450\n",
       "Logistic L-BFSG, l2=1e-06 lr=1.0     0.931200\n",
       "Logistic L-BFSG, l2=0.0001 lr=1.0    0.931000\n",
       "Logistic L-BFSG, l2=0.001 lr=1.0     0.930167\n",
       "T=1 Gradient GRFBoost                0.922150\n",
       "Logistic L-BFSG, l2=0.01 lr=1.0      0.918317\n",
       "Logistic L-BFSG, l2=0.1 lr=1.0       0.894617\n",
       "Logistic L-BFSG, l2=1 lr=1.0         0.838183\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"acc_train\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment layer by layer with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.33945271372795105\n",
      "Gradient hat norm tensor(198.0060, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.6588780879974365\n",
      "Gradient hat norm tensor(132.4522, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.1048954725265503\n",
      "Gradient hat norm tensor(65.1581, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.2053641080856323\n",
      "Gradient hat norm tensor(44.8501, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.7405498027801514\n",
      "Gradient hat norm tensor(32.7763, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 2.224963903427124\n",
      "Gradient hat norm tensor(25.0595, device='cuda:0')\n",
      "alpha 1.0\n",
      "linesearch loss tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 3.6836495399475098\n",
      "Gradient hat norm tensor(14.5713, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 3.8727076053619385\n",
      "Gradient hat norm tensor(9.0920, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 7.4931769371032715\n",
      "Gradient hat norm tensor(6.6352, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.016352470964193344\n",
      "Gradient hat norm tensor(5.8634, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.013967355713248253\n",
      "Gradient hat norm tensor(5.2619, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.016038687899708748\n",
      "Gradient hat norm tensor(5.6519, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.015640607103705406\n",
      "Gradient hat norm tensor(5.5188, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.01538341585546732\n",
      "Gradient hat norm tensor(5.4181, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.014461412094533443\n",
      "Gradient hat norm tensor(5.2664, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 3.7600932121276855\n",
      "Gradient hat norm tensor(7.0570, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.013705299235880375\n",
      "Gradient hat norm tensor(5.3371, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.01223501842468977\n",
      "Gradient hat norm tensor(4.8414, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.012786706909537315\n",
      "Gradient hat norm tensor(5.0744, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.011432201601564884\n",
      "Gradient hat norm tensor(4.7598, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Time to experiment with RandFeatureBoost\n",
    "\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 512\n",
    "n_blocks = 20\n",
    "boost_lr=1.0\n",
    "\n",
    "model = GradientRandomFeatureBoostingClassification(\n",
    "    generator=generator,\n",
    "    hidden_dim=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_layers=n_blocks,\n",
    "    l2_reg=0.0001,\n",
    "    boost_lr=boost_lr,\n",
    "    feature_type=\"SWIM\",\n",
    "    upscale=\"SWIM\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626833200454712\n",
      "0.9571999907493591\n"
     ]
    }
   ],
   "source": [
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0: Train acc: 0.9140666723251343, Test acc: 0.9178999662399292\n",
      "Block 1: Train acc: 0.9391999840736389, Test acc: 0.9405999779701233\n",
      "Block 2: Train acc: 0.949066698551178, Test acc: 0.9469999670982361\n",
      "Block 3: Train acc: 0.9535666704177856, Test acc: 0.9508000016212463\n",
      "Block 4: Train acc: 0.95660001039505, Test acc: 0.9531999826431274\n",
      "Block 5: Train acc: 0.9593999981880188, Test acc: 0.9551999568939209\n",
      "Block 6: Train acc: 0.9609000086784363, Test acc: 0.9566999673843384\n",
      "Block 7: Train acc: 0.9617166519165039, Test acc: 0.9573999643325806\n",
      "Block 8: Train acc: 0.9622666835784912, Test acc: 0.9566999673843384\n",
      "Block 9: Train acc: 0.9624500274658203, Test acc: 0.9565999507904053\n",
      "Block 10: Train acc: 0.9620500206947327, Test acc: 0.9563999772071838\n",
      "Block 11: Train acc: 0.9622333645820618, Test acc: 0.9565999507904053\n",
      "Block 12: Train acc: 0.9622666835784912, Test acc: 0.9563999772071838\n",
      "Block 13: Train acc: 0.9622833728790283, Test acc: 0.9565999507904053\n",
      "Block 14: Train acc: 0.9623667001724243, Test acc: 0.9569999575614929\n",
      "Block 15: Train acc: 0.9625999927520752, Test acc: 0.9573999643325806\n",
      "Block 16: Train acc: 0.962850034236908, Test acc: 0.9572999477386475\n",
      "Block 17: Train acc: 0.9627166986465454, Test acc: 0.9569999575614929\n",
      "Block 18: Train acc: 0.9627166986465454, Test acc: 0.9573999643325806\n",
      "Block 19: Train acc: 0.9626833200454712, Test acc: 0.9571999907493591\n"
     ]
    }
   ],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    with torch.no_grad():\n",
    "        X_train = model.upscale(X_train)\n",
    "        X_test  = model.upscale(X_test)\n",
    "\n",
    "        for t, (layer, (Delta, Delta_b), cls) in enumerate(zip(model.layers, model.deltas, model.classifiers[1:])):\n",
    "            X_train += model.boost_lr * (layer(X_train)@Delta + Delta_b )\n",
    "            X_test +=  model.boost_lr * (layer(X_test)@Delta + Delta_b )\n",
    "\n",
    "            #delta norm\n",
    "\n",
    "            pred_train = cls(X_train)\n",
    "            pred_test = cls(X_test)\n",
    "            pred_train = torch.argmax(pred_train, dim=1)\n",
    "            pred_test = torch.argmax(pred_test, dim=1)\n",
    "            acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "            acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "            print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "            # print(\"delta norm\", torch.linalg.norm(Delta).item())\n",
    "            # print(\"X_train norm\", torch.linalg.norm(X_train).item() / X_train.size(0))\n",
    "            # print(\"X_test norm\", torch.linalg.norm(X_test).item() / X_test.size(0))\n",
    "            \n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: normalize the gradient before fitting the next layer. This is to find the optimal direction. Then do line search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rand feat boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:03<00:19,  1.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     10\u001b[0m boost_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m RandFeatBoost(\n\u001b[1;32m     13\u001b[0m     generator\u001b[38;5;241m=\u001b[39mgenerator,\n\u001b[1;32m     14\u001b[0m     in_dim\u001b[38;5;241m=\u001b[39mD,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     upscale_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSWIM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 27\u001b[0m pred_train, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m pred_test \u001b[38;5;241m=\u001b[39m model(X_test)\n\u001b[1;32m     29\u001b[0m pred_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(pred_train, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Code/zephyrox/pytorch_based/SWIM/models.py:757\u001b[0m, in \u001b[0;36mRandFeatBoost.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madam_lr, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs)):\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    758\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    760\u001b[0m         \u001b[38;5;66;03m#forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Time to experiment with RandFeatureBoost\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "num_epochs = 30\n",
    "batch_size = 512\n",
    "n_blocks = 5\n",
    "adam_lr=1e-2\n",
    "boost_lr=0.9\n",
    "\n",
    "model = RandFeatBoost(\n",
    "    generator=generator,\n",
    "    in_dim=D,\n",
    "    hidden_size=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_blocks=n_blocks,\n",
    "    activation=nn.Tanh(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    adam_lr=adam_lr,\n",
    "    boost_lr=boost_lr,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    upscale_type=\"SWIM\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    X_train = model.upscale(X_train)\n",
    "    X_test  = model.upscale(X_test)\n",
    "\n",
    "    for t, (layer, DELTA, classifier) in enumerate(zip(model.layers, model.deltas, model.classifiers)):\n",
    "        X_train = X_train + model.boost_lr * DELTA * (layer(X_train) - X_train)\n",
    "        X_test = X_test + model.boost_lr * DELTA * (layer(X_test) - X_test)\n",
    "        \n",
    "        pred_train = classifier(X_train)\n",
    "        pred_test = classifier(X_test)\n",
    "        pred_train = torch.argmax(pred_train, dim=1)\n",
    "        pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "\n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SWIM-ID vs DENSE-ID vs SWIM-DENSE \n",
    "# implement 'finding gradient direction' gradient boosting\n",
    "\n",
    "# Test whether this is actually better than non-boost with same hidden size !!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with DENSE\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 800\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "n_blocks = 10\n",
    "adam_lr=1e-2\n",
    "boost_lr=0.9\n",
    "\n",
    "model = RandFeatBoost(\n",
    "    generator=generator,\n",
    "    in_dim=D,\n",
    "    hidden_size=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_blocks=n_blocks,\n",
    "    activation=nn.Tanh(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    adam_lr=adam_lr,\n",
    "    boost_lr=boost_lr,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    upscale_type=\"dense\",\n",
    "    second_in_resblock=\"identity\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    X_train = model.upscale(X_train)\n",
    "    X_test  = model.upscale(X_test)\n",
    "\n",
    "    for t, (layer, DELTA, classifier) in enumerate(zip(model.layers, model.deltas, model.classifiers)):\n",
    "        X_train = X_train + model.boost_lr * DELTA * (layer(X_train) - X_train)\n",
    "        X_test = X_test + model.boost_lr * DELTA * (layer(X_test) - X_test)\n",
    "        \n",
    "        pred_train = classifier(X_train)\n",
    "        pred_test = classifier(X_test)\n",
    "        pred_train = torch.argmax(pred_train, dim=1)\n",
    "        pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT TIME: TODO TODO TODO TODO TODO\n",
    "\n",
    "# do gradient boosting for BINARY CLASSIFICATION\n",
    " \n",
    "# do f(x_t, x_0) and not just f(x_t)\n",
    "\n",
    "# xgboost model\n",
    "\n",
    "# optuna (with xgboost to start with?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

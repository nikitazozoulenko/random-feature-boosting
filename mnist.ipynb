{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import openml\n",
    "\n",
    "#from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocessing.stream_transforms import normalize_mean_std_traindata, normalize_streams, augment_time, add_basepoint_zero\n",
    "from utils.utils import print_name, print_shape\n",
    "from models import ResNet, NeuralEulerODE, RidgeClassifierCVModule, E2EResNet, LogisticRegressionSGD, RandFeatBoost\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options\n",
    "device = \"cuda\" # torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([60000, 784])\n",
      "Train labels shape: torch.Size([60000, 10])\n",
      "Test data shape: torch.Size([10000, 784])\n",
      "Test labels shape: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_path = \"/home/nikita/hdd/MNIST\"\n",
    "trainset = datasets.MNIST(mnist_path, download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST(mnist_path, download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "# Flatten the data\n",
    "X_train, y_train_cat = next(iter(trainloader))\n",
    "X_train = X_train.view(len(trainset), -1).to(device)\n",
    "X_test, y_test_cat = next(iter(testloader))\n",
    "X_test = X_test.view(len(testset), -1).to(device)\n",
    "\n",
    "# Convert train and test labels to one-hot encoding\n",
    "y_train = nn.functional.one_hot(y_train_cat, num_classes=10).float().to(device)\n",
    "y_test = nn.functional.one_hot(y_test_cat, num_classes=10).float().to(device)\n",
    "y_train_cat = y_train_cat.to(device)\n",
    "y_test_cat = y_test_cat.to(device)\n",
    "\n",
    "# Normalize by mean and std\n",
    "X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LogisticRegression\n",
    "\n",
    "N = 100\n",
    "D = 50\n",
    "C = 10\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "X = torch.randn(N, D, generator=gen)\n",
    "y = torch.randint(0, C, size=(N,), generator=gen)\n",
    "model = LogisticRegression(\n",
    "        gen,\n",
    "        in_dim = D,\n",
    "        out_dim = C,\n",
    "        l2_reg = 1.0,\n",
    "        max_iter = 100,\n",
    "    )\n",
    "_, _ = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Random Feature Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 10.0\n",
      "linesearch loss tensor(2.0531, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0531, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0531, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0531, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0531, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0531, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0531, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0530, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0529, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0529, grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.060540515929460526\n",
      "Gradient hat norm tensor(0.4610)\n",
      "alpha 10.0\n",
      "linesearch loss tensor(2.0529, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0529, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0529, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0529, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0529, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0528, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0528, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0528, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0528, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0528, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0528, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0528, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0528, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0528, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0528, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0527, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0527, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0527, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0527, grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(2.0527, grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.06324659287929535\n",
      "Gradient hat norm tensor(0.4707)\n"
     ]
    }
   ],
   "source": [
    "from models import GradientRandomFeatureBoostingClassification\n",
    "\n",
    "N = 100\n",
    "D = 50\n",
    "C = 10\n",
    "bottleneck_dim = 70\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "X = torch.randn(N, D, generator=gen)\n",
    "y = torch.randint(0, C, size=(N,), generator=gen)\n",
    "y = nn.functional.one_hot(y, num_classes=C).float()\n",
    "model = GradientRandomFeatureBoostingClassification(\n",
    "        gen,\n",
    "        hidden_dim = D,\n",
    "        bottleneck_dim = bottleneck_dim,\n",
    "        out_dim = C,\n",
    "        n_layers = 2,\n",
    "        upscale = \"dense\",\n",
    "        feature_type = \"dense\",\n",
    "    )\n",
    "_, _ = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allmodels_1dataset(\n",
    "        generator: torch.Generator,\n",
    "        X_train: Tensor,\n",
    "        y_train: Tensor,\n",
    "        X_test: Tensor,\n",
    "        y_test: Tensor,\n",
    "        ):\n",
    "    \n",
    "    D = X_train.shape[1]\n",
    "    hidden_size = 512\n",
    "    bottleneck_dim = 1*hidden_size\n",
    "    num_epochs = 50\n",
    "    batch_size = 128\n",
    "\n",
    "    # (name, model, kwargs). kwargs separate to save memory\n",
    "    model_list = [\n",
    "        # [\"T=10 RandFeatureBoost\", RandFeatBoost,\n",
    "        #         {\"generator\": generator,\n",
    "        #          \"in_dim\": D,\n",
    "        #          \"hidden_size\": hidden_size,\n",
    "        #          \"out_dim\": 10,\n",
    "        #          \"n_blocks\": 9,\n",
    "        #          \"activation\": nn.Tanh(),\n",
    "        #          \"loss_fn\": nn.CrossEntropyLoss(),\n",
    "        #          \"adam_lr\": 1e-2,\n",
    "        #          \"boost_lr\": 1.0,\n",
    "        #          \"epochs\": num_epochs,\n",
    "        #          \"batch_size\": batch_size,\n",
    "        #          \"upscale_type\": \"SWIM\",  # \"dense\", \"identity\"\n",
    "        #          }],\n",
    "\n",
    "        # [\"Tabular Ridge\", RidgeClassifierCVModule, {}],\n",
    "\n",
    "        # [\"Logistic SGD\", LogisticRegressionSGD, \n",
    "        #         {\"generator\": generator,\n",
    "        #          \"num_epochs\": num_epochs,\n",
    "        #          \"batch_size\": batch_size,\n",
    "        #          }],\n",
    "\n",
    "        # [\"Logistic L-BFSG\", LogisticRegression, \n",
    "        #         {\"generator\": generator,\n",
    "        #          \"in_dim\": D,\n",
    "        #          \"out_dim\": 10,\n",
    "        #          }],\n",
    "\n",
    "        # [\"T=1 Dense\", ResNet,\n",
    "        #         {\"generator\": generator,\n",
    "        #          \"in_dim\": D,\n",
    "        #          \"hidden_size\": hidden_size,\n",
    "        #          \"bottleneck_dim\": None,\n",
    "        #          \"n_blocks\": 0,\n",
    "        #          \"upsample_layer\": \"dense\",\n",
    "        #          \"output_layer\": \"logistic regression\",\n",
    "        #          }],\n",
    "\n",
    "        # [\"T=1 SWIM Grad\", ResNet,\n",
    "        #         {\"generator\": generator,\n",
    "        #         \"in_dim\": D,\n",
    "        #         \"hidden_size\": hidden_size,\n",
    "        #         \"bottleneck_dim\": None,\n",
    "        #         \"n_blocks\": 0,\n",
    "        #         \"upsample_layer\": \"SWIM\",\n",
    "        #         \"output_layer\": \"logistic regression\",\n",
    "        #         }],\n",
    "        \n",
    "        # [\"T=1 SWIM Unif\", ResNet,\n",
    "        #         {\"generator\": generator,\n",
    "        #         \"in_dim\": D,\n",
    "        #         \"hidden_size\": hidden_size,\n",
    "        #         \"bottleneck_dim\": None,\n",
    "        #         \"n_blocks\": 0,\n",
    "        #         \"upsample_layer\": \"SWIM\",\n",
    "        #         \"sampling_method\": \"uniform\",\n",
    "        #         \"output_layer\": \"logistic regression\",\n",
    "        #         }],\n",
    "\n",
    "    ]\n",
    "    for lr in [1.0]:\n",
    "        for l2_reg in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]:\n",
    "            model_list += [\n",
    "                [f\"Logistic L-BFSG, l2={l2_reg} lr={lr}\", \n",
    "                    LogisticRegression, \n",
    "                    {\"generator\": generator,\n",
    "                    \"in_dim\": D,\n",
    "                    \"out_dim\": 10,\n",
    "                    \"l2_reg\": l2_reg,\n",
    "                    \"lr\": lr,\n",
    "                    }],\n",
    "            ]\n",
    "    \n",
    "    for n_blocks in range(0, 20, 5):\n",
    "        model_list += [\n",
    "            [f\"T={n_blocks+1} Gradient GRFBoost\", \n",
    "             GradientRandomFeatureBoostingClassification,\n",
    "                {\"generator\": generator,\n",
    "                \"hidden_dim\": hidden_size,\n",
    "                \"bottleneck_dim\": bottleneck_dim,\n",
    "                \"out_dim\": 10,\n",
    "                \"n_layers\": n_blocks,\n",
    "                \"activation\": nn.Tanh(),\n",
    "                \"l2_reg\": 0.00001,\n",
    "                \"feature_type\": \"SWIM\",\n",
    "                \"boost_lr\": 0.5,\n",
    "                \"upscale\": \"SWIM\",\n",
    "                },\n",
    "                ],\n",
    "        ]\n",
    "\n",
    "    # for n_blocks in [4]:\n",
    "    #     model_list += [\n",
    "    #     [f\"T={n_blocks+1} End2End\", E2EResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": bottleneck_dim,\n",
    "    #             \"out_dim\": 10,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"activation\": nn.Tanh(),\n",
    "    #             \"loss\": nn.CrossEntropyLoss(),\n",
    "    #             \"lr\": 1e-2,\n",
    "    #             \"epochs\": num_epochs,\n",
    "    #             \"batch_size\": batch_size}\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResSWIM Grad-dense\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": bottleneck_dim,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"SWIM\",\n",
    "    #             \"res_layer1\": \"SWIM\",\n",
    "    #             \"res_layer2\": \"dense\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResSWIM Grad-id\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": hidden_size,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"SWIM\",\n",
    "    #             \"res_layer1\": \"SWIM\",\n",
    "    #             \"res_layer2\": \"identity\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResDense\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": hidden_size,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"dense\",\n",
    "    #             \"res_layer1\": \"dense\",\n",
    "    #             \"res_layer2\": \"identity\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "    # ]\n",
    "    \n",
    "    results = []\n",
    "    model_names = []\n",
    "    for name, model, model_args in model_list:\n",
    "        print(name)\n",
    "        t0 = time.perf_counter()\n",
    "        model = model(**model_args).to(X_train.device)\n",
    "        pred_train, _ = model.fit(X_train, y_train)\n",
    "        t1 = time.perf_counter()\n",
    "        pred_test = model(X_test)\n",
    "        t2 = time.perf_counter()\n",
    "        \n",
    "        #convert to class predictions:\n",
    "        if len(pred_train.shape) == 2:\n",
    "            pred_train = torch.argmax(pred_train, dim=1)\n",
    "            pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        result = np.array( [acc_train, acc_test, t1-t0, t2-t1] )\n",
    "        results.append( result )\n",
    "        model_names.append( name )\n",
    "\n",
    "    return model_names, results\n",
    "\n",
    "\n",
    "\n",
    "def run_all_experiments(\n",
    "        name_save: str = \"PLACEHOLDER\",\n",
    "        ):\n",
    "    # Fetch and process each dataset\n",
    "    experiments = {}\n",
    "    generator = torch.Generator(device=device).manual_seed(999)\n",
    "    results = run_allmodels_1dataset(\n",
    "        generator, X_train, y_train, X_test, y_test, \n",
    "        )\n",
    "    experiments[\"MNIST\"] = results\n",
    "\n",
    "    # Save results\n",
    "    # Assuming experiments is a dict where keys are dataset names and values are tuples (model_names, results)\n",
    "    attributes = [\"acc_train\", \"acc_test\", \"t_fit\", \"t_feat\"]\n",
    "    data_list = []\n",
    "    # Process the data\n",
    "    for dataset_name, (model_names, results) in experiments.items():\n",
    "        dataset_data = {}\n",
    "        for attr_idx, attribute in enumerate(attributes):\n",
    "            for model_idx, model_name in enumerate(model_names):\n",
    "                dataset_data[(attribute, model_name)] = results[model_idx][attr_idx]\n",
    "        data_list.append(pd.DataFrame(dataset_data, index=[dataset_name]))\n",
    "\n",
    "    # Combine all datasets into a single DataFrame\n",
    "    df = pd.concat(data_list)\n",
    "    df = df.sort_index(axis=1)\n",
    "    print(df)\n",
    "    df.to_pickle(f\"MNIST_{name_save}.pkl\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic L-BFSG, l2=1 lr=1.0\n",
      "Logistic L-BFSG, l2=0.1 lr=1.0\n",
      "Logistic L-BFSG, l2=0.01 lr=1.0\n",
      "Logistic L-BFSG, l2=0.001 lr=1.0\n",
      "Logistic L-BFSG, l2=0.0001 lr=1.0\n",
      "Logistic L-BFSG, l2=1e-05 lr=1.0\n",
      "Logistic L-BFSG, l2=1e-06 lr=1.0\n",
      "T=1 Gradient GRFBoost\n",
      "T=6 Gradient GRFBoost\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(5.7010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.9132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(12.8609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.07353143393993378\n",
      "Gradient hat norm tensor(512.1280, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.3513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.2836092412471771\n",
      "Gradient hat norm tensor(331.9391, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.4215553402900696\n",
      "Gradient hat norm tensor(156.0949, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.5264145135879517\n",
      "Gradient hat norm tensor(102.6270, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.6608768105506897\n",
      "Gradient hat norm tensor(68.4777, device='cuda:0')\n",
      "T=11 Gradient GRFBoost\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.2750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(6.0929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.9826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(15.7782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.07140400260686874\n",
      "Gradient hat norm tensor(529.7994, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.3093600571155548\n",
      "Gradient hat norm tensor(348.7671, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.4422467052936554\n",
      "Gradient hat norm tensor(167.2786, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.658907413482666\n",
      "Gradient hat norm tensor(83.5107, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.902441680431366\n",
      "Gradient hat norm tensor(56.2064, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.216383457183838\n",
      "Gradient hat norm tensor(25.5963, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.3961548805236816\n",
      "Gradient hat norm tensor(21.2264, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.5278934240341187\n",
      "Gradient hat norm tensor(16.0252, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.5019376277923584\n",
      "Gradient hat norm tensor(14.9559, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.2814475297927856\n",
      "Gradient hat norm tensor(15.9348, device='cuda:0')\n",
      "T=16 Gradient GRFBoost\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(6.2022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.9935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(15.5743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.07089880853891373\n",
      "Gradient hat norm tensor(529.6008, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.27981653809547424\n",
      "Gradient hat norm tensor(339.2474, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.3986343443393707\n",
      "Gradient hat norm tensor(177.3702, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.5423808097839355\n",
      "Gradient hat norm tensor(108.6247, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.6824899315834045\n",
      "Gradient hat norm tensor(66.1677, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.0682989358901978\n",
      "Gradient hat norm tensor(31.0393, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.1333669424057007\n",
      "Gradient hat norm tensor(24.9601, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.36312735080719\n",
      "Gradient hat norm tensor(18.4229, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.3568307161331177\n",
      "Gradient hat norm tensor(19.7517, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.4745619297027588\n",
      "Gradient hat norm tensor(15.2163, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.8517887592315674\n",
      "Gradient hat norm tensor(11.9629, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 2.0708413124084473\n",
      "Gradient hat norm tensor(11.2544, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.9381413459777832\n",
      "Gradient hat norm tensor(11.9423, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 2.330448865890503\n",
      "Gradient hat norm tensor(9.7006, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 3.043452262878418\n",
      "Gradient hat norm tensor(8.4880, device='cuda:0')\n",
      "                               acc_test                                   \\\n",
      "      Logistic L-BFSG, l2=0.0001 lr=1.0 Logistic L-BFSG, l2=0.001 lr=1.0   \n",
      "MNIST                            0.9238                           0.9231   \n",
      "\n",
      "                                                                      \\\n",
      "      Logistic L-BFSG, l2=0.01 lr=1.0 Logistic L-BFSG, l2=0.1 lr=1.0   \n",
      "MNIST                          0.9174                         0.8991   \n",
      "\n",
      "                                                                     \\\n",
      "      Logistic L-BFSG, l2=1 lr=1.0 Logistic L-BFSG, l2=1e-05 lr=1.0   \n",
      "MNIST                        0.848                           0.9243   \n",
      "\n",
      "                                                              \\\n",
      "      Logistic L-BFSG, l2=1e-06 lr=1.0 T=1 Gradient GRFBoost   \n",
      "MNIST                            0.922                0.9226   \n",
      "\n",
      "                                                     ...  \\\n",
      "      T=11 Gradient GRFBoost T=16 Gradient GRFBoost  ...   \n",
      "MNIST                 0.9684                  0.969  ...   \n",
      "\n",
      "                                 t_fit                                  \\\n",
      "      Logistic L-BFSG, l2=0.001 lr=1.0 Logistic L-BFSG, l2=0.01 lr=1.0   \n",
      "MNIST                         0.168494                        0.170229   \n",
      "\n",
      "                                                                   \\\n",
      "      Logistic L-BFSG, l2=0.1 lr=1.0 Logistic L-BFSG, l2=1 lr=1.0   \n",
      "MNIST                        0.17205                     0.322618   \n",
      "\n",
      "                                                                         \\\n",
      "      Logistic L-BFSG, l2=1e-05 lr=1.0 Logistic L-BFSG, l2=1e-06 lr=1.0   \n",
      "MNIST                         0.173497                         0.174033   \n",
      "\n",
      "                                                                           \\\n",
      "      T=1 Gradient GRFBoost T=11 Gradient GRFBoost T=16 Gradient GRFBoost   \n",
      "MNIST              1.274701               5.309116               7.186161   \n",
      "\n",
      "                             \n",
      "      T=6 Gradient GRFBoost  \n",
      "MNIST              3.380649  \n",
      "\n",
      "[1 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "_ = run_all_experiments(name_save=\"TESTING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T=16 Gradient GRFBoost               0.9690\n",
       "T=11 Gradient GRFBoost               0.9684\n",
       "T=6 Gradient GRFBoost                0.9649\n",
       "Logistic L-BFSG, l2=1e-05 lr=1.0     0.9243\n",
       "Logistic L-BFSG, l2=0.0001 lr=1.0    0.9238\n",
       "Logistic L-BFSG, l2=0.001 lr=1.0     0.9231\n",
       "T=1 Gradient GRFBoost                0.9226\n",
       "Logistic L-BFSG, l2=1e-06 lr=1.0     0.9220\n",
       "Logistic L-BFSG, l2=0.01 lr=1.0      0.9174\n",
       "Logistic L-BFSG, l2=0.1 lr=1.0       0.8991\n",
       "Logistic L-BFSG, l2=1 lr=1.0         0.8480\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"MNIST_TESTING.pkl\")\n",
    "df[\"acc_test\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T=5 End2End        0.9717\n",
    "# T=1 Dense          0.9215\n",
    "# T=1 SWIM Unif      0.9207\n",
    "# T=1 SWIM Grad      0.9204\n",
    "# Logistic SGD       0.8990\n",
    "# Tabular Ridge      0.8606\n",
    "# Logistic L-BFSG    0.8480\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T=16 Gradient GRFBoost               0.978417\n",
       "T=11 Gradient GRFBoost               0.976083\n",
       "T=6 Gradient GRFBoost                0.973017\n",
       "Logistic L-BFSG, l2=1e-05 lr=1.0     0.931450\n",
       "Logistic L-BFSG, l2=1e-06 lr=1.0     0.931200\n",
       "Logistic L-BFSG, l2=0.0001 lr=1.0    0.931000\n",
       "Logistic L-BFSG, l2=0.001 lr=1.0     0.930167\n",
       "T=1 Gradient GRFBoost                0.922150\n",
       "Logistic L-BFSG, l2=0.01 lr=1.0      0.918317\n",
       "Logistic L-BFSG, l2=0.1 lr=1.0       0.894617\n",
       "Logistic L-BFSG, l2=1 lr=1.0         0.838183\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"acc_train\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment layer by layer with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.33945271372795105\n",
      "Gradient hat norm tensor(198.0060, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.6588780879974365\n",
      "Gradient hat norm tensor(132.4522, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.1048954725265503\n",
      "Gradient hat norm tensor(65.1581, device='cuda:0')\n",
      "alpha 0.009999999776482582\n",
      "linesearch loss tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.2053641080856323\n",
      "Gradient hat norm tensor(44.8501, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 1.7405498027801514\n",
      "Gradient hat norm tensor(32.7763, device='cuda:0')\n",
      "alpha 0.10000000149011612\n",
      "linesearch loss tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 2.224963903427124\n",
      "Gradient hat norm tensor(25.0595, device='cuda:0')\n",
      "alpha 1.0\n",
      "linesearch loss tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 3.6836495399475098\n",
      "Gradient hat norm tensor(14.5713, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 3.8727076053619385\n",
      "Gradient hat norm tensor(9.0920, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 7.4931769371032715\n",
      "Gradient hat norm tensor(6.6352, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.016352470964193344\n",
      "Gradient hat norm tensor(5.8634, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.013967355713248253\n",
      "Gradient hat norm tensor(5.2619, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.016038687899708748\n",
      "Gradient hat norm tensor(5.6519, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.015640607103705406\n",
      "Gradient hat norm tensor(5.5188, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.01538341585546732\n",
      "Gradient hat norm tensor(5.4181, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.014461412094533443\n",
      "Gradient hat norm tensor(5.2664, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 3.7600932121276855\n",
      "Gradient hat norm tensor(7.0570, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.013705299235880375\n",
      "Gradient hat norm tensor(5.3371, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.01223501842468977\n",
      "Gradient hat norm tensor(4.8414, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.012786706909537315\n",
      "Gradient hat norm tensor(5.0744, device='cuda:0')\n",
      "alpha 10.0\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "linesearch loss tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Linesearch 0.011432201601564884\n",
      "Gradient hat norm tensor(4.7598, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Time to experiment with RandFeatureBoost\n",
    "\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 512\n",
    "n_blocks = 20\n",
    "boost_lr=1.0\n",
    "\n",
    "model = GradientRandomFeatureBoostingClassification(\n",
    "    generator=generator,\n",
    "    hidden_dim=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_layers=n_blocks,\n",
    "    l2_reg=0.0001,\n",
    "    boost_lr=boost_lr,\n",
    "    feature_type=\"SWIM\",\n",
    "    upscale=\"SWIM\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626833200454712\n",
      "0.9571999907493591\n"
     ]
    }
   ],
   "source": [
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0: Train acc: 0.9140666723251343, Test acc: 0.9178999662399292\n",
      "Block 1: Train acc: 0.9391999840736389, Test acc: 0.9405999779701233\n",
      "Block 2: Train acc: 0.949066698551178, Test acc: 0.9469999670982361\n",
      "Block 3: Train acc: 0.9535666704177856, Test acc: 0.9508000016212463\n",
      "Block 4: Train acc: 0.95660001039505, Test acc: 0.9531999826431274\n",
      "Block 5: Train acc: 0.9593999981880188, Test acc: 0.9551999568939209\n",
      "Block 6: Train acc: 0.9609000086784363, Test acc: 0.9566999673843384\n",
      "Block 7: Train acc: 0.9617166519165039, Test acc: 0.9573999643325806\n",
      "Block 8: Train acc: 0.9622666835784912, Test acc: 0.9566999673843384\n",
      "Block 9: Train acc: 0.9624500274658203, Test acc: 0.9565999507904053\n",
      "Block 10: Train acc: 0.9620500206947327, Test acc: 0.9563999772071838\n",
      "Block 11: Train acc: 0.9622333645820618, Test acc: 0.9565999507904053\n",
      "Block 12: Train acc: 0.9622666835784912, Test acc: 0.9563999772071838\n",
      "Block 13: Train acc: 0.9622833728790283, Test acc: 0.9565999507904053\n",
      "Block 14: Train acc: 0.9623667001724243, Test acc: 0.9569999575614929\n",
      "Block 15: Train acc: 0.9625999927520752, Test acc: 0.9573999643325806\n",
      "Block 16: Train acc: 0.962850034236908, Test acc: 0.9572999477386475\n",
      "Block 17: Train acc: 0.9627166986465454, Test acc: 0.9569999575614929\n",
      "Block 18: Train acc: 0.9627166986465454, Test acc: 0.9573999643325806\n",
      "Block 19: Train acc: 0.9626833200454712, Test acc: 0.9571999907493591\n"
     ]
    }
   ],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    with torch.no_grad():\n",
    "        X_train = model.upscale(X_train)\n",
    "        X_test  = model.upscale(X_test)\n",
    "\n",
    "        for t, (layer, (Delta, Delta_b), cls) in enumerate(zip(model.layers, model.deltas, model.classifiers[1:])):\n",
    "            X_train += model.boost_lr * (layer(X_train)@Delta + Delta_b )\n",
    "            X_test +=  model.boost_lr * (layer(X_test)@Delta + Delta_b )\n",
    "\n",
    "            #delta norm\n",
    "\n",
    "            pred_train = cls(X_train)\n",
    "            pred_test = cls(X_test)\n",
    "            pred_train = torch.argmax(pred_train, dim=1)\n",
    "            pred_test = torch.argmax(pred_test, dim=1)\n",
    "            acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "            acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "            print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "            # print(\"delta norm\", torch.linalg.norm(Delta).item())\n",
    "            # print(\"X_train norm\", torch.linalg.norm(X_train).item() / X_train.size(0))\n",
    "            # print(\"X_test norm\", torch.linalg.norm(X_test).item() / X_test.size(0))\n",
    "            \n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: normalize the gradient before fitting the next layer. This is to find the optimal direction. Then do line search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rand feat boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 5/30 [00:03<00:19,  1.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     10\u001b[0m boost_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m RandFeatBoost(\n\u001b[1;32m     13\u001b[0m     generator\u001b[38;5;241m=\u001b[39mgenerator,\n\u001b[1;32m     14\u001b[0m     in_dim\u001b[38;5;241m=\u001b[39mD,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     upscale_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSWIM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 27\u001b[0m pred_train, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m pred_test \u001b[38;5;241m=\u001b[39m model(X_test)\n\u001b[1;32m     29\u001b[0m pred_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(pred_train, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Code/zephyrox/pytorch_based/SWIM/models.py:757\u001b[0m, in \u001b[0;36mRandFeatBoost.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madam_lr, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs)):\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    758\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    760\u001b[0m         \u001b[38;5;66;03m#forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Time to experiment with RandFeatureBoost\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "num_epochs = 30\n",
    "batch_size = 512\n",
    "n_blocks = 5\n",
    "adam_lr=1e-2\n",
    "boost_lr=0.9\n",
    "\n",
    "model = RandFeatBoost(\n",
    "    generator=generator,\n",
    "    in_dim=D,\n",
    "    hidden_size=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_blocks=n_blocks,\n",
    "    activation=nn.Tanh(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    adam_lr=adam_lr,\n",
    "    boost_lr=boost_lr,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    upscale_type=\"SWIM\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    X_train = model.upscale(X_train)\n",
    "    X_test  = model.upscale(X_test)\n",
    "\n",
    "    for t, (layer, DELTA, classifier) in enumerate(zip(model.layers, model.deltas, model.classifiers)):\n",
    "        X_train = X_train + model.boost_lr * DELTA * (layer(X_train) - X_train)\n",
    "        X_test = X_test + model.boost_lr * DELTA * (layer(X_test) - X_test)\n",
    "        \n",
    "        pred_train = classifier(X_train)\n",
    "        pred_test = classifier(X_test)\n",
    "        pred_train = torch.argmax(pred_train, dim=1)\n",
    "        pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "\n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SWIM-ID vs DENSE-ID vs SWIM-DENSE \n",
    "# implement 'finding gradient direction' gradient boosting\n",
    "\n",
    "# Test whether this is actually better than non-boost with same hidden size !!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with DENSE\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 800\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "n_blocks = 10\n",
    "adam_lr=1e-2\n",
    "boost_lr=0.9\n",
    "\n",
    "model = RandFeatBoost(\n",
    "    generator=generator,\n",
    "    in_dim=D,\n",
    "    hidden_size=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_blocks=n_blocks,\n",
    "    activation=nn.Tanh(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    adam_lr=adam_lr,\n",
    "    boost_lr=boost_lr,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    upscale_type=\"dense\",\n",
    "    second_in_resblock=\"identity\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    X_train = model.upscale(X_train)\n",
    "    X_test  = model.upscale(X_test)\n",
    "\n",
    "    for t, (layer, DELTA, classifier) in enumerate(zip(model.layers, model.deltas, model.classifiers)):\n",
    "        X_train = X_train + model.boost_lr * DELTA * (layer(X_train) - X_train)\n",
    "        X_test = X_test + model.boost_lr * DELTA * (layer(X_test) - X_test)\n",
    "        \n",
    "        pred_train = classifier(X_train)\n",
    "        pred_test = classifier(X_test)\n",
    "        pred_train = torch.argmax(pred_train, dim=1)\n",
    "        pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT TIME: TODO TODO TODO TODO TODO\n",
    "\n",
    "# do gradient boosting for BINARY CLASSIFICATION\n",
    " \n",
    "# do f(x_t, x_0) and not just f(x_t)\n",
    "\n",
    "# xgboost model\n",
    "\n",
    "# optuna (with xgboost to start with?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

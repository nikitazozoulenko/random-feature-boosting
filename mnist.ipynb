{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import openml\n",
    "\n",
    "#from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options\n",
    "device = \"cuda\" # torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([60000, 784])\n",
      "Train labels shape: torch.Size([60000, 10])\n",
      "Test data shape: torch.Size([10000, 784])\n",
      "Test labels shape: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def normalize_mean_std_traindata(X_train: Tensor, X_test: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    mean = X_train.mean(dim=0)\n",
    "    std = X_train.std(dim=0)\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "\n",
    "    X_train = torch.clip(X_train, -5, 5)\n",
    "    X_test = torch.clip(X_test, -5, 5)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_path = \"/home/nikita/hdd/MNIST\"\n",
    "trainset = datasets.MNIST(mnist_path, download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST(mnist_path, download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "# Flatten the data\n",
    "X_train, y_train_cat = next(iter(trainloader))\n",
    "X_train = X_train.view(len(trainset), -1).to(device)\n",
    "X_test, y_test_cat = next(iter(testloader))\n",
    "X_test = X_test.view(len(testset), -1).to(device)\n",
    "\n",
    "# Convert train and test labels to one-hot encoding\n",
    "y_train = nn.functional.one_hot(y_train_cat, num_classes=10).float().to(device)\n",
    "y_test = nn.functional.one_hot(y_test_cat, num_classes=10).float().to(device)\n",
    "y_train_cat = y_train_cat.to(device)\n",
    "y_test_cat = y_test_cat.to(device)\n",
    "\n",
    "# Normalize by mean and std\n",
    "X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_pred tensor([[ -0.2574, -10.2641,   0.3835,  ...,  11.1232,   0.0372,   3.3196],\n",
      "        [  5.8839,   1.3779,  13.1877,  ..., -18.7733,   4.5620, -11.9310],\n",
      "        [ -5.7133,   6.3624,   1.9242,  ...,   0.8247,   0.3554,  -1.5758],\n",
      "        ...,\n",
      "        [ -7.5551,  -7.3165,  -2.6753,  ...,   2.3219,   4.0589,   4.8147],\n",
      "        [ -2.7736,  -1.8737,  -3.1286,  ...,  -4.0853,   6.4928,  -3.2438],\n",
      "        [  2.6829, -10.5148,   4.8185,  ...,  -7.0494,  -0.5013,  -4.2465]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train accuracy: 0.9334999918937683\n",
      "Test accuracy: 0.9265999794006348\n"
     ]
    }
   ],
   "source": [
    "from models.base import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(\n",
    "        n_classes = 10,\n",
    "        l2_lambda = 0.001,\n",
    "        max_iter = 300,\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientRFBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9990167021751404\n",
      "Test accuracy: 0.9726999998092651\n"
     ]
    }
   ],
   "source": [
    "from models.random_feature_representation_boosting import GradientRFRBoostClassifier\n",
    "\n",
    "model = GradientRFRBoostClassifier(\n",
    "    in_dim = 784,\n",
    "    hidden_dim = 512,\n",
    "    n_classes = 10,\n",
    "    randfeat_xt_dim = 512,\n",
    "    randfeat_x0_dim = 512,\n",
    "    n_layers = 3,\n",
    "    l2_cls =  0.000001,\n",
    "    l2_ghat = 0.00001,\n",
    "    feature_type=\"SWIM\",\n",
    "    upscale_type = \"SWIM\",\n",
    "    lbfgs_max_iter = 300,\n",
    "    boost_lr = 1.0,\n",
    "    use_batchnorm=True,\n",
    "    SWIM_scale=1.0,\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "#TODO NEXT: add xtx0 to the classification case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ce at layer 0: 0.27760976552963257\n",
      "Test ce at layer 0: 0.27482572197914124\n",
      "Train acc at layer 0: 0.9209666848182678\n",
      "Test acc at layer 0: 0.9220999479293823\n",
      "\n",
      "Train ce at layer 1: 0.09838391095399857\n",
      "Test ce at layer 1: 0.13978564739227295\n",
      "Train acc at layer 1: 0.9709666967391968\n",
      "Test acc at layer 1: 0.9599999785423279\n",
      "\n",
      "Train ce at layer 2: 0.06271514296531677\n",
      "Test ce at layer 2: 0.12089692056179047\n",
      "Train acc at layer 2: 0.9815833568572998\n",
      "Test acc at layer 2: 0.9641000032424927\n",
      "\n",
      "Train ce at layer 3: 0.045359883457422256\n",
      "Test ce at layer 3: 0.11288446933031082\n",
      "Train acc at layer 3: 0.9872333407402039\n",
      "Test acc at layer 3: 0.9679999947547913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def see_results_for_every_layer(X_train, y_train, X_test, y_test, model, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        X0_train = X_train\n",
    "        X0_test = X_test\n",
    "\n",
    "        X_train = model.upscale(X0_train)\n",
    "        X_test = model.upscale(X0_test)\n",
    "\n",
    "        pred_train = model.top_level_modules[0](X_train)\n",
    "        pred_test = model.top_level_modules[0](X_test)\n",
    "\n",
    "        ce = loss_fn(pred_train, y_train)\n",
    "        ce_test = loss_fn(pred_test, y_test)\n",
    "        acc = (pred_train.argmax(1) == y_train.argmax(1)).float().mean()\n",
    "        acc_test = (pred_test.argmax(1) == y_test.argmax(1)).float().mean()\n",
    "        print(f\"Train ce at layer 0: {ce}\")\n",
    "        print(f\"Test ce at layer 0: {ce_test}\")\n",
    "        print(f\"Train acc at layer 0: {acc}\")\n",
    "        print(f\"Test acc at layer 0: {acc_test}\")\n",
    "        print()\n",
    "        \n",
    "        for t, (feat_layer, ghat_layer, classifier, batchnorm) in enumerate(zip(model.random_feature_layers, \n",
    "                                                                     model.ghat_boosting_layers, \n",
    "                                                                     model.top_level_modules[1:],\n",
    "                                                                     model.batchnorms)):\n",
    "            features_train = feat_layer(X_train, X0_train)\n",
    "            features_test = feat_layer(X_test, X0_test)\n",
    "            X_train += model.boost_lr * ghat_layer(features_train)\n",
    "            X_train = batchnorm(X_train)\n",
    "            X_test  += model.boost_lr * ghat_layer(features_test)\n",
    "            X_test = batchnorm(X_test)\n",
    "            \n",
    "            pred_train = classifier(X_train)\n",
    "            pred_test = classifier(X_test)\n",
    "\n",
    "            ce = loss_fn(pred_train, y_train)\n",
    "            ce_test = loss_fn(pred_test, y_test)\n",
    "            acc = (pred_train.argmax(1) == y_train.argmax(1)).float().mean()\n",
    "            acc_test = (pred_test.argmax(1) == y_test.argmax(1)).float().mean()\n",
    "            print(f\"Train ce at layer {t+1}: {ce}\")\n",
    "            print(f\"Test ce at layer {t+1}: {ce_test}\")\n",
    "            print(f\"Train acc at layer {t+1}: {acc}\")\n",
    "            print(f\"Test acc at layer {t+1}: {acc_test}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "see_results_for_every_layer(X_train, y_train, X_test, y_test, model, nn.functional.cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End2End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:18<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_pred tensor([[-1.97553634643554687500e-03, -5.60329854488372802734e-03,\n",
      "         -3.84935736656188964844e-03,  ...,\n",
      "          1.01216506958007812500e+00,  4.82936203479766845703e-03,\n",
      "          2.54796445369720458984e-03],\n",
      "        [-1.76844596862792968750e-02,  1.61109864711761474609e-02,\n",
      "          1.00505971908569335938e+00,  ...,\n",
      "         -8.94658267498016357422e-04,  3.86652350425720214844e-03,\n",
      "         -1.08496844768524169922e-03],\n",
      "        [ 1.16206407546997070312e-02,  9.91596341133117675781e-01,\n",
      "          1.04889273643493652344e-04,  ...,\n",
      "          5.59684634208679199219e-03, -7.64999538660049438477e-03,\n",
      "         -5.87991625070571899414e-03],\n",
      "        ...,\n",
      "        [ 3.46951186656951904297e-04, -3.10655683279037475586e-03,\n",
      "         -2.05048918724060058594e-03,  ...,\n",
      "         -3.10361385345458984375e-04,  1.76037102937698364258e-03,\n",
      "         -5.23433834314346313477e-03],\n",
      "        [-8.93914699554443359375e-03, -6.42771273851394653320e-03,\n",
      "         -1.36475265026092529297e-03,  ...,\n",
      "          5.29793649911880493164e-03,  3.78524549305438995361e-02,\n",
      "          3.19596379995346069336e-03],\n",
      "        [ 2.18762457370758056641e-03, -7.46537744998931884766e-03,\n",
      "         -2.60429829359054565430e-03,  ...,\n",
      "          4.26127761602401733398e-03,  1.36948376893997192383e-03,\n",
      "          4.29590791463851928711e-03]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Train accuracy: 0.9958500266075134\n",
      "Test accuracy: 0.9812999963760376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from models.end2end import End2EndMLPResNet\n",
    "\n",
    "model = End2EndMLPResNet(\n",
    "    in_dim = X_train.shape[1],\n",
    "    hidden_dim = 128,\n",
    "    bottleneck_dim = 32,\n",
    "    out_dim = 10,\n",
    "    n_blocks = 4,\n",
    "    lr = 0.01,\n",
    "    end_lr_factor = 0.01,\n",
    "    n_epochs = 20,\n",
    "    weight_decay = 0.001,\n",
    "    batch_size = 512\n",
    "    )\n",
    "X_train_pred = model.fit_transform(X_train, y_train)\n",
    "X_test_pred = model(X_test)\n",
    "\n",
    "print(\"X_test_pred\", X_test_pred)\n",
    "\n",
    "train_accuracy = (torch.argmax(X_train_pred, dim=1) == y_train_cat).float().mean().item()\n",
    "test_accuracy = (torch.argmax(X_test_pred, dim=1) == y_test_cat).float().mean().item()\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
